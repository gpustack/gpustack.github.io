{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"api-reference/","title":"API Reference","text":"<p>GPUStack provides a built-in Swagger UI. You can access it by navigating to <code>&lt;gpustack-server-url&gt;/docs</code> in your browser to view and interact with the APIs.</p> <p></p>"},{"location":"architecture/","title":"Architecture","text":"<p>The following diagram shows the architecture of GPUStack:</p> <p></p>"},{"location":"architecture/#server","title":"Server","text":"<p>The GPUStack server consists of the following components:</p> <ul> <li>API Server: Provides a RESTful interface for clients to interact with the system. It handles authentication and authorization.</li> <li>Scheduler: Responsible for assigning model instances to workers.</li> <li>Model Controller: Manages the rollout and scaling of model instances to match the desired model replicas.</li> <li>HTTP Proxy: Routes completion API requests to backend inference servers.</li> </ul>"},{"location":"architecture/#worker","title":"Worker","text":"<p>GPUStack workers are responsible for:</p> <ul> <li>Running inference servers for model instances assigned to the worker.</li> <li>Reporting status to the server.</li> </ul>"},{"location":"architecture/#sql-database","title":"SQL Database","text":"<p>The GPUStack server connects to a SQL database as the datastore. Currently, GPUStack uses SQLite. Stay tuned for support for external databases like PostgreSQL in upcoming releases.</p>"},{"location":"architecture/#inference-server","title":"Inference Server","text":"<p>Inference servers are the backends that performs the inference tasks. GPUStack uses llama-box as the inference server.</p>"},{"location":"code-of-conduct/","title":"Contributor Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@gpustack.ai. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p>"},{"location":"contributing/","title":"Contributing to GPUStack","text":"<p>Thanks for taking the time to contribute to GPUStack!</p> <p>Please review and follow the Code of Conduct.</p>"},{"location":"contributing/#filing-issues","title":"Filing Issues","text":"<p>If you find any bugs or are having any trouble, please search the reported issue as someone may have experienced the same issue, or we are actively working on a solution.</p> <p>If you can't find anything related to your issue, contact us by filing an issue. To help us diagnose and resolve, please include as much information as possible, including:</p> <ul> <li>Software: GPUStack version, installation method, operating system info, etc.</li> <li>Hardware: Node info, GPU info, etc.</li> <li>Steps to reproduce: Provide as much detail on how you got into the reported situation.</li> <li>Logs: Please include any relevant logs, such as server logs, worker logs, etc.</li> </ul>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":"<p>For setting up development environment, please refer to Development Guide.</p> <p>If you're fixing a small issue, you can simply submit a PR. However, if you're planning to submit a bigger PR to implement a new feature or fix a relatively complex bug, please open an issue that explains the change and the motivation for it. If you're addressing a bug, please explain how to reproduce it.</p>"},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<p>If you have any updates to our documentation, feel free to file an issue with the <code>documentation</code> label or make a pull request.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<p>Install <code>python 3.10+</code>.</p>"},{"location":"development/#set-up-environment","title":"Set Up Environment","text":"<pre><code>make install\n</code></pre>"},{"location":"development/#run","title":"Run","text":"<pre><code>poetry run gpustack\n</code></pre>"},{"location":"development/#build","title":"Build","text":"<pre><code>make build\n</code></pre> <p>And check artifacts in <code>dist</code>.</p>"},{"location":"development/#test","title":"Test","text":"<pre><code>make test\n</code></pre>"},{"location":"development/#update-dependencies","title":"Update Dependencies","text":"<pre><code>poetry add &lt;something&gt;\n</code></pre> <p>Or</p> <pre><code>poetry add --group dev &lt;something&gt;\n</code></pre> <p>For dev/testing dependencies.</p>"},{"location":"overview/","title":"GPUStack","text":"<p>GPUStack is an open-source GPU cluster manager for running large language models(LLMs).</p>"},{"location":"overview/#key-features","title":"Key Features","text":"<ul> <li>Supports a Wide Variety of Hardware: Run with different brands of GPUs in Apple MacBooks, Windows PCs, and Linux servers.</li> <li>Scales with Your GPU Inventory: Easily add more GPUs or nodes to scale up your operations.</li> <li>Lightweight Python Package: Minimal dependencies and operational overhead.</li> <li>OpenAI-compatible APIs: Serve APIs that are compatible with OpenAI standards.</li> <li>User and API key management: Simplified management of users and API keys.</li> <li>GPU metrics monitoring: Monitor GPU performance and utilization in real-time.</li> <li>Token usage and rate metrics: Track token usage and manage rate limits effectively.</li> </ul>"},{"location":"overview/#supported-platforms","title":"Supported Platforms","text":"<ul> <li> MacOS</li> <li> Linux</li> <li> Windows</li> </ul>"},{"location":"overview/#supported-accelerators","title":"Supported Accelerators","text":"<ul> <li> Apple Metal</li> <li> NVIDIA CUDA</li> </ul> <p>We plan to support the following accelerators in future releases.</p> <ul> <li> AMD ROCm</li> <li> Intel oneAPI</li> <li> MTHREADS MUSA</li> <li> Qualcomm AI Engine</li> </ul>"},{"location":"overview/#supported-models","title":"Supported Models","text":"<p>GPUStack uses llama.cpp as the backend and supports large language models in GGUF format. Models from the following sources are supported:</p> <ol> <li> <p>Hugging Face</p> </li> <li> <p>Ollama Library</p> </li> </ol> <p>Here are some example models:</p> <ul> <li> LLaMA</li> <li> Mistral 7B</li> <li> Mixtral MoE</li> <li> DBRX</li> <li> Falcon</li> <li> Baichuan</li> <li> Aquila</li> <li> Yi</li> <li> StableLM</li> <li> Deepseek</li> <li> Qwen</li> <li> Phi</li> <li> Gemma</li> <li> Mamba</li> <li> Grok-1</li> </ul>"},{"location":"overview/#openai-compatible-apis","title":"OpenAI-Compatible APIs","text":"<p>GPUStack serves OpenAI compatible APIs. For details, please refer to OpenAI Compatible APIs</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#linux-or-macos","title":"Linux or MacOS","text":"<p>GPUStack provides a script to install it as a service on systemd or launchd based systems. To install GPUStack using this method, just run:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>Optionally, you can add extra workers to form a GPUStack cluster by running the following command on other nodes (replace <code>http://myserver</code> and <code>mytoken</code> with your actual server URL and token):</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken\n</code></pre> <p>In the default setup, you can run the following to get the token used for adding workers:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre>"},{"location":"quickstart/#windows","title":"Windows","text":"<p>Run PowerShell as administrator (avoid using PowerShell ISE), then run the following command to install GPUStack:</p> <pre><code>Invoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>Optionally, you can add extra workers to form a GPUStack cluster by running the following command on other nodes (replace <code>http://myserver</code> and <code>mytoken</code> with your actual server URL and token):</p> <pre><code>Invoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --server-url http://myserver --token mytoken\"\n</code></pre> <p>In the default setup, you can run the following to get the token used for adding workers:</p> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n</code></pre>"},{"location":"quickstart/#manual-installation","title":"Manual Installation","text":"<p>For manual installation or detailed configurations, refer to the installation docs.</p>"},{"location":"quickstart/#getting-started","title":"Getting Started","text":"<ol> <li>Run and chat with the llama3 model:</li> </ol> <pre><code>gpustack chat llama3 \"tell me a joke.\"\n</code></pre> <ol> <li>Open <code>http://myserver</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</li> </ol> <p>Linux or MacOS</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>Windows</p> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> <ol> <li>Click <code>Playground</code> in the navigation menus. Now you can chat with the LLM in the UI playground.</li> </ol> <p></p> <ol> <li> <p>Click <code>API Keys</code> in the navigation menus, then click the <code>New API Key</code> button.</p> </li> <li> <p>Fill in the <code>Name</code> and click the <code>Save</code> button.</p> </li> <li> <p>Copy the generated API key and save it somewhere safe. Please note that you can only see it once on creation.</p> </li> <li> <p>Now you can use the API key to access the OpenAI-compatible API. For example, use curl as the following:</p> </li> </ol> <pre><code>export GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#view-gpustack-logs","title":"View GPUStack Logs","text":"<p>If you installed GPUStack using the installation script, you can view GPUStack logs at the following path:</p>"},{"location":"troubleshooting/#linux-or-macos","title":"Linux or MacOS","text":"<pre><code>/var/log/gpustack.log\n</code></pre>"},{"location":"troubleshooting/#windows","title":"Windows","text":"<pre><code>\"$env:APPDATA\\gpustack\\log\\gpustack.log\"\n</code></pre>"},{"location":"troubleshooting/#configure-log-level","title":"Configure Log Level","text":"<p>You can enable the DEBUG log level on <code>gpustack start</code> by setting the <code>--debug</code> parameter.</p> <p>You can configure log level of GPUStack server at runtime by running the following command on the server node:</p> <pre><code>curl -X PUT http://localhost/debug/log_level -d \"debug\"\n</code></pre>"},{"location":"upgrade/","title":"Upgrade","text":"<p>You can upgrade GPUStack using the installation script or by manually installing the desired version of the GPUStack Python package.</p> <p>Note</p> <p>When upgrading, upgrade the GPUStack server first, then upgrade the workers.</p>"},{"location":"upgrade/#upgrade-gpustack-using-the-installation-script","title":"Upgrade GPUStack Using the Installation Script","text":"<p>To upgrade GPUStack from an older version, re-run the installation script using the same configuration options you originally used.</p> <p>Running the installation script will:</p> <ol> <li>Install the latest version of the GPUStack Python package.</li> <li>Update the system service (systemd, launchd, or Windows) init script to reflect the arguments passed to the installation script.</li> <li>Restart the GPUStack service.</li> </ol>"},{"location":"upgrade/#linux-and-macos","title":"Linux and macOS","text":"<p>For example, to upgrade GPUStack to the latest version on a Linux system and MacOS:</p> <pre><code>curl -sfL https://get.gpustack.ai | &lt;EXISTING_INSTALL_ENV&gt; sh -s - &lt;EXISTING_GPUSTACK_ARGS&gt;\n</code></pre> <p>To upgrade to a specific version, specify the <code>INSTALL_PACKAGE_SPEC</code> environment variable similar to the <code>pip install</code> command:</p> <pre><code>curl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack==x.y.z &lt;EXISTING_INSTALL_ENV&gt; sh -s - &lt;EXISTING_GPUSTACK_ARGS&gt;\n</code></pre>"},{"location":"upgrade/#windows","title":"Windows","text":"<p>To upgrade GPUStack to the latest version on a Windows system:</p> <pre><code>$env:&lt;EXISTING_INSTALL_ENV&gt; = &lt;EXISTING_INSTALL_ENV_VALUE&gt;\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>To upgrade to a specific version:</p> <pre><code>$env:INSTALL_PACKAGE_SPEC = gpustack==x.y.z\n$env:&lt;EXISTING_INSTALL_ENV&gt; = &lt;EXISTING_INSTALL_ENV_VALUE&gt;\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } &lt;EXISTING_GPUSTACK_ARGS&gt;\"\n</code></pre>"},{"location":"upgrade/#manual-upgrade","title":"Manual Upgrade","text":"<p>If you install GPUStack manually, upgrade using the common <code>pip</code> workflow.</p> <p>For example, to upgrade GPUStack to the latest version:</p> <pre><code>pip install --upgrade gpustack\n</code></pre> <p>Then restart the GPUStack service according to your setup.</p>"},{"location":"cli-reference/chat/","title":"gpustack chat","text":"<p>Chat with a large language model.</p> <pre><code>gpustack chat model [prompt]\n</code></pre>"},{"location":"cli-reference/chat/#positional-arguments","title":"Positional Arguments","text":"Name Description model The model to use for chat. prompt The prompt to send to the model. [Optional]"},{"location":"cli-reference/chat/#one-time-chat-with-a-prompt","title":"One-time Chat with a Prompt","text":"<p>If a prompt is provided, it performs a one-time inference. For example:</p> <pre><code>gpustack chat llama3 \"tell me a joke.\"\n</code></pre> <p>Example output:</p> <pre><code>Why couldn't the bicycle stand up by itself?\n\nBecause it was two-tired!\n</code></pre>"},{"location":"cli-reference/chat/#interactive-chat","title":"Interactive Chat","text":"<p>If the <code>prompt</code> argument is not provided, you can chat with the large language model interactively. For example:</p> <pre><code>gpustack chat llama3\n</code></pre> <p>Example output:</p> <pre><code>&gt;tell me a joke.\nHere's one:\n\nWhy couldn't the bicycle stand up by itself?\n\n(wait for it...)\n\nBecause it was two-tired!\n\nHope that made you smile!\n&gt;Do you have a better one?\nHere's another one:\n\nWhy did the scarecrow win an award?\n\n(think about it for a sec...)\n\nBecause he was outstanding in his field!\n\nHope that one stuck with you!\n\nDo you want to hear another one?\n&gt;\\quit\n</code></pre>"},{"location":"cli-reference/chat/#interactive-commands","title":"Interactive Commands","text":"<p>Followings are available commands in interactive chat:</p> <pre><code>Commands:\n  \\q or \\quit - Quit the chat\n  \\c or \\clear - Clear chat context in prompt\n  \\? or \\h or \\help - Print this help message\n</code></pre>"},{"location":"cli-reference/chat/#connect-to-external-gpustack-server","title":"Connect to External GPUStack Server","text":"<p>If you are not running <code>gpustack chat</code> on the server node, or if you are serving on a custom host or port, you should provide the following environment variables:</p> Name Description GPUSTACK_SERVER_URL URL of the GPUStack server, e.g., <code>http://myserver</code>. GPUSTACK_API_KEY GPUStack API key."},{"location":"cli-reference/start/","title":"gpustack start","text":"<p>Run GPUStack server or worker.</p> <pre><code>gpustack start [OPTIONS]\n</code></pre>"},{"location":"cli-reference/start/#configurations","title":"Configurations","text":""},{"location":"cli-reference/start/#common-options","title":"Common Options","text":"Flag Default Description <code>--config-file</code> value Path to the YAML config file. <code>-d</code> value, <code>--debug</code> value <code>False</code> To enable debug mode, the short flag -d is not supported in Windows because this flag is reserved by PowerShell for CommonParameters. <code>--data-dir</code> value Directory to store data. Default is OS specific. <code>-t</code> value, <code>--token</code> value Auto-generated. Shared secret used to add a worker."},{"location":"cli-reference/start/#server-options","title":"Server Options","text":"Flag Default Description <code>--host</code> value <code>0.0.0.0</code> Host to bind the server to. <code>--port</code> value <code>80</code> Port to bind the server to. <code>--disable-worker</code> <code>False</code> Disable embedded worker. <code>--bootstrap-password</code> value Auto-generated. Initial password for the default admin user. <code>--system-reserved</code> value <code>\"{\\\"memory\\\": 1, \\\"gpu_memory\\\": 1}\"</code> The system reserves resources for the worker during scheduling, measured in GiB. By default, 1 GiB of memory and 1 GiB of GPU memory are reserved. <code>--ssl-keyfile</code> value Path to the SSL key file. <code>--ssl-certfile</code> value Path to the SSL certificate file. <code>--force-auth-localhost</code> <code>False</code> Force authentication for requests originating from localhost (127.0.0.1).When set to True, all requests from localhost will require authentication. <code>--ollama-library-base-url</code> <code>https://registry.ollama.ai</code> Base URL for the Ollama library."},{"location":"cli-reference/start/#worker-options","title":"Worker Options","text":"Flag Default Description <code>-s</code> value, <code>--server-url</code> value Server to connect to. <code>--worker-ip</code> value IP address of the worker node. Auto-detected by default. <code>--disable-metrics</code> <code>False</code> Disable metrics. <code>--metrics-port</code> value <code>10151</code> Port to expose metrics. <code>--worker-port</code> value <code>10150</code> Port to bind the worker to. Use a consistent value for all workers. <code>--log-dir</code> value Directory to store logs."},{"location":"cli-reference/start/#config-file","title":"Config File","text":"<p>You can configure start options using a YAML-format config file when starting GPUStack server or worker. Here is a complete example:</p> <pre><code># Common Options\ndebug: false\ndata_dir: /path/to/dir\ntoken: mytoken\n\n# Server Options\nhost: 0.0.0.0\nport: 80\ndisable_worker: false\nssl_keyfile: /path/to/keyfile\nssl_certfile: /path/to/certfile\nforce_auth_localhost: false\nbootstrap_password: myadminpassword\nsystem_reserved:\n  memory: 1\n  gpu_memory: 1\nollama_library_base_url: https://registry.mycompany.com\n\n# Worker Options\nserver_url: http://myserver\nworker_ip: 192.168.1.101\ndisable_metrics: false\nmetrics_port: 10151\nworker_port: 10150\nlog_dir: /path/to/dir\n</code></pre>"},{"location":"installation/docker-installation/","title":"Docker Installation","text":"<p>You can use the official Docker image to run GPUStack in a container. Installation using docker is supported on:</p> <ul> <li>Linux with Nvidia GPUs</li> </ul>"},{"location":"installation/docker-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Nvidia Container Toolkit</li> </ul>"},{"location":"installation/docker-installation/#run-gpustack-with-docker","title":"Run GPUStack with Docker","text":"<p>Run the following command to start the GPUStack server:</p> <pre><code>docker run -d --gpus all -p 80:80 gpustack/gpustack\n</code></pre> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For example, to start a GPUStack worker:</p> <pre><code>docker run -d --gpus all -p 80:80 gpustack/gpustack start --server-url http://myserver --token mytoken\n</code></pre> <p>For more configurations, please refer to the CLI Reference.</p>"},{"location":"installation/docker-installation/#run-gpustack-with-docker-compose","title":"Run GPUStack with Docker Compose","text":"<p>Get the docker-compose file from GPUStack repository, run the following command to start the GPUStack server:</p> <pre><code>docker-compose up -d\n</code></pre> <p>You can update the <code>docker-compose.yml</code> file to customize the command while starting a GPUStack worker.</p>"},{"location":"installation/installation-script/","title":"Installation Script","text":""},{"location":"installation/installation-script/#linux-and-macos","title":"Linux and MacOS","text":"<p>You can use the installation script available at <code>https://get.gpustack.ai</code> to install GPUStack as a service on systemd and launchd based systems.</p> <p>You can set additional environment variables and CLI flags when running the script. The following are examples running the installation script with different configurations:</p> <pre><code># Run server.\ncurl -sfL https://get.gpustack.ai | sh -s -\n\n# Run server without the embedded worker.\ncurl -sfL https://get.gpustack.ai | sh -s - --disable-worker\n\n# Run server with TLS.\ncurl -sfL https://get.gpustack.ai | sh -s - --ssl-keyfile /path/to/keyfile --ssl-certfile /path/to/certfile\n\n# Run worker with specified IP.\ncurl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken --worker-ip 192.168.1.100\n\n# Install with a custom index URL.\ncurl -sfL https://get.gpustack.ai | INSTALL_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple sh -s -\n\n# Install a custom wheel package other than releases form pypi.org.\ncurl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=https://repo.mycompany.com/my-gpustack.whl sh -s -\n</code></pre>"},{"location":"installation/installation-script/#windows","title":"Windows","text":"<p>You can use the installation script available at <code>https://get.gpustack.ai</code> to install GPUStack as a service on Windows Service Manager.</p> <p>You can set additional environment variables and CLI flags when running the script. The following are examples running the installation script with different configurations:</p> <pre><code># Run server.\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Run server without the embedded worker.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --disable-worker\"\n\n# Run server with TLS.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --ssl-keyfile 'C:\\path\\to\\keyfile' --ssl-certfile 'C:\\path\\to\\certfile'\"\n\n# Run worker with specified IP.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --server-url 'http://myserver' --token 'mytoken' --worker-ip '192.168.1.100'\"\n\n# Run worker with customize reserved resource.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } --server-url 'http://myserver' --token 'mytoken' --system-reserved '{\"\"memory\"\":5, \"\"gpu_memory\"\":5}'\"\n\n# Install with a custom index URL.\n$env:INSTALL_INDEX_URL = \"https://pypi.tuna.tsinghua.edu.cn/simple\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Install a custom wheel package other than releases form pypi.org.\n$env:INSTALL_PACKAGE_SPEC = \"https://repo.mycompany.com/my-gpustack.whl\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>Warning</p> <p>Avoid using PowerShell ISE as it is not compatible with the installation script.</p>"},{"location":"installation/installation-script/#available-environment-variables","title":"Available Environment Variables","text":"Name Default Description <code>INSTALL_INDEX_URL</code> (empty) Base URL of the Python Package Index. <code>INSTALL_PACKAGE_SPEC</code> <code>gpustack</code> The package spec to install. It supports PYPI package names, URLs, and local paths. See https://pip.pypa.io/en/stable/cli/pip_install/#pip-install for details. <code>INSTALL_PRE_RELEASE</code> (empty) If set to 1, pre-release packages will be installed."},{"location":"installation/installation-script/#available-cli-flags","title":"Available CLI Flags","text":"<p>The appended CLI flags of the installation script are passed directly as flags for the <code>gpustack start</code> command. You can refer to the CLI Reference for details.</p>"},{"location":"installation/installation-script/#run-server","title":"Run Server","text":"<p>To run a GPUStack server, install GPUStack without the <code>--server-url</code> flag. By default, the GPUStack server also runs a worker.</p> <p>If you want to run the server without the embedded worker, use the <code>--disable-worker</code> flag.</p>"},{"location":"installation/installation-script/#add-worker","title":"Add Worker","text":"<p>To add a GPUStack worker, install GPUStack with the <code>--server-url</code> flag to specify the server it should connect to.</p>"},{"location":"installation/manual-installation/","title":"Manual Installation","text":""},{"location":"installation/manual-installation/#prerequites","title":"Prerequites:","text":"<p>Install python3.10 or above with pip.</p>"},{"location":"installation/manual-installation/#install-gpustack-cli","title":"Install GPUStack CLI","text":"<p>Run the following to install GPUStack:</p> <pre><code>pip install gpustack\n</code></pre> <p>To verify, run:</p> <pre><code>gpustack version\n</code></pre>"},{"location":"installation/manual-installation/#run-gpustack","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server:</p> <pre><code>gpustack start\n</code></pre> <p>By default, GPUStack uses <code>/var/lib/gpustack</code> as the data directory so you need <code>sudo</code> or proper permission for that. You can also set a custom data directory by running:</p> <pre><code>gpustack start --data-dir mypath\n</code></pre>"},{"location":"installation/manual-installation/#run-gpustack-as-a-system-service","title":"Run GPUStack as a System Service","text":"<p>A recommended way is to run GPUStack as a startup service. For example, using systemd:</p> <p>Create a service file in <code>/etc/systemd/system/gpustack.service</code>:</p> <pre><code>[Unit]\nDescription=GPUStack Service\n\n[Service]\nExecStart=gpustack start\nRestart=always\nRestartSec=3\nStandardOutput=append:/var/log/gpustack.log\nStandardError=append:/var/log/gpustack.log\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Then start GPUStack:</p> <pre><code>systemctl daemon-reload\nsystemctl enable gpustack\n</code></pre>"},{"location":"installation/uninstallation/","title":"Uninstallation","text":""},{"location":"installation/uninstallation/#uninstallation-script","title":"Uninstallation Script","text":"<p>Warning</p> <p>Uninstallation script deletes the data in local datastore(sqlite), configuration, model cache, and all of the scripts and CLI tools. It does not remove any data from external datastores.</p> <p>If you installed GPUStack using the installation script, a script to uninstall GPUStack was generated during installation.</p>"},{"location":"installation/uninstallation/#linux-or-macos","title":"Linux or MacOS","text":"<p>Run the following command to uninstall GPUStack:</p> <pre><code>sudo /var/lib/gpustack/uninstall.sh\n</code></pre>"},{"location":"installation/uninstallation/#windows","title":"Windows","text":"<p>Run the following command in PowerShell to uninstall GPUStack:</p> <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; &amp; \"$env:APPDATA\\gpustack\\uninstall.ps1\"\n</code></pre>"},{"location":"installation/uninstallation/#manual-uninstallation","title":"Manual Uninstallation","text":"<p>If you install GPUStack manually, the followings are example commands to uninstall GPUStack. You can modify according to your setup:</p> <pre><code># Stop and remove the service.\nsystemctl stop gpustack.service\nrm /etc/systemd/system/gpustack.service\nsystemctl daemon-reload\n# Uninstall the CLI.\npip uninstall gpustack\n# Remove the data directory.\nrm -rf /var/lib/gpustack\n</code></pre>"},{"location":"user-guide/api-key-management/","title":"API Key Management","text":"<p>GPUStack supports authentication using API keys. Each GPUStack user can generate and manage their own API keys.</p>"},{"location":"user-guide/api-key-management/#create-api-key","title":"Create API Key","text":"<ol> <li>Navigate to the <code>API Keys</code> page.</li> <li>Click the <code>New API Key</code> button.</li> <li>Fill in the <code>Name</code>, <code>Description</code>, and select the <code>Expiration</code> of the API key.</li> <li>Click the <code>Save</code> button.</li> <li>Copy and store the key somewhere safe, then click the <code>Done</code> button.</li> </ol> <p>Note</p> <p>Please note that you can only see the generated API key once upon creation.</p>"},{"location":"user-guide/api-key-management/#delete-api-key","title":"Delete API Key","text":"<ol> <li>Navigate to the <code>API Keys</code> page.</li> <li>Find the API key you want to delete.</li> <li>Click the <code>Delete</code> button in the <code>Operations</code> column.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/api-key-management/#use-api-key","title":"Use API Key","text":"<p>GPUStack supports using the API key as a bearer token. The following is an example using curl:</p> <pre><code>export GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"user-guide/model-management/","title":"Model Management","text":"<p>You can manage large language models in GPUStack by navigating to the <code>Models</code> page. A model in GPUStack contains one or multiple replicas of model instances. On deployment, GPUStack automatically computes resource requirements for the model instances from model metadata and schedules them to available workers accordingly.</p>"},{"location":"user-guide/model-management/#deploy-model","title":"Deploy Model","text":"<ol> <li> <p>To deploy a model, click the <code>Deploy Model</code> button.</p> </li> <li> <p>Fill in the <code>Name</code> of the model.</p> </li> <li> <p>Select the <code>Source</code> of the model. Currently, models from <code>Hugging Face</code> and the <code>Ollama Library</code> in GGUF format are supported.</p> </li> <li> <p>For <code>Hugging Face</code> models, search and fill in the Hugging Face repo ID, e.g., <code>microsoft/Phi-3-mini-4k-instruct-gguf</code>, then select the <code>File Name</code>, e.g., <code>phi-3-mini-4k-instruct-q4.gguf</code>. For <code>Ollama Library</code> models, select an <code>Ollama Model</code> from the dropdown list, or input any Ollama model you need, e.g., <code>llama3:70b</code>.</p> </li> <li> <p>Adjust the <code>Replicas</code> as needed.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-management/#edit-model","title":"Edit Model","text":"<ol> <li>Find the model you want to edit on the model list page.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>Update the attributes as needed. For example, change the <code>Replicas</code> to scale up or down.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-management/#delete-model","title":"Delete Model","text":"<ol> <li>Find the model you want to delete on the model list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/model-management/#view-model-instance","title":"View Model Instance","text":"<ol> <li>Find the model you want to check on the model list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the model.</li> </ol>"},{"location":"user-guide/model-management/#delete-model-instance","title":"Delete Model Instance","text":"<ol> <li>Find the model you want to check on the model list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the model.</li> <li>Find the model instance you want to delete.</li> <li>Click the ellipsis button for the model instance in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol> <p>Note</p> <p>After a model instance is deleted, GPUStack will recreate a new instance to satisfy the expected replicas of the model if necessary.</p>"},{"location":"user-guide/model-management/#view-model-instance-logs","title":"View Model Instance Logs","text":"<ol> <li>Find the model you want to check on the model list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the model.</li> <li>Find the model instance you want to check.</li> <li>Click the <code>View Logs</code> button for the model instance in the <code>Operations</code> column.</li> </ol>"},{"location":"user-guide/model-management/#use-self-hosted-model","title":"Use Self-hosted Model","text":"<p>You can deploy self-hosted Ollama models by configuring the <code>--ollama-library-base-url</code> option in the GPUStack server. The <code>Ollama Library</code> URL should point to the base URL of the Ollama model registry. For example, <code>https://registry.mycompany.com</code>.</p> <p>Here is an example workflow to set up a registry, publish a model, and use it in GPUStack:</p> <pre><code># Run a self-hosted OCI registry\ndocker run -d -p 5001:5000 --name registry registry:2\n\n# Push a model to the registry using Ollama\nollama pull llama3\nollama cp llama3 localhost:5001/library/llama3\nollama push localhost:5001/library/llama3 --insecure\n\n# Start GPUStack server with the custom Ollama library URL\ncurl -sfL https://get.gpustack.ai | sh -s - --ollama-library-base-url http://localhost:5001\n</code></pre> <p>That's it! You can now deploy the model <code>llama3</code> from <code>Ollama Library</code> source in GPUStack as usual, but the model will now be fetched from the self-hosted registry.</p>"},{"location":"user-guide/openai-compatible-apis/","title":"OpenAI Compatible APIs","text":"<p>GPUStack serves OpenAI compatible APIs using the <code>/v1-openai</code> path.</p>"},{"location":"user-guide/openai-compatible-apis/#supported-endpoints","title":"Supported Endpoints","text":"<p>The following API endpoints are supported:</p> <ul> <li> List Models</li> <li> Create Completions</li> <li> Create Chat Completions</li> <li> Create Embeddings</li> </ul>"},{"location":"user-guide/openai-compatible-apis/#usage","title":"Usage","text":"<p>The following are examples using the APIs in different languages:</p>"},{"location":"user-guide/openai-compatible-apis/#curl","title":"curl","text":"<pre><code>export GPUSTACK_API_KEY=myapikey\ncurl http://myserver/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"user-guide/openai-compatible-apis/#openai-python-api-library","title":"OpenAI Python API library","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"http://myserver/v1-openai\", api_key=\"myapikey\")\n\ncompletion = client.chat.completions.create(\n  model=\"llama3\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre>"},{"location":"user-guide/openai-compatible-apis/#openai-node-api-library","title":"OpenAI Node API library","text":"<pre><code>const OpenAI = require(\"openai\");\n\nconst openai = new OpenAI({\n  apiKey: \"myapikey\",\n  baseURL: \"http://myserver/v1-openai\",\n});\n\nasync function main() {\n  const params = {\n    model: \"llama3\",\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant.\",\n      },\n      {\n        role: \"user\",\n        content: \"Hello!\",\n      },\n    ],\n  };\n  const chatCompletion = await openai.chat.completions.create(params);\n  console.log(chatCompletion.choices[0].message);\n}\nmain();\n</code></pre>"},{"location":"user-guide/playground/","title":"Playground","text":"<p>GPUStack provides a playground UI for users to test or play with the completion API. The following is an example screenshot:</p> <p></p>"},{"location":"user-guide/playground/#prompts","title":"Prompts","text":"<p>You can adjust the prompt messages on the left side of the playground. There are three role types of prompt messages: system, user, and assistant.</p> <ul> <li>System: Typically a predefined instruction or guidance that sets the context, defines the behavior, or imposes specific constraints on how the model should generate its responses.</li> <li>User: The input or query provided by the user (the person interacting with the LLM).</li> <li>Assistant: The response generated by the LLM.</li> </ul>"},{"location":"user-guide/playground/#edit-system-message","title":"Edit System Message","text":"<p>You can add and edit the system message at the top of the playground.</p>"},{"location":"user-guide/playground/#edit-user-and-assistant-messages","title":"Edit User and Assistant Messages","text":"<p>To add a user or assistant message, click the <code>New Message</code> button.</p> <p>To remove a user or assistant message, click the minus button at the right corner of the message.</p> <p>To change the role of a message, click the <code>User</code> or <code>Assistant</code> text at the beginning of the message.</p>"},{"location":"user-guide/playground/#clear-prompts","title":"Clear Prompts","text":"<p>Click the <code>Clear</code> button to clear all the prompts.</p>"},{"location":"user-guide/playground/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground. Please refer to Model Management to learn about how to manage models.</p>"},{"location":"user-guide/playground/#customize-parameters","title":"Customize Parameters","text":"<p>You can customize completion parameters in the <code>Parameters</code> section.</p>"},{"location":"user-guide/playground/#do-completion","title":"Do Completion","text":"<p>You can do a completion by clicking the <code>Submit</code> button.</p>"},{"location":"user-guide/playground/#view-code","title":"View Code","text":"<p>Once you've done experimenting with the prompts and parameters, you can click the <code>View Code</code> button to check how you can call the API with the same input by code. Code examples in <code>curl</code>, <code>python</code>, and <code>nodejs</code> are provided.</p>"},{"location":"user-guide/user-management/","title":"User Management","text":"<p>GPUStack supports users of two roles: <code>Admin</code> and <code>User</code>. Admins can monitor system status, manage models, users, and system settings. Users can manage their own API keys and use the completion API.</p>"},{"location":"user-guide/user-management/#default-admin","title":"Default Admin","text":"<p>On bootstrap, GPUStack creates a default admin user. The initial password for the default admin is stored in <code>&lt;data-dir&gt;/initial_admin_password</code>. In the default setup, it should be <code>/var/lib/gpustack/initial_admin_password</code>. You can customize the default admin password by setting the <code>--bootstrap-password</code> parameter when starting <code>gpustack</code>.</p>"},{"location":"user-guide/user-management/#create-user","title":"Create User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Click the <code>Create User</code> button.</li> <li>Fill in <code>Name</code>, <code>Full Name</code>, <code>Password</code>, and select <code>Role</code> for the user.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/user-management/#update-user","title":"Update User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to edit.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>Update the attributes as needed.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/user-management/#delete-user","title":"Delete User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to delete.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol>"}]}