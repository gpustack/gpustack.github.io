{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"api-reference/","title":"API Reference","text":"<p>GPUStack provides a built-in Swagger UI. You can access it by navigating to <code>&lt;gpustack-server-url&gt;/docs</code> in your browser to view and interact with the APIs.</p> <p></p>"},{"location":"architecture/","title":"Architecture","text":"<p>The diagram below provides a high-level view of the GPUStack architecture.</p> <p></p> <p>The diagram below details the internal components and their interactions.</p> <p></p>"},{"location":"architecture/#server","title":"Server","text":"<p>The GPUStack server consists of the following components:</p> <ul> <li>API Server: Provides a RESTful interface for clients to interact with the system. It handles authentication and authorization.</li> <li>Scheduler: Responsible for assigning model instances to workers.</li> <li>Controllers: Manages the state of resources in the system. For example, they handle the rollout and scaling of model instances to match the desired number of replicas.</li> </ul>"},{"location":"architecture/#worker","title":"Worker","text":"<p>The GPUStack worker consists of the following components:</p> <ul> <li>GPUStack Runtime: Detects GPU devices and interacts with the container runtime to deploy model instances.</li> <li>Serving Manager: Manages the lifecycle of model instances on the worker.</li> <li>Metric Exporter: Exports metrics about the model instances and their performance.</li> </ul>"},{"location":"architecture/#ai-gateway","title":"AI Gateway","text":"<p>The AI Gateway handles incoming API requests from clients. It routes requests to the appropriate model instances based on the requested model. GPUStack uses Higress for API routing and load balancing.</p>"},{"location":"architecture/#sql-database","title":"SQL Database","text":"<p>The GPUStack server connects to a SQL database as the datastore. GPUStack uses an Embedded PostgreSQL by default, but you can configure it to use an external PostgreSQL or MySQL as well.</p>"},{"location":"architecture/#inference-server","title":"Inference Server","text":"<p>Inference servers are the backends that performs the inference tasks. GPUStack supports vLLM, SGLang, Ascend MindIE and vox-box as the built-in inference server. You can also add custom inference backends.</p>"},{"location":"architecture/#ray","title":"Ray","text":"<p>Ray is a distributed computing framework that GPUStack utilizes to run distributed vLLM. GPUStack bootstraps Ray cluster on-demand to run distributed vLLM across multiple workers.</p>"},{"location":"code-of-conduct/","title":"Contributor Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@gpustack.ai. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p>"},{"location":"contributing/","title":"Contributing to GPUStack","text":"<p>Thanks for taking the time to contribute to GPUStack!</p> <p>Please review and follow the Code of Conduct.</p>"},{"location":"contributing/#filing-issues","title":"Filing Issues","text":"<p>If you find any bugs or are having any trouble, please search the reported issue as someone may have experienced the same issue, or we are actively working on a solution.</p> <p>If you can't find anything related to your issue, contact us by filing an issue. To help us diagnose and resolve, please include as much information as possible, including:</p> <ul> <li>Software: GPUStack version, installation method, operating system info, etc.</li> <li>Hardware: Node info, GPU info, etc.</li> <li>Steps to reproduce: Provide as much detail on how you got into the reported situation.</li> <li>Logs: Please include any relevant logs, such as server logs, worker logs, etc.</li> </ul>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":"<p>For setting up development environment, please refer to Development Guide.</p> <p>If you're fixing a small issue, you can simply submit a PR. However, if you're planning to submit a bigger PR to implement a new feature or fix a relatively complex bug, please open an issue that explains the change and the motivation for it. If you're addressing a bug, please explain how to reproduce it.</p>"},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<p>If you have any updates to our documentation, feel free to file an issue with the <code>documentation</code> label or make a pull request.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<p>Install Python (version 3.10 to 3.12).</p>"},{"location":"development/#set-up-environment","title":"Set Up Environment","text":"<pre><code>make install\n</code></pre>"},{"location":"development/#run","title":"Run","text":"<pre><code>uv run gpustack\n</code></pre>"},{"location":"development/#build","title":"Build","text":"<pre><code>make build\n</code></pre> <p>And check artifacts in <code>dist</code>.</p>"},{"location":"development/#test","title":"Test","text":"<pre><code>make test\n</code></pre>"},{"location":"development/#update-dependencies","title":"Update Dependencies","text":"<pre><code>uv add &lt;something&gt;\n</code></pre> <p>Or</p> <pre><code>uv add --dev &lt;something&gt;\n</code></pre> <p>For dev/testing dependencies.</p>"},{"location":"environment-variables/","title":"Environment Variables","text":"<p>GPUStack supports various environment variables for configuration. </p> <p>Most command line parameters can also be set via environment variables with the <code>GPUSTACK_</code> prefix and in uppercase format (e.g., <code>--data-dir</code> can be set via <code>GPUSTACK_DATA_DIR</code>). </p> <p>For a complete list of command line parameters that can be set as environment variables, see CLI Reference. This will not be discussed here.</p>"},{"location":"environment-variables/#priority-order","title":"Priority Order","text":"<p>Configuration values are applied in the following priority order (highest to lowest):</p> <ol> <li>Command line arguments</li> <li>Environment variables</li> <li>Configuration file</li> <li>Default values</li> </ol> <p>This means that command line arguments will always override environment variables, and environment variables will override values in the configuration file.</p>"},{"location":"environment-variables/#gpustack-core-environment-variables","title":"GPUStack Core Environment Variables","text":"<p>These environment variables are typically used for third-party service integrations.</p>"},{"location":"environment-variables/#hugging-face-hub","title":"Hugging Face Hub","text":"Variable Description Default <code>HF_ENDPOINT</code> Hugging Face Hub endpoint. e.g., <code>https://hf-mirror.com</code> (empty)"},{"location":"environment-variables/#database-configuration","title":"Database Configuration","text":"Variable Description Default <code>GPUSTACK_DB_ECHO</code> Enable database query logging. <code>false</code> <code>GPUSTACK_DB_POOL_SIZE</code> Database connection pool size. <code>5</code> <code>GPUSTACK_DB_MAX_OVERFLOW</code> Database connection pool max overflow. <code>10</code> <code>GPUSTACK_DB_POOL_TIMEOUT</code> Database connection pool timeout in seconds. <code>30</code>"},{"location":"environment-variables/#network-configuration","title":"Network Configuration","text":"Variable Description Default <code>GPUSTACK_PROXY_TIMEOUT_SECONDS</code> Proxy timeout in seconds. <code>1800</code> <code>GPUSTACK_TCP_CONNECTOR_LIMIT</code> HTTP client TCP connector limit. <code>1000</code>"},{"location":"environment-variables/#authentication-security","title":"Authentication &amp; Security","text":"Variable Description Default <code>GPUSTACK_JWT_TOKEN_EXPIRE_MINUTES</code> JWT token expiration time in minutes. <code>120</code>"},{"location":"environment-variables/#gateway-configuration","title":"Gateway Configuration","text":"Variable Description Default <code>GPUSTACK_HIGRESS_EXT_AUTH_TIMEOUT_MS</code> Higress external authentication timeout in milliseconds. <code>3000</code>"},{"location":"environment-variables/#worker-and-model-configuration","title":"Worker and Model Configuration","text":"Variable Description Default <code>GPUSTACK_WORKER_HEARTBEAT_GRACE_PERIOD</code> Worker heartbeat grace period in seconds. <code>150</code> <code>GPUSTACK_WORKER_ORPHAN_WORKLOAD_CLEANUP_GRACE_PERIOD</code> Worker orphan workload cleanup grace period in seconds. <code>300</code> <code>GPUSTACK_MODEL_INSTANCE_RESCHEDULE_GRACE_PERIOD</code> Model instance reschedule grace period in seconds. <code>300</code> <code>GPUSTACK_MODEL_INSTANCE_HEALTH_CHECK_INTERVAL</code> Model instance health check interval in seconds. <code>3</code> <code>GPUSTACK_MODEL_EVALUATION_CACHE_MAX_SIZE</code> Maximum size of model evaluation cache. <code>1000</code> <code>GPUSTACK_MODEL_EVALUATION_CACHE_TTL</code> TTL of model evaluation cache in seconds. <code>3600</code> <code>GPUSTACK_DISABLE_OS_FILELOCK</code> Disable OS file lock. <code>false</code>"},{"location":"environment-variables/#model-deployment-configuration","title":"Model Deployment Configuration","text":"<p>Note</p> <p>These environment variables are not set when starting GPUStack. Instead, they should be configured in the Advanced Options &gt; Environment Variables section when deploying a model. They are used to customize the model serving behavior.</p> Variable Description Default <code>GPUSTACK_MODEL_SERVING_COMMAND_SCRIPT_DISABLED</code> Disable the automatic serving command script execution. When set to <code>1</code> or <code>true</code>, the script that handles package installation and other setup tasks will not run. <code>0</code> <code>PYPI_PACKAGES_INSTALL</code> Additional PyPI packages to install in the model serving environment. Multiple packages should be space-separated. The script will use <code>uv pip install</code> if available, otherwise <code>pip install</code>. (empty)"},{"location":"environment-variables/#usage-example","title":"Usage Example","text":"<p>When deploying a model, navigate to Advanced Options &gt; Environment Variables and add:</p> <pre><code># Install additional packages before model serving starts\nPYPI_PACKAGES_INSTALL=torch-audio==2.0.0 transformers==4.30.0\n\n# Disable the serving command script entirely\nGPUSTACK_MODEL_SERVING_COMMAND_SCRIPT_DISABLED=1\n</code></pre> <p>The serving command script automatically handles:</p> <ul> <li>Installing additional PyPI packages specified in <code>PYPI_PACKAGES_INSTALL</code></li> <li>Supporting both <code>uv pip</code> and <code>pip</code> for package installation</li> <li>Handling custom PyPI indices via <code>PIP_INDEX_URL</code> and <code>PIP_EXTRA_INDEX_URL</code></li> </ul>"},{"location":"environment-variables/#gpustack-runtime-environment-variables","title":"GPUStack Runtime Environment Variables","text":"<p>These environment variables are used by GPUStack runtime. Commonly used to adjust the behavior of inference backends running in Docker/Kubernetes.</p> <p>They are only usable within workers. Please set the environment variables in the workers\u2019 containers to ensure they take effect properly.</p>"},{"location":"environment-variables/#global-variables","title":"Global Variables","text":"Variable Description Default <code>GPUSTACK_RUNTIME_LOG_LEVEL</code> Log level. <code>INFO</code> <code>GPUSTACK_RUNTIME_LOG_TO_FILE</code> Log to file path instead of stdout. (empty) <code>GPUSTACK_RUNTIME_LOG_WARNING</code> Enable logging warnings. <code>0</code> <code>GPUSTACK_RUNTIME_LOG_EXCEPTION</code> Enable logging exceptions. <code>1</code>"},{"location":"environment-variables/#detector-variables","title":"Detector Variables","text":"Variable Description Default <code>GPUSTACK_RUNTIME_DETECT</code> Detector to use. Options: Auto, AMD, ASCEND, CAMBRICON, HYGON, ILUVATAR, METAX, MTHREADS, NVIDIA. <code>Auto</code> <code>GPUSTACK_RUNTIME_DETECT_NO_PCI_CHECK</code> Enable no PCI check during detection. Useful for WSL environments. (empty) <code>GPUSTACK_RUNTIME_DETECT_BACKEND_MAP_RESOURCE_KEY</code> Backend mapping to resource keys. The default values named by each vendor <code>GPUSTACK_RUNTIME_DETECT_PHYSICAL_INDEX_PRIORITY</code> Use physical index priority at detecting devices. <code>1</code>"},{"location":"environment-variables/#deployer-variables","title":"Deployer Variables","text":"Variable Description Default <code>GPUSTACK_RUNTIME_DEPLOY</code> Deployer to use. Options: Auto, Docker, Kubernetes. <code>Auto</code> <code>GPUSTACK_RUNTIME_DEPLOY_DEFAULT_REGISTRY</code> Default container registry for deployer to pull images from. (empty) <code>GPUSTACK_RUNTIME_DEPLOY_API_CALL_ERROR_DETAIL</code> Enable detailing the API call error during deployment. <code>1</code> <code>GPUSTACK_RUNTIME_DEPLOY_ASYNC</code> Enable asynchronous deployment. <code>1</code> <code>GPUSTACK_RUNTIME_DEPLOY_ASYNC_THREADS</code> The number of threads in the threadpool. (empty) <code>GPUSTACK_RUNTIME_DEPLOY_MIRRORED_DEPLOYMENT</code> Enable mirrored deployment mode. <code>0</code> <code>GPUSTACK_RUNTIME_DEPLOY_MIRRORED_NAME</code> The name of the deployer. (empty) <code>GPUSTACK_RUNTIME_DEPLOY_MIRRORED_DEPLOYMENT_IGNORE_ENVIRONMENTS</code> Environment variable names to ignore during mirrored deployment. (empty) <code>GPUSTACK_RUNTIME_DEPLOY_MIRRORED_DEPLOYMENT_IGNORE_VOLUMES</code> Volume mount destinations to ignore during mirrored deployment. (empty) <code>GPUSTACK_RUNTIME_DEPLOY_CORRECT_RUNNER_IMAGE</code> Correct the gpustack-runner image by rendering it with the host's detection. <code>1</code> <code>GPUSTACK_RUNTIME_DEPLOY_LABEL_PREFIX</code> Label prefix for the deployer. <code>runtime.gpustack.ai</code> <code>GPUSTACK_RUNTIME_DEPLOY_AUTOMAP_RESOURCE_KEY</code> The resource key to use for automatic mapping of container backend visible devices environment variables. <code>gpustack.ai/devices</code> <code>GPUSTACK_RUNTIME_DEPLOY_RESOURCE_KEY_MAP_RUNTIME_VISIBLE_DEVICES</code> Manual mapping of runtime visible devices environment variables. The default values named by each vendor <code>GPUSTACK_RUNTIME_DEPLOY_RESOURCE_KEY_MAP_BACKEND_VISIBLE_DEVICES</code> Manual mapping of backend visible devices environment variables. The default values named by each vendor <code>GPUSTACK_RUNTIME_DEPLOY_RUNTIME_VISIBLE_DEVICES_VALUE_UUID</code> Use UUIDs for the given runtime visible devices environment variables. (empty) <code>GPUSTACK_RUNTIME_DEPLOY_BACKEND_VISIBLE_DEVICES_VALUE_ALIGNMENT</code> Enable value alignment for the given backend visible devices environment variables. <code>ASCEND_RT_VISIBLE_DEVICES,NPU_VISIBLE_DEVICES</code>"},{"location":"environment-variables/#docker-variables","title":"Docker Variables","text":"Variable Description Default <code>GPUSTACK_RUNTIME_DOCKER_MIRRORED_NAME_FILTER_LABELS</code> Filter labels for selecting the mirrored deployer container in Docker. (empty) <code>GPUSTACK_RUNTIME_DOCKER_PAUSE_IMAGE</code> Docker image used for the pause container. <code>gpustack/runtime:pause</code> <code>GPUSTACK_RUNTIME_DOCKER_UNHEALTHY_RESTART_IMAGE</code> Docker image used for unhealthy restart container. <code>gpustack/runtime:health</code> <code>GPUSTACK_RUNTIME_DOCKER_EPHEMERAL_FILES_DIR</code> Directory for storing ephemeral files for Docker. <code>~/.cache/gpustack-runtime</code> <code>GPUSTACK_RUNTIME_DOCKER_MUTE_ORIGINAL_HEALTHCHECK</code> Mute the original healthcheck of the container. <code>1</code>"},{"location":"environment-variables/#kubernetes-variables","title":"Kubernetes Variables","text":"Variable Description Default <code>GPUSTACK_RUNTIME_KUBERNETES_NODE_NAME</code> Name of the Kubernetes Node to deploy workloads to. (empty) <code>GPUSTACK_RUNTIME_KUBERNETES_NAMESPACE</code> Namespace of the Kubernetes to deploy workloads to. <code>default</code> <code>GPUSTACK_RUNTIME_KUBERNETES_DOMAIN_SUFFIX</code> Domain suffix for Kubernetes services. <code>cluster.local</code> <code>GPUSTACK_RUNTIME_KUBERNETES_SERVICE_TYPE</code> Service type for Kubernetes services. Options: ClusterIP, NodePort, LoadBalancer. <code>ClusterIP</code> <code>GPUSTACK_RUNTIME_KUBERNETES_QUORUM_READ</code> Whether to use quorum read for Kubernetes services. <code>0</code>"},{"location":"environment-variables/#rocm-detector-variables","title":"ROCm Detector Variables","text":"Variable Description Default <code>ROCM_SMI_LIB_PATH</code> ROCm SMI library path. (empty) <code>ROCM_HOME</code> ROCm home directory. (empty) <code>ROCM_PATH</code> ROCm path. <code>/opt/rocm</code> <code>ROCM_CORE_LIB_PATH</code> ROCm core library path. (empty)"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#support-matrix","title":"Support Matrix","text":""},{"location":"faq/#hybird-cluster-support","title":"Hybird Cluster Support","text":"<p>GPUStack supports heterogeneous clusters spanning NVIDIA, AMD, Ascend, Hygon, Moore Threads, Iluvatar, MetaX, and Cambricon GPUs, and works across both AMD64 and ARM64 architectures.</p>"},{"location":"faq/#distributed-inference-support","title":"Distributed Inference Support","text":"<p>Single-Node Multi-GPU</p> <ul> <li> vLLM</li> <li> SGLang</li> <li> MindIE</li> </ul> <p>Multi-Node Multi-GPU</p> <ul> <li> vLLM</li> <li> SGLang</li> <li> MindIE</li> </ul> <p>Tip</p> <p>Related documentations:</p> <p>vLLM\uff1aDistributed Inference and Serving</p> <p>SGLang\uff1aMulti-Node Deployment</p> <p>MindIE: Multi-Node Inference</p>"},{"location":"faq/#installation","title":"Installation","text":""},{"location":"faq/#how-can-i-change-the-registered-worker-name","title":"How can I change the registered worker name?","text":"<p>You can set it to a custom name using the <code>--worker-name</code> flag when running GPUStack:</p> <pre><code>sudo docker run -d --name gpustack \\\n    ...\n    gpustack/gpustack \\\n+    --worker-name New-Name\n</code></pre>"},{"location":"faq/#how-can-i-change-the-registered-worker-ip","title":"How can I change the registered worker IP?","text":"<p>You can set it to a custom IP using the <code>--worker-ip</code> flag when running GPUStack:</p> <pre><code>sudo docker run -d --name gpustack \\\n    ...\n    gpustack/gpustack \\\n+    --worker-ip xx.xx.xx.xx\n</code></pre>"},{"location":"faq/#where-are-gpustacks-data-stored","title":"Where are GPUStack's data stored?","text":"<p>When running the GPUStack container, the Docker volume is mounted using <code>--volume/-v</code> parameter. The default data path is under the Docker data directory, specifically in the volumes subdirectory, and the default path is:</p> <pre><code>/var/lib/docker/volumes/gpustack-data/_data\n</code></pre> <p>You can check it by the following method:</p> <pre><code>docker volume ls\ndocker volume inspect gpustack-data\n</code></pre> <p>If you need to change it to a custom path, modify the mount configuration when running container. For example, to mount the host directory <code>/data/gpustack</code>:</p> <pre><code>sudo docker run -d --name gpustack \\\n    ...\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n-    --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /data/gpustack:/var/lib/gpustack \\\n    ...\n    gpustack/gpustack\n</code></pre>"},{"location":"faq/#where-are-model-files-stored","title":"Where are model files stored?","text":"<p>When running the GPUStack container, the Docker volume is mounted using <code>--volume/-v</code> parameter. The default cache path is under the Docker data directory, specifically in the volumes subdirectory, and the default path is:</p> <pre><code>/var/lib/docker/volumes/gpustack-data/_data/cache\n</code></pre> <p>You can check it by the following method:</p> <pre><code>docker volume ls\ndocker volume inspect gpustack-data\n</code></pre> <p>If you need to change it to a custom path, modify the mount configuration when running container.</p> <p>For example, to mount the host directory <code>/data/model-cache</code>:</p> <pre><code>sudo docker run -d --name gpustack \\\n    ...\n    --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /data/model-cache:/var/lib/gpustack/cache \\\n    ...\n    gpustack/gpustack\n</code></pre>"},{"location":"faq/#managing-models","title":"Managing Models","text":""},{"location":"faq/#how-can-i-deploy-the-model-from-hugging-face","title":"How can I deploy the model from Hugging Face?","text":"<p>To deploy models from Hugging Face, the server node and the worker nodes where the model instances are scheduled must have access to Hugging Face, or you can use a mirror.</p> <p>For example, configure the <code>hf-mirror.com</code> mirror:</p> <pre><code>sudo docker run -d --name gpustack \\\n+    -e HF_ENDPOINT=https://hf-mirror.com \\\n    ...\n    gpustack/gpustack\n</code></pre>"},{"location":"faq/#how-can-i-deploy-the-model-from-local-path","title":"How can I deploy the model from Local Path?","text":"<p>When deploying models from Local Path, it is recommended to upload the model files to each node and maintain the same absolute path.</p> <p>Another option is to mount a shared storage across multiple nodes.</p> <p>And the model files must be mounted into the container, and the host directory and the container mount path must be identical.</p> <p>When deploying Safetensors models from Local Path, the path must point to the absolute path of the model directory which contain <code>*.safetensors</code>, <code>config.json</code>, and other files.</p> <p>When deploying GGUF models from Local Path, the path must point to the absolute path of the <code>.gguf</code> file. For sharded model files, use the absolute path of the first <code>.gguf</code> file (00001).</p>"},{"location":"faq/#what-should-i-do-if-the-model-is-stuck-in-pending-state","title":"What should I do if the model is stuck in <code>Pending</code> state?","text":"<p><code>Pending</code> means that there are currently no workers meeting the model\u2019s requirements, move the mouse over the <code>Pending</code> status to view the reason.</p> <p>First, check the <code>Workers</code> section to ensure that the worker status is Ready.</p> <p>Then, each backend has its own handling logic. For example, for vLLM:</p> <p>vLLM requires that all GPUs have more than 90% of their memory available by default (controlled by the <code>--gpu-memory-utilization</code> parameter). Ensure that there is enough allocatable GPU memory exceeding 90%. Note that even if other models are in an <code>Error</code> or <code>Downloading</code> state, the GPU memory has already been allocated.</p> <p>If all GPUs have more than 90% available memory but still show <code>Pending</code>, it indicates insufficient memory. For <code>safetensors</code> models, the required GPU memory (GB) can be estimated as:</p> <pre><code>GPU Memory (GB) = Model weight size (GB) * 1.2 + 2\n</code></pre> <p>If the allocatable GPU memory is less than 90%, but you are sure the model can run with a lower allocation, you can adjust the <code>--gpu-memory-utilization</code> parameter. For example, add <code>--gpu-memory-utilization=0.5</code> in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Parameters</code> to allocate 50% of the GPU memory.</p> <p>Note: If the model encounters an error after running and the logs show <code>CUDA: out of memory</code>, it means the allocated GPU memory is insufficient. You will need to further adjust <code>--gpu-memory-utilization</code>, add more resources, or deploy a smaller model.</p> <p>The context size for the model also affects the required GPU memory. You can adjust the <code>--max-model-len</code> parameter to set a smaller context. In GPUStack, if this parameter is not set, its default value is 8192. If it is specified in the backend parameters, the actual setting will take effect.</p> <p>You can adjust it to a smaller context as needed, for example, <code>--max-model-len=2048</code>. However, keep in mind that the max tokens for each inference request cannot exceed the value of <code>--max-model-len</code>. Therefore, setting a very small context may cause inference truncation.</p> <p>The <code>--enforce-eager</code> parameter also helps reduce GPU memory usage. However, this parameter in vLLM forces the model to execute in eager execution mode, meaning that operations are executed immediately as they are called, rather than being deferred for optimization in graph-based execution (like in lazy execution). This can make the execution slower but easier to debug. However, it can also reduce performance due to the lack of optimizations provided by graph execution.</p> <p>SGLang and MindIE follow a similar process, differing only in their parameters. For more information, see the Built-in Inference Backends section.</p>"},{"location":"faq/#what-should-i-do-if-the-model-is-stuck-in-scheduled-state","title":"What should I do if the model is stuck in <code>Scheduled</code> state?","text":"<p>Try restarting the GPUStack container where the model is scheduled. If the issue persists, check the worker logs here to analyze the cause.</p>"},{"location":"faq/#what-should-i-do-if-the-model-is-stuck-in-error-state","title":"What should I do if the model is stuck in <code>Error</code> state?","text":"<p>Move the mouse over the <code>Error</code> status to view the reason. If there is a <code>View More</code> button, click it to check the error messages in the model logs and analyze the cause of the error.</p>"},{"location":"faq/#why-doesnt-deleting-a-model-free-up-disk-space","title":"Why doesn\u2019t deleting a model free up disk space?","text":"<p>This is to avoid re-downloading the model when redeploying. You need to clean it up in <code>Model Files</code> manually.</p>"},{"location":"faq/#using-models","title":"Using Models","text":""},{"location":"faq/#how-can-i-resolve-the-error-at-most-1-images-may-be-provided-in-one-request","title":"How can I resolve the error: At most 1 image(s) may be provided in one request?","text":"<p>This is a limitation of vLLM. You can adjust the <code>--limit-mm-per-prompt</code> parameter in <code>Edit Model \u2192 Advanced \u2192 Backend Parameters</code> as needed. For example, <code>--limit-mm-per-prompt={\"image\": 4}</code> means that it supports up to 4 images per inference request, see the details here.</p>"},{"location":"faq/#managing-gpustack","title":"Managing GPUStack","text":""},{"location":"faq/#how-do-i-use-gpustack-behind-a-proxy","title":"How do I use GPUStack behind a proxy?","text":"<p>Pass environment variables when running GPUStack:</p> <pre><code>sudo docker run run -d --name gpustack \\\n    -e HTTP_PROXY=\"http://username:password@proxy-server:port\" \\\n    -e HTTPS_PROXY=\"http://username:password@proxy-server:port\" \\\n    -e ALL_PROXY=\"socks5://username:password@proxy-server:port\" \\\n    -e NO_PROXY=\"localhost,127.0.0.1,192.168.0.0/24,172.16.0.0/16,10.0.0.0/8\" \\\n    ...\n</code></pre>"},{"location":"migration/","title":"Migrating from v0.7 and Earlier Versions to v2","text":"<p>Note</p> <p>Since v2.0.0, GPUStack supports Linux only. For other OS, move the data directory to a Linux system and run the migration.</p>"},{"location":"migration/#embedded-database-migration-sqlite-postgresql","title":"Embedded Database Migration (SQLite \u2192 PostgreSQL)","text":"<p>In v0.7 and earlier, GPUStack used an embedded SQLite database by default to store management data. Starting from v2.0.0, GPUStack dropped SQLite support and now uses an embedded PostgreSQL database by default for improved performance and scalability.</p> <p>If you previously deployed GPUStack with the embedded SQLite database, follow the steps below to migrate your data to the new PostgreSQL-based format.</p> <p>Warning</p> <p>Backup First: Before starting the migration, it\u2019s strongly recommended to back up your database.</p> <p>For default installations on v0.7 or earlier, stop the GPUStack server and create a backup of data dir located inside the container at:</p> <pre><code>/var/lib/gpustack\n</code></pre>"},{"location":"migration/#migration-steps","title":"Migration Steps","text":""},{"location":"migration/#identify-your-legacy-data-directory","title":"Identify Your Legacy Data Directory","text":"<p>Locate the data directory used by your previous GPUStack installation. The default path is:</p> <pre><code>/var/lib/gpustack\n</code></pre> <p>For other installation methods, refer to this link to locate the data directory.</p> <p>In the following steps, this path is referenced as <code>${your-data-dir}</code>.</p>"},{"location":"migration/#migrate-using-docker","title":"Migrate Using Docker","text":"<ul> <li> <p>Server Migration (NVIDIA GPUs)</p> <p>If you are using NVIDIA GPUs, run the following Docker command to start the migration. Replace <code>${your-data-dir}</code> with your legacy data directory containing the original SQLite database and related files.</p> <p>By mounting <code>${your-data-dir}</code> to <code>/var/lib/gpustack</code> and setting the environment variable <code>GPUSTACK_MIGRATION_DATA_DIR</code>, GPUStack to automatically migrate the SQLite data to the new embedded PostgreSQL database during startup.</p> <pre><code>sudo docker run -d --name gpustack-server \\\n    --restart=unless-stopped \\\n    --privileged \\\n    --network=host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n+   --env GPUSTACK_MIGRATION_DATA_DIR=/var/lib/gpustack \\\n+   --volume ${your-data-dir}:/var/lib/gpustack \\\n    --runtime nvidia \\\n    gpustack/gpustack\n</code></pre> <p>This command will launch the GPUStack server in Docker, preserving and migrating your existing data.</p> </li> <li> <p>Worker Migration (NVIDIA GPUs)</p> <p>For worker nodes, replace <code>${your-data-dir}</code> with the legacy worker data directory path. Use the following command:</p> <pre><code>sudo docker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    --privileged \\\n    --network=host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n+   --volume ${your-data-dir}:/var/lib/gpustack \\\n    --runtime nvidia \\\n    gpustack/gpustack \\\n    --server-url ${server-url} \\\n    --token ${token}\n</code></pre> <p>This will launch the GPUStack worker using your existing data and connect it to the specified server.</p> </li> <li> <p>Other GPU Architectures</p> <p>For architectures other than NVIDIA (e.g., AMD, Ascend), the migration process remains the same. To migrate on these platforms:</p> <ol> <li> <p>Get the installation commands, please refer to the commands in the Installation Documentation.</p> </li> <li> <p>Mount the legacy data directory to <code>/var/lib/gpustack</code>.</p> </li> <li> <p>(Server Only) Add the environment variable <code>GPUSTACK_MIGRATION_DATA_DIR</code> as shown in the NVIDIA examples, only the server needs to add this environment variable.</p> </li> </ol> <p>The Server and Worker migration commands can be used directly after applying these changes.</p> </li> </ul>"},{"location":"migration/#recreating-model-instances","title":"Recreating Model Instances","text":"<p>After the upgrade is complete, existing Model Instances may remain stuck in the <code>Starting</code> state. If this happens, recreating the Model Instance will allow the model to run normally.</p>"},{"location":"migration/#external-database-migration","title":"External Database Migration","text":"<p>GPUStack supports using an external database to store the management data. If you previously deployed GPUStack with an external database, follow the steps below to migrate your data.</p> <p>Warning</p> <p>Backup Required. Before proceeding, back up the following:</p> <ul> <li> <p>The data directory used by your previous GPUStack installation, typically located at:</p> <pre><code>/var/lib/gpustack\n</code></pre> </li> <li> <p>Your external database, using the backup procedure recommended by your database system.</p> </li> </ul>"},{"location":"migration/#migration-steps_1","title":"Migration Steps","text":""},{"location":"migration/#identify-your-legacy-data-directory_1","title":"Identify Your Legacy Data Directory","text":"<p>Locate the data directory used by your previous GPUStack installation. The default path is:</p> <pre><code>/var/lib/gpustack\n</code></pre> <p>For other installation methods, refer to this link to locate the data directory.</p> <p>In the following steps, this path is referenced as <code>${your-data-dir}</code>.</p>"},{"location":"migration/#migrate-using-docker_1","title":"Migrate Using Docker","text":"<ul> <li> <p>Server Migration (NVIDIA GPUs)</p> <p>Keep the <code>--database-url</code> flag, <code>GPUSTACK_DATABASE_URL</code> environment variable, or <code>database_url</code> in your configuration file exactly as before. It must continue pointing to your existing external database.</p> <p>If you are using NVIDIA GPUs, run the following Docker command to start the migration. Replace <code>${your-data-dir}</code> with your legacy data directory. By mounting <code>${your-data-dir}</code> to <code>/var/lib/gpustack</code>.</p> <pre><code>sudo docker run -d --name gpustack-server \\\n    --restart=unless-stopped \\\n    --privileged \\\n    --network=host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n+   --volume ${your-data-dir}:/var/lib/gpustack \\\n    --runtime nvidia \\\n    gpustack/gpustack \\\n+     --database-url ${your-database-url}\n</code></pre> </li> <li> <p>Worker Migration (NVIDIA GPUs)</p> <p>For worker nodes, replace <code>${your-data-dir}</code> with the legacy worker data directory path. Use the following command:</p> <pre><code>sudo docker run -d --name gpustack-worker \\\n    --restart=unless-stopped \\\n    --privileged \\\n    --network=host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n+   --volume ${your-data-dir}:/var/lib/gpustack \\\n    --runtime nvidia \\\n    gpustack/gpustack \\\n    --server-url ${server-url} \\\n    --token ${token}\n</code></pre> </li> <li> <p>Other GPU Architectures</p> <p>For architectures other than NVIDIA (e.g., AMD, Ascend), the migration process remains the same. To migrate on these platforms:</p> <ol> <li> <p>Get the installation commands, please refer to the commands in the Installation Documentation.</p> </li> <li> <p>Mount the legacy data directory to <code>/var/lib/gpustack</code>.</p> </li> <li> <p>(Server Only) Continue use gpustack start flag <code>--database-url</code> or <code>GPUSTACK_DATABASE_URL</code> environment variable or config file with <code>database_url</code> as before.</p> </li> </ol> </li> </ul>"},{"location":"migration/#recreating-model-instances_1","title":"Recreating Model Instances","text":"<p>After the upgrade is complete, existing Model Instances may remain stuck in the <code>Starting</code> state. If this happens, recreating the Model Instance will allow the model to run normally.</p>"},{"location":"overview/","title":"Overview","text":"<p> Star Watch Fork </p> <p>GPUStack is an open-source GPU cluster manager designed for efficient AI model deployment. It lets you run models efficiently on your own GPU hardware by choosing the best inference engines, scheduling GPU resources, analyzing model architectures, and automatically configuring deployment parameters.</p> <p>The following figure shows how GPUStack delivers improved inference throughput over the unoptimized vLLM baseline:</p> <p></p> <p>For detailed benchmarking methods and results, visit our Inference Performance Lab.</p>"},{"location":"overview/#tested-inference-engines-gpus-and-models","title":"Tested Inference Engines, GPUs, and Models","text":"<p>GPUStack uses a plug-in architecture that makes it easy to add new AI models, inference engines, and GPU hardware. We work closely with partners and the open-source community to test and optimize emerging models across different inference engines and GPUs. Below is the current list of supported inference engines, GPUs, and models, which will continue to expand over time.</p> <p>Tested Inference Engines:</p> <ul> <li>vLLM</li> <li>SGLang</li> <li>TensorRT-LLM</li> <li>MindIE</li> </ul> <p>Tested GPUs:</p> <ul> <li>NVIDIA A100</li> <li>NVIDIA H100/H200</li> <li>Ascend 910B</li> </ul> <p>Tuned Models:</p> <ul> <li>Qwen3</li> <li>gpt-oss</li> <li>GLM-4.5-Air</li> <li>GLM-4.5/4.6</li> <li>DeepSeek-R1</li> </ul>"},{"location":"overview/#architecture","title":"Architecture","text":"<p>GPUStack enables development teams, IT organizations, and service providers to deliver Model-as-a-Service at scale. It supports industry-standard APIs for LLM, voice, image, and video models. The platform includes built-in user authentication and access control, real-time monitoring of GPU performance and utilization, and detailed metering of token usage and API request rates.</p> <p>The figure below illustrates how a single GPUStack server can manage multiple GPU clusters across both on-premises and cloud environments. The GPUStack scheduler allocates GPUs to maximize resource utilization and selects the appropriate inference engines for optimal performance. Administrators also gain full visibility into system health and metrics through integrated Grafana and Prometheus dashboards.</p> <p></p> <p>GPUStack provides a powerful framework for deploying AI models. Its core features include:</p> <ul> <li>Multi-Cluster GPU Management. Manages GPU clusters across multiple environments. This includes on-premises servers, Kubernetes clusters, and cloud providers.</li> <li>Pluggable Inference Engines. Automatically configures high-performance inference engines such as vLLM, SGLang, and TensorRT-LLM. You can also add custom inference engines as needed.</li> <li>Performance-Optimized Configurations. Offers pre-tuned modes for low latency or high throughput. GPUStack supports extended KV cache systems like LMCache and HiCache to reduce TTFT. It also includes built-in support for speculative decoding methods such as EAGLE3, MTP, and N-grams.</li> <li>Enterprise-Grade Operations. Offers support for automated failure recovery, load balancing, monitoring, authentication, and access control.</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#install-gpustack","title":"Install GPUStack","text":"<p>Note</p> <p>GPUStack now supports Linux only.</p> <p>If you are using NVIDIA GPUs, ensure the NVIDIA driver, Docker and NVIDIA Container Toolkit are installed. Then start the GPUStack with the following command:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime nvidia \\\n    gpustack/gpustack\n</code></pre> <p>If you cannot pull images from <code>Docker Hub</code> or the download is very slow, you can use our <code>Quay.io</code> mirror by pointing your registry to <code>quay.io</code>:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime nvidia \\\n    quay.io/gpustack/gpustack \\\n    --system-default-container-registry quay.io\n</code></pre> <p>For more details on the installation or other GPU hardware platforms, please refer to the Installation Requirements.</p> <p>Check the GPUStack startup logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>After GPUStack starts, run the following command to get the default admin password:</p> <pre><code>sudo docker exec gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>Open your browser and navigate to <code>http://your_host_ip</code> to access the GPUStack UI. Use the default username <code>admin</code> and the password you retrieved above to log in.</p>"},{"location":"quickstart/#deploy-a-model","title":"Deploy a Model","text":"<ol> <li> <p>Navigate to the <code>Catalog</code> page in the GPUStack UI.</p> </li> <li> <p>Select the <code>Qwen3 0.6B</code> model from the list of available models.</p> </li> <li> <p>After the deployment compatibility checks pass, click the <code>Save</code> button to deploy the model.</p> </li> </ol> <p></p> <ol> <li>GPUStack will start downloading the model files and deploying the model. When the deployment status shows <code>Running</code>, the model has been deployed successfully.</li> </ol> <p></p> <ol> <li>Click <code>Playground - Chat</code> in the navigation menu, check that the model <code>qwen3-0.6b</code> is selected from the top-right <code>Model</code> dropdown. Now you can chat with the model in the UI playground.</li> </ol> <p></p>"},{"location":"quickstart/#use-the-model-via-api","title":"Use the model via API","text":"<ol> <li> <p>Hover over the user avatar and navigate to the <code>API Keys</code> page, then click the <code>New API Key</code> button.</p> </li> <li> <p>Fill in the <code>Name</code> and click the <code>Save</code> button.</p> </li> <li> <p>Copy the generated API key and save it somewhere safe. Please note that you can only see it once on creation.</p> </li> <li> <p>You can now use the API key to access the OpenAI-compatible API endpoints provided by GPUStack. For example, use curl as the following:</p> </li> </ol> <pre><code># Replace `your_api_key` and `your_gpustack_server_url`\n# with your actual API key and GPUStack server URL.\nexport GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"qwen3-0.6b\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Tell me a joke.\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"quickstart/#cleanup","title":"Cleanup","text":"<p>After you complete using the deployed model, you can go to the <code>Deployments</code> page in the GPUStack UI and delete the model to free up resources.</p>"},{"location":"scheduler/","title":"Scheduler","text":""},{"location":"scheduler/#summary","title":"Summary","text":"<p>The scheduler's primary responsibility is to calculate the resources required by models instance and to evaluate and select the optimal workers/GPUs for model instances through a series of strategies. This ensures that model instances can run efficiently. This document provides a detailed overview of the policies and processes used by the scheduler.</p>"},{"location":"scheduler/#scheduling-process","title":"Scheduling Process","text":""},{"location":"scheduler/#filtering-phase","title":"Filtering Phase","text":"<p>The filtering phase aims to narrow down the available workers or GPUs to those that meet specific criteria. The main policies involved are:</p> <ul> <li>Cluster Matching Policy</li> <li>GPU Matching Policy</li> <li>Label Matching Policy</li> <li>Status Policy</li> <li>Resource Fit Policy</li> </ul>"},{"location":"scheduler/#cluster-matching-policy","title":"Cluster Matching Policy","text":"<p>This policy filters workers based on the cluster configuration of the model. Only those workers that belong to the specified cluster are retained for further evaluation.</p>"},{"location":"scheduler/#gpu-matching-policy","title":"GPU Matching Policy","text":"<p>This policy filters workers based on the user selected GPUs. Only workers that included the selected GPUs are retained for further evaluation.</p>"},{"location":"scheduler/#label-matching-policy","title":"Label Matching Policy","text":"<p>This policy filters workers based on the label selectors configured for the model. If no label selectors are defined for the model, all workers are considered. Otherwise, the system checks whether the labels of each worker node match the model's label selectors, retaining only those workers that match.</p>"},{"location":"scheduler/#status-policy","title":"Status Policy","text":"<p>This policy filters workers based on their status, retaining only those that are in a READY state.</p>"},{"location":"scheduler/#backend-framework-matching-policy","title":"Backend Framework Matching Policy","text":"<p>This policy filters workers based on the backend framework required by the model (e.g., vLLM, SGLang). Only those workers with GPUs that support the specified backend framework are retained for further evaluation.</p>"},{"location":"scheduler/#resource-fit-policy","title":"Resource Fit Policy","text":"<p>The Resource Fit Policy is a critical strategy in the scheduling system, used to filter workers or GPUs based on resource compatibility. The goal of this policy is to ensure that model instances can run on the selected workers. The Resource Fit Policy prioritizes candidates in the following order:</p> <p>Resource requirements are determined based on:</p> <ul> <li> <p>For GGUF models: Uses the GGUF parser to estimate the model's resource requirements.</p> </li> <li> <p>For other model types: Estimated by the backend (e.g., vLLM, SGLang, MindIE, VoxBox).</p> </li> </ul> <p>Backends have different capabilities:</p> <ul> <li> <p>vLLM, SGLang, MindIE: GPU-only, no CPU or partial offload.</p> </li> <li> <p>Custom backends, VoxBox: Support GPU offload or CPU execution.</p> </li> </ul> <p>Candidates are evaluated in the following order, and the process stops once the first valid placement is found:</p> <ol> <li> <p>Single Worker, Single GPU (Full Fit) A single GPU fully satisfies the model\u2019s requirements.</p> </li> <li> <p>Single Worker, Multiple GPUs (Full Fit) Multiple GPUs on the same worker jointly satisfy the requirements.</p> </li> <li> <p>Distributed Inference (Across Workers) GPUs across multiple workers can be used when the backend supports distributed execution.</p> </li> <li> <p>Single Worker, CPU Execution CPU-only execution, supported only for Custom and VoxBox backends.</p> </li> </ol>"},{"location":"scheduler/#scoring-phase","title":"Scoring Phase","text":"<p>The scoring phase evaluates the filtered candidates, scoring them to select the optimal deployment location. The primary strategy involved is:</p> <ul> <li>Placement Strategy Policy</li> </ul>"},{"location":"scheduler/#placement-strategy-policy","title":"Placement Strategy Policy","text":"<ul> <li>Binpack</li> </ul> <p>This strategy aims to \"pack\" as many model instances as possible into the fewest number of \"bins\" (e.g., Workers/GPUs) to optimize resource utilization. The goal is to minimize the number of bins used while maximizing resource efficiency, ensuring each bin is filled as efficiently as possible without exceeding its capacity. Model instances are placed in the bin with the least remaining space to minimize leftover capacity in each bin.</p> <ul> <li>Spread</li> </ul> <p>This strategy seeks to distribute multiple model instances across different workers as evenly as possible, improving system fault tolerance and load balancing.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#view-gpustack-logs","title":"View GPUStack Logs","text":"<p>You can view GPUStack logs with the following commands for the default setup:</p> <pre><code>docker logs -f gpustack\n</code></pre>"},{"location":"troubleshooting/#enable-debug-mode","title":"Enable Debug Mode","text":"<p>You can enable the <code>DEBUG</code> mode by setting the <code>--debug</code> flag when running GPUStack:</p> <pre><code>sudo docker run -d --name gpustack \\\n    ...\n    gpustack/gpustack \\\n+    --debug\n    ...\n</code></pre> <p>You can also enable GPUStack's debug mode at runtime by running the following command inside the server container:</p> <pre><code>gpustack reload-config --set debug=true\n</code></pre>"},{"location":"troubleshooting/#configure-log-level","title":"Configure Log Level","text":"<p>You can configure log level of the GPUStack server at runtime by running the following command inside the server container:</p> <pre><code>curl -X PUT http://localhost/debug/log_level -d \"debug\"\n</code></pre> <p>The same applies to GPUStack workers:</p> <pre><code>curl -X PUT http://localhost:10150/debug/log_level -d \"debug\"\n</code></pre> <p>The available log levels are: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>.</p>"},{"location":"troubleshooting/#reset-admin-password","title":"Reset Admin Password","text":"<p>In case you forgot the admin password, you can reset it by running the following command inside the server container:</p> <pre><code>gpustack reset-admin-password\n</code></pre> <p>If you changed the default port using <code>--port</code> when starting GPUStack, specify the GPUStack URL using the <code>--server-url</code> parameter. It must be run locally on the server and accessed via <code>localhost</code>:</p> <pre><code>gpustack reset-admin-password --server-url http://localhost:9090\n</code></pre>"},{"location":"upgrade/","title":"Upgrade","text":"<p>You can upgrade GPUStack by pulling and running a newer Docker image.</p> <p>The following upgrade instructions apply only to GPUStack v2.0 and later.</p> <p>For installations prior to v0.7, please refer to the migration guide.</p> <p>Note</p> <ol> <li> <p>When upgrading, upgrade the GPUStack server first, then upgrade the workers.</p> </li> <li> <p>Please DO NOT upgrade from/to the main(dev) version or a release candidate(rc) version, as they may contain breaking changes. Use a fresh installation if you want to try the main or rc versions.</p> </li> </ol> <p>Warning</p> <p>Backup First: Before proceeding with an upgrade, it\u2019s strongly recommended to back up your database.</p> <p>For default installations, stop the GPUStack server and create a backup of the PostgreSQL database directory located inside the container at:</p> <pre><code>/var/lib/gpustack/postgresql/data\n</code></pre> <p>You can upgrade by pulling a new image (either a specific version tag or the latest tag), removing the old container, and starting a new one with the updated image.</p> <p>For example:</p> <pre><code>docker pull gpustack/gpustack:latest  # or: docker pull gpustack/gpustack:vx.y.z\n\ndocker stop gpustack\ndocker rm gpustack\n\ndocker run -d --name gpustack \\\n  ... \\\n  gpustack/gpustack:latest\n  ...\n</code></pre>"},{"location":"cli-reference/copy-images/","title":"gpustack copy-images","text":"<p>Copy images to other registry, powered by https://github.com/containers/skopeo.</p> <pre><code>gpustack copy-images [OPTIONS]\n</code></pre>"},{"location":"cli-reference/copy-images/#configurations","title":"Configurations","text":"Flag Default Description <code>--backend</code> <code>{cann,corex,cuda,dtk,maca,musa,neuware,rocm}</code> (empty) Filter gpustack/runner images by backend name <code>--backend-version</code> <code>BACKEND_VERSION</code> (empty) Filter gpustack/runner images by exact backend version <code>--backend-version-prefix</code> <code>BACKEND_VERSION_PREFIX</code> (empty) Filter gpustack/runner images by backend version prefix <code>--backend-variant</code> <code>BACKEND_VARIANT</code> (empty) Filter gpustack/runner images by backend variant <code>--service</code> <code>{voxbox,vllm,mindie,sglang}</code> (empty) Filter gpustack/runner images by service name <code>--service-version</code> <code>SERVICE_VERSION</code> (empty) Filter gpustack/runner images by exact service version <code>--service-version-prefix</code> <code>SERVICE_VERSION_PREFIX</code> (empty) Filter gpustack/runner images by service version prefix <code>--repository</code> <code>REPOSITORY</code> (empty) Filter images by repository name <code>--platform</code> <code>{linux/amd64,linux/arm64}</code> (empty) Filter images by platform <code>--max-workers</code> <code>MAX_WORKERS</code> <code>1</code> Maximum number of worker threads to use for copying images concurrently <code>--max-retries</code> <code>MAX_RETRIES</code> <code>1</code> Maximum number of retries for copying an image <code>--source</code>, <code>--src</code> <code>SOURCE</code> <code>docker.io</code> Source registry <code>--source-namespace</code>, <code>--src-namespace</code> <code>SOURCE_NAMESPACE</code> (empty) Source namespace in the source registry; for multi-level namespaces, specify parent levels to <code>--source</code> and child to <code>--source-namespace</code> <code>--source-username</code>, <code>--src-user</code> <code>SOURCE_USERNAME</code> (env: <code>SOURCE_USERNAME</code>) Username for source registry authentication <code>--source-password</code>, <code>--src-passwd</code> <code>SOURCE_PASSWORD</code> (env: <code>SOURCE_PASSWORD</code>) Password/Token for source registry authentication <code>--destination</code>, <code>--dest</code> <code>DESTINATION</code> <code>docker.io</code> Destination registry <code>--destination-namespace</code>, <code>--dest-namespace</code> <code>DESTINATION_NAMESPACE</code> (empty) Destination namespace in the destination registry; for multi-level namespaces, specify parent levels to <code>--destination</code> and child to <code>--destination-namespace</code> <code>--destination-username</code>, <code>--dest-user</code> <code>DESTINATION_USERNAME</code> (env: <code>DESTINATION_USERNAME</code>) Username for destination registry authentication <code>--destination-password</code>, <code>--dest-passwd</code> <code>DESTINATION_PASSWORD</code> (env: <code>DESTINATION_PASSWORD</code>) Password/Token for destination registry authentication"},{"location":"cli-reference/download-tools/","title":"gpustack download-tools","text":"<p>Download dependency tools, including gguf-parser, and fastfetch.</p> <pre><code>gpustack download-tools [OPTIONS]\n</code></pre>"},{"location":"cli-reference/download-tools/#configurations","title":"Configurations","text":"Flag Default Description <code>----tools-download-base-url</code> value (empty) Base URL to download dependency tools. <code>--save-archive</code> value (empty) Path to save downloaded tools as a tar archive. <code>--load-archive</code> value (empty) Path to load downloaded tools from a tar archive, instead of downloading. <code>--system</code> value Default is the current OS. Operating system to download tools for. Options: <code>linux</code>, <code>windows</code>, <code>macos</code>. <code>--arch</code> value Default is the current architecture. Architecture to download tools for. Options: <code>amd64</code>, <code>arm64</code>."},{"location":"cli-reference/list-images/","title":"gpustack list-images","text":"<p>List images.</p> <pre><code>gpustack list-images [OPTIONS]\n</code></pre>"},{"location":"cli-reference/list-images/#configurations","title":"Configurations","text":"Flag Default Description <code>--backend</code> <code>{cann,corex,cuda,dtk,maca,musa,neuware,rocm}</code> (empty) Filter gpustack/runner images by backend name <code>--backend-version</code> <code>BACKEND_VERSION</code> (empty) Filter gpustack/runner images by exact backend version <code>--backend-version-prefix</code> <code>BACKEND_VERSION_PREFIX</code> (empty) Filter gpustack/runner images by backend version prefix <code>--backend-variant</code> <code>BACKEND_VARIANT</code> (empty) Filter gpustack/runner images by backend variant <code>--service</code> <code>{voxbox,vllm,mindie,sglang}</code> (empty) Filter gpustack/runner images by service name <code>--service-version</code> <code>SERVICE_VERSION</code> (empty) Filter gpustack/runner images by exact service version <code>--service-version-prefix</code> <code>SERVICE_VERSION_PREFIX</code> (empty) Filter gpustack/runner images by service version prefix <code>--repository</code> <code>REPOSITORY</code> (empty) Filter images by repository name <code>--platform</code> <code>{linux/amd64,linux/arm64}</code> (empty) Filter images by platform <code>--format {text,json}</code> <code>text</code> Output format. <code>text</code> for human-readable text format, <code>json</code> for JSON array format"},{"location":"cli-reference/reload-config/","title":"gpustack reload-config","text":"<p>Reload runtime-safe configuration values.</p> <pre><code>gpustack reload-config [OPTIONS]\n</code></pre>"},{"location":"cli-reference/reload-config/#configurations","title":"Configurations","text":"Flag Default Description <code>--set</code> value (empty) Set a single whitelisted field using <code>key=value</code> (keys in hyphen-case). Values are coerced by field type: booleans accept <code>true/false</code>, <code>1/0</code>, <code>yes/no</code>, <code>y/n</code>, <code>on/off</code>; lists accept comma-separated strings; dicts require JSON strings. Multiple <code>--set</code> flags are allowed; later ones override earlier ones and override <code>--file</code> values. <code>--file</code> value (empty) Load configuration from a YAML file. Only whitelisted fields are applied. Keys are normalized to snake_case. Values provided via <code>--set</code> override those from the file. <code>--list</code> <code>False</code> Show whitelisted fields and values. When present, other options are ignored. <code>--api-key</code> value (empty) When force-auth-localhost is enabled, provide an API key for server-side authentication as an admin user. <code>--server-port</code> value <code>8080</code> Target port of the GPUStack API server for applying or listing runtime config. When omitted, defaults to <code>GPUSTACK_API_PORT</code> if set, otherwise <code>8080</code>. <code>--worker-port</code> value <code>10150</code> Target port of the GPUStack worker for applying or listing runtime config. When omitted, defaults to <code>GPUSTACK_WORKER_PORT</code> if set, otherwise <code>10150</code>."},{"location":"cli-reference/save-images/","title":"gpustack save-images","text":"<p>Save images as Docker Archive to local path, powered by https://github.com/containers/skopeo.</p> <pre><code>gpustack save-images [OPTIONS] [output]\n</code></pre>"},{"location":"cli-reference/save-images/#configurations","title":"Configurations","text":"Flag Default Description <code>--backend</code> <code>{cann,corex,cuda,dtk,maca,musa,neuware,rocm}</code> (empty) Filter gpustack/runner images by backend name <code>--backend-version</code> <code>BACKEND_VERSION</code> (empty) Filter gpustack/runner images by exact backend version <code>--backend-version-prefix</code> <code>BACKEND_VERSION_PREFIX</code> (empty) Filter gpustack/runner images by backend version prefix <code>--backend-variant</code> <code>BACKEND_VARIANT</code> (empty) Filter gpustack/runner images by backend variant <code>--service</code> <code>{voxbox,vllm,mindie,sglang}</code> (empty) Filter gpustack/runner images by service name <code>--service-version</code> <code>SERVICE_VERSION</code> (empty) Filter gpustack/runner images by exact service version <code>--service-version-prefix</code> <code>SERVICE_VERSION_PREFIX</code> (empty) Filter gpustack/runner images by service version prefix <code>--repository</code> <code>REPOSITORY</code> (empty) Filter images by repository name <code>--platform</code> <code>{linux/amd64,linux/arm64}</code> (empty) Filter images by platform <code>--max-workers</code> <code>MAX_WORKERS</code> <code>1</code> Maximum number of worker threads to use for copying images concurrently <code>--max-retries</code> <code>MAX_RETRIES</code> <code>1</code> Maximum number of retries for copying an image <code>--source</code>, <code>--src</code> <code>SOURCE</code> <code>docker.io</code> Source registry <code>--source-namespace</code>, <code>--src-namespace</code> <code>SOURCE_NAMESPACE</code> (empty) Source namespace in the source registry; for multi-level namespaces, specify parent levels to <code>--source</code> and child to <code>--source-namespace</code> <code>--source-username</code>, <code>--src-user</code> <code>SOURCE_USERNAME</code> (env: <code>SOURCE_USERNAME</code>) Username for source registry authentication <code>--source-password</code>, <code>--src-passwd</code> <code>SOURCE_PASSWORD</code> (env: <code>SOURCE_PASSWORD</code>) Password/Token for source registry authentication"},{"location":"cli-reference/start/","title":"gpustack start","text":"<p>Run GPUStack server or worker.</p> <pre><code>gpustack start [OPTIONS]\n</code></pre>"},{"location":"cli-reference/start/#configurations","title":"Configurations","text":""},{"location":"cli-reference/start/#common-options","title":"Common Options","text":"Flag Default Description <code>--advertise-address</code> value (empty) The IP address to expose for external access.If not set, the system will auto-detect a suitable local IP address. <code>--port</code> value 80 Port to bind the server to. <code>--tls-port</code> value 443 Port to bind the TLS server to. <code>--api-port</code> value <code>8080</code> Port to bind the GPUStack API server to. <code>--config-file</code> value (empty) Path to the YAML config file. <code>-d</code> value, <code>--debug</code> value <code>False</code> To enable debug mode, the short flag -d is not supported in Windows because this flag is reserved by PowerShell for CommonParameters. <code>--data-dir</code> value (empty) Directory to store data. Default is OS specific. <code>--cache-dir</code> value (empty) Directory to store cache (e.g., model files). Defaults to /cache. <code>--huggingface-token</code> value (empty) User Access Token to authenticate to the Hugging Face Hub. Can also be configured via the <code>HF_TOKEN</code> environment variable. <code>--bin-dir</code> value (empty) Directory to store additional binaries, e.g., versioned backend executables. <code>--pipx-path</code> value (empty) Path to the pipx executable, used to install versioned backends. <code>--system-default-container-registry</code> value <code>docker.io</code> Default container registry for GPUStack to pull system and inference images. <code>--image-name-override</code> value (empty) Override the default image name for the GPUStack container. <code>--image-repo</code> value <code>gpustack/gpustack</code> Override the default image repository for the GPUStack container. <code>--gateway-mode</code> value <code>auto</code> Gateway running mode. Options: embedded, in-cluster, external, disabled, or auto (default). <code>--gateway-kubeconfig</code> value (empty) Path to the kubeconfig file for gateway. Only useful for external gateway-mode. <code>--gateway-concurrency</code> value <code>16</code> Number of concurrent connections for the gateway. <code>--service-discovery-name</code> value (empty) The name of the service discovery service in DNS. Only useful when deployed in Kubernetes with service discovery. <code>--namespace</code> value (empty) Kubernetes namespace for GPUStack to deploy gateway routing rules and model instances."},{"location":"cli-reference/start/#server-options","title":"Server Options","text":"Flag Default Description <code>--database-port</code> value <code>5432</code> Port of the embedded PostgresSQL database. <code>--metrics-port</code> value <code>10161</code> Port to expose server metrics. <code>--disable-metrics</code> <code>False</code> Disable server metrics. <code>--disable-worker</code> <code>False</code> Disable built-in worker. <code>--bootstrap-password</code> value Auto-generated. Initial password for the default admin user. <code>--database-url</code> value Embedded PostgreSQL. URL of the database. Supports PostgreSQL 13.0+, and MySQL 8.0+. Example: postgresql://user:password@host:port/db_name or mysql://user:password@host:port/db_name <code>--ssl-keyfile</code> value (empty) Path to the SSL key file. <code>--ssl-certfile</code> value (empty) Path to the SSL certificate file. <code>--force-auth-localhost</code> <code>False</code> Force authentication for requests originating from localhost (127.0.0.1). When set to True, all requests from localhost will require authentication. <code>--disable-update-check</code> <code>False</code> Disable update check. <code>--disable-openapi-docs</code> <code>False</code> Disable autogenerated OpenAPI documentation endpoints (Swagger UI at /docs, ReDoc at /redoc, and the raw spec at /openapi.json). <code>--model-catalog-file</code> value (empty) Path or URL to the model catalog file. <code>--enable-cors</code> <code>False</code> Enable Cross-Origin Resource Sharing (CORS) on the server. <code>--allow-credentials</code> <code>False</code> Allow cookies and credentials in cross-origin requests. <code>--allow-origins</code> value <code>[\"*\"]</code> Origins allowed for cross-origin requests. Specify the flag multiple times for multiple origins. Example: <code>--allow-origins https://example.com --allow-origins https://api.example.com</code> <code>--allow-methods</code> value <code>[\"GET\", \"POST\"]</code> HTTP methods allowed in cross-origin requests. Specify the flag multiple times for multiple methods. Example: <code>--allow-methods GET --allow-methods POST</code> <code>--allow-headers</code> value <code>[\"Authorization\", \"Content-Type\"]</code> HTTP request headers allowed in cross-origin requests. Specify the flag multiple times for multiple headers. Example: <code>--allow-headers Authorization --allow-headers Content-Type</code> <code>--oidc-issuer</code> value (empty) OpenID Connect issuer URL. <code>--oidc-client-id</code> value (empty) OpenID Connect client ID. <code>--oidc-client-secret</code> value (empty) OpenID Connect client secret. <code>--oidc-redirect-uri</code> value (empty) The redirect URI configured in your OIDC application. This must be set to <code>&lt;server-url&gt;/auth/oidc/callback</code>. <code>--saml-idp-server-url</code> value (empty) SAML Identity Provider server URL. <code>--saml-idp-entity-id</code> value (empty) SAML Identity Provider entity ID. <code>--saml-idp-x509-cert</code> value (empty) SAML Identity Provider X.509 certificate. <code>--saml-sp-entity-id</code> value (empty) SAML Service Provider entity ID. <code>--saml-sp-acs-url</code> value (empty) SAML Service Provider Assertion Consumer Service URL. This must be set to <code>&lt;server-url&gt;/auth/saml/callback</code>. <code>--saml-sp-x509-cert</code> value (empty) SAML Service Provider X.509 certificate. <code>--saml-sp-private-key</code> value (empty) SAML Service Provider private key. <code>--saml-security</code> value (empty) SAML security settings in JSON format. <code>--external-auth-name</code> value (empty) Mapping of external authentication user information to username, e.g., preferred_username. <code>--external-auth-full-name</code> value (empty) Mapping of external authentication user information to user's full name. Multiple elements can be combined, e.g., <code>name</code> or <code>firstName+lastName</code>. <code>--external-auth-avatar-url</code> value (empty) Mapping of external authentication user information to user's avatar URL. <code>--server-external-url</code> value (empty) The external server URL for worker registration. This option is required when provisioning workers via cloud providers, ensuring that workers can connect to the server correctly. <code>--saml-sp-attribute-prefix</code> value (empty) SAML Service Provider attribute prefix, used for fetching attributes specified by --external-auth-*. <code>--oidc-use-userinfo</code> <code>False</code> Use the UserInfo endpoint to fetch user details after authentication."},{"location":"cli-reference/start/#worker-options","title":"Worker Options","text":"Flag Default Description <code>-t</code> value, <code>--token</code> value Auto-generated. Shared secret used to register worker. <code>-s</code> value, <code>--server-url</code> value (empty) Server to connect to. <code>--worker-name</code> value (empty) Name of the worker node. Use the hostname by default. <code>--worker-ip</code> value (empty) Deprecated, use advertise-address instead. <code>--disable-worker-metrics</code> <code>False</code> Disable metrics. <code>--worker-metrics-port</code> value <code>10151</code> Port to expose metrics. <code>--worker-port</code> value <code>10150</code> Port to bind the worker to. Use a consistent value for all workers. <code>--service-port-range</code> value <code>40000-40063</code> Port range for inference services, specified as a string in the form 'N1-N2'. Both ends of the range are inclusive. <code>--ray-port-range</code> value <code>41000-41999</code> Port range for Ray services(vLLM distributed deployment using), specified as a string in the form 'N1-N2'. Both ends of the range are inclusive. <code>--log-dir</code> value (empty) Directory to store logs. <code>--system-reserved</code> value <code>\"{\\\"ram\\\": 2, \\\"vram\\\": 1}\"</code> The system reserves resources for the worker during scheduling, measured in GiB. By default, 2 GiB of RAM and 1G of VRAM is reserved, Note: '{\\\"memory\\\": 2, \\\"gpu_memory\\\": 1}' is also supported, but it is deprecated and will be removed in future releases. <code>--tools-download-base-url</code> value Base URL for downloading dependency tools. <code>--enable-hf-transfer</code> <code>False</code> Enable faster downloads from the Hugging Face Hub using hf_transfer. https://huggingface.co/docs/huggingface_hub/v0.29.3/package_reference/environment_variables#hfhubenablehftransfer <code>--enable-hf-xet</code> <code>False</code> Enable downloading model files using Hugging Face Xet. <code>--worker-ifname</code> value (empty) Network interface name of the worker node. Auto-detected by default."},{"location":"cli-reference/start/#available-environment-variables","title":"Available Environment Variables","text":"<p>Most command line parameters can also be set via environment variables with the <code>GPUSTACK_</code> prefix and in uppercase format (e.g., <code>--data-dir</code> can be set via <code>GPUSTACK_DATA_DIR</code>). </p> <p>For environment variables beyond the command-line parameters mentioned above, please refer to the environment variables documentation.</p>"},{"location":"cli-reference/start/#config-file","title":"Config File","text":"<p>You can configure start options using a YAML-format config file when starting GPUStack server or worker. Here is a complete example:</p> <pre><code># Common Options\nport: 80\ntls_port: 443\nadvertise_address: exposed_server_or_worker_ip\ndebug: false\ndata_dir: /path/to/data_dir\ncache_dir: /path/to/cache_dir\ntoken: your_token\nhuggingface_token: your_huggingface_token\n\n# Server Options\napi_port: 8080\nmetrics_port: 10161\ndisable_worker: false\nbootstrap_password: your_admin_password\ndatabase_url: postgresql://user:password@host:port/db_name\n# database_url: mysql://user:password@host:port/db_name\nssl_keyfile: /path/to/keyfile\nssl_certfile: /path/to/certfile\nforce_auth_localhost: false\ndisable_update_check: false\ndisable_openapi_docs: false\nmodel_catalog_file: /path_or_url/to/model_catalog_file\nenable_cors: false\nallow_credentials: false\nallow_origins: [\"*\"]\nallow_methods: [\"GET\", \"POST\"]\nallow_headers: [\"Authorization\", \"Content-Type\"]\noidc_issuer: https://your_oidc_issuer\noidc_client_id: your_oidc_client_id\noidc_client_secret: your_oidc_client_secret\noidc_redirect_uri: http://your_gpustack_server_url/auth/oidc/callback\nsaml_idp_server_url: https://your_saml_idp_server_url\nsaml_idp_entity_id: your_saml_idp_entity_id\nsaml_idp_x509_cert: your_saml_idp_x509_cert_pem\nsaml_sp_entity_id: your_saml_sp_entity_id\nsaml_sp_acs_url: http://your_gpustack_server_url/auth/saml/callback\nsaml_sp_x509_cert: your_saml_sp_x509_cert_pem\nsaml_sp_private_key: your_saml_sp_private_key_pem\nsaml_security: '{\"wantAssertionsSigned\": true, \"wantMessagesSigned\": true}'\nexternal_auth_name: email\nexternal_auth_full_name: name\nexternal_auth_avatar_url: picture\nserver_external_url: http://your_gpustack_server_url_for_external_access\n\n# Worker Options\nserver_url: http://your_gpustack_server_url\nworker_name: your_worker_name\nworker_ip: 192.168.1.101\ndisable_metrics: false\nworker_metrics_port: 10151\nworker_port: 10150\nservice_port_range: 40000-40063\nray_port_range: 41000-41999\nlog_dir: /path/to/log_dir\nsystem_reserved:\n  ram: 2\n  vram: 1\ntools_download_base_url: https://mirror.your_company.com\nenable_hf_transfer: false\nenable_hf_xet: false\n</code></pre>"},{"location":"installation/air-gapped/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment, which means setting up GPUStack offline without internet access.</p>"},{"location":"installation/air-gapped/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/air-gapped/#driver","title":"Driver","text":"<p>Ensure your system has the appropriate GPU drivers installed for your hardware.</p> <p>See the Installation Requirements for details on driver compatibility.</p>"},{"location":"installation/air-gapped/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker.</p> <p>If your system supports a container toolkit, install and configure it as needed (e.g., NVIDIA Container Toolkit, AMD ROCm Container Toolkit, etc.).</p>"},{"location":"installation/air-gapped/#container-images","title":"Container Images","text":"<ul> <li>Copy Images</li> </ul> <p>GPUStack provides various container images for different inference backends, available on Docker Hub.</p> <p>To transfer the required container images to your internal registry from a machine with internet access, use the GPUStack <code>copy-images</code> command:</p> <pre><code>sudo docker run --rm -it --entrypoint \"\" gpustack/gpustack \\\n    gpustack copy-images \\\n    --destination &lt;your_internal_registry&gt; \\\n    --destination-username &lt;your_username&gt; \\\n    --destination-password &lt;your_password&gt;\n</code></pre> <p>If you cannot pull images from <code>Docker Hub</code> or the download is very slow, you can use our <code>Quay.io</code> mirror by pointing the source registry to <code>quay.io</code>:</p> <pre><code>sudo docker run --rm -it --entrypoint \"\" gpustack/gpustack \\\n    gpustack copy-images \\\n    --source quay.io \\\n    --destination &lt;your_internal_registry&gt; \\\n    --destination-username &lt;your_username&gt; \\\n    --destination-password &lt;your_password&gt;\n</code></pre> <p>For more details on <code>copy-images</code>, refer to the CLI Reference.</p> <ul> <li>List Images</li> </ul> <p>If you cannot access your internal registry directly, you can first pull the <code>gpustack/gpustack</code> image and then use <code>list-images</code> command to see which images need to be downloaded:</p> <pre><code>sudo docker run --rm -it --entrypoint \"\" gpustack/gpustack \\\n    gpustack list-images\n</code></pre> <p>Note</p> <p>This uses the latest version by default. To target a specific version, use the full image tag, e.g., gpustack/gpustack:vx.y.z.</p> <p>The displayed image list includes all supported accelerators, inference backends, versions, and architectures. If you only need a subset, see the CLI Reference for filtering options.</p> <ul> <li>Save Images</li> </ul>"},{"location":"installation/air-gapped/#installation","title":"Installation","text":"<p>After preparing the internal container registry with the required images, you can install GPUStack in the air-gapped environment.</p> <p>For example, to install with NVIDIA and start the GPUStack server with the built-in worker, run:</p> <pre><code> sudo docker run -d --name gpustack \\\n     --restart unless-stopped \\\n     --privileged \\\n     --network host \\\n     --volume /var/run/docker.sock:/var/run/docker.sock \\\n     --volume gpustack-data:/var/lib/gpustack \\\n     --runtime nvidia \\\n-    gpustack/gpustack\n+    &lt;your_internal_registry&gt;/gpustack/gpustack \\\n+    --system-default-container-registry &lt;your_internal_registry&gt;\n</code></pre> <p>If your accelerator is not NVIDIA, adjust the startup command accordingly.</p>"},{"location":"installation/docker-compose/","title":"GPUStack Installation via Docker Compose","text":"<p>This guide explains how to deploy GPUStack and observability components (Prometheus, Grafana) using Docker Compose. NVIDIA and Ascend platforms are covered, with notes for other GPU types.</p>"},{"location":"installation/docker-compose/#overview-of-services","title":"Overview of Services","text":"<p>Services:</p> <ul> <li>gpustack-server: Central server for scheduling, management, and built-in inference.</li> <li>gpustack-worker: (Optional) Distributed inference worker, can run on separate nodes.</li> <li>prometheus: Metrics collection.</li> <li>grafana: Metrics visualization.</li> </ul>"},{"location":"installation/docker-compose/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Compose installed (guide).</li> <li>Required ports available (see requirements).</li> </ul>"},{"location":"installation/docker-compose/#nvidia","title":"NVIDIA","text":""},{"location":"installation/docker-compose/#requirements","title":"Requirements","text":"<ul> <li>NVIDIA GPU driver (CUDA 12.4+), verify with:   <pre><code>nvidia-smi\n</code></pre></li> <li>NVIDIA Container Toolkit, verify with:   <pre><code>sudo docker info | grep nvidia\n</code></pre></li> </ul>"},{"location":"installation/docker-compose/#deployment","title":"Deployment","text":"<ul> <li>Server (compose file):   <pre><code>sudo docker compose -f docker-compose.server.nvidia.yaml up -d\n</code></pre>   Access UI: <code>http://your_host_ip</code>   Get admin password:   <pre><code>sudo docker exec -it gpustack-server cat /var/lib/gpustack/initial_admin_password\n</code></pre></li> <li>Worker (compose file):</li> <li>Edit file: set <code>server-url</code> and <code>token</code>.</li> <li>Start:   <pre><code>sudo docker compose -f docker-compose.worker.nvidia.yaml up -d\n</code></pre></li> </ul>"},{"location":"installation/docker-compose/#ascend","title":"Ascend","text":""},{"location":"installation/docker-compose/#requirements_1","title":"Requirements","text":"<ul> <li>Ascend NPU Driver supporting Ascend CANN 8.2 or higher, verify with:   <pre><code>sudo npu-smi info\n</code></pre></li> <li>Ascend Container Toolkit, verify with:   <pre><code>sudo docker info 2&gt;/dev/null | grep -q \"ascend\" \\\n      &amp;&amp; echo \"Ascend Container Toolkit OK\" \\\n      || (echo \"Ascend Container Toolkit not configured\"; exit 1)\n</code></pre></li> </ul>"},{"location":"installation/docker-compose/#deployment_1","title":"Deployment","text":"<ul> <li>Device detection (before starting):   <pre><code>export ASCEND_VISIBLE_DEVICES=$(ls /dev/davinci* 2&gt;/dev/null | head -1 | grep -o '[0-9]\\+' || echo \"0\")\n</code></pre></li> <li>Server (compose file):   <pre><code>sudo -E docker compose -f docker-compose.server.ascend.yaml up -d\n</code></pre>   Access UI: <code>http://your_host_ip</code>   Get admin password:   <pre><code>sudo docker exec -it gpustack-server cat /var/lib/gpustack/initial_admin_password\n</code></pre></li> <li>Worker (compose file):</li> <li>Edit file: set <code>server-url</code> and <code>token</code>.</li> <li>Start:   <pre><code>sudo -E docker compose -f docker-compose.worker.ascend.yaml up -d\n</code></pre></li> </ul>"},{"location":"installation/docker-compose/#other-gpu-platforms","title":"Other GPU Platforms","text":"<p>Refer to requirements for platform-specific setup (AMD, MLU, etc.).</p>"},{"location":"installation/docker-compose/#deployment_2","title":"Deployment","text":"<ul> <li>Edit Compose files as needed:</li> <li>Adjust/remove <code>runtime: nvidia</code>.</li> <li>Set environment variables and volumes for your hardware.</li> <li>For workers, set correct <code>server-url</code> and <code>token</code>.</li> <li>Start services using <code>docker compose -f &lt;compose-file&gt; up -d</code>.</li> </ul>"},{"location":"installation/requirements/","title":"Installation Requirements","text":"<p>This page outlines the software and networking requirements for nodes running GPUStack.</p>"},{"location":"installation/requirements/#operating-system-requirements","title":"Operating System Requirements","text":"<p>GPUStack supports most modern Linux distributions on AMD64 and ARM64 architectures.</p> <p>Note</p> <ul> <li>GPUStack is not recommended for direct installation via PyPi. For best compatibility, use the provided Docker images.</li> <li>The Network Time Protocol (NTP) package must be installed to ensure consistent state synchronization between nodes.</li> </ul> <p>GPUStack has been tested and verified on the following operating systems:</p> OS Versions Ubuntu &gt;= 20.04 Debian &gt;= 11 RHEL &gt;= 8 Rocky &gt;= 8 Fedora &gt;= 36 OpenSUSE &gt;= 15.3 (Leap) OpenEuler &gt;= 22.03"},{"location":"installation/requirements/#accelerator-runtime-requirements","title":"Accelerator Runtime Requirements","text":"<p>GPUStack supports a variety of General-Purpose Accelerators as inference backends, including:</p> <ul> <li> NVIDIA GPU</li> <li> AMD GPU</li> <li> Ascend NPU</li> <li> Hygon DCU (Experimental)</li> <li> MThreads GPU (Experimental)</li> <li> Iluvatar GPU (Experimental)</li> <li> MetaX GPU (Experimental)</li> <li> Cambricon MLU (Experimental)</li> </ul> <p>Ensure all required drivers and toolkits are installed before running GPUStack.</p>"},{"location":"installation/requirements/#nvidia-gpu","title":"NVIDIA GPU","text":"<p>To use NVIDIA GPU, install:</p> <ul> <li>NVIDIA GPU Driver</li> <li>NVIDIA Container Toolkit</li> </ul>"},{"location":"installation/requirements/#amd-gpu","title":"AMD GPU","text":"<p>To use AMD GPU, install:</p> <ul> <li>AMD GPU Driver</li> <li>AMD Container Runtime</li> </ul>"},{"location":"installation/requirements/#ascend-npu","title":"Ascend NPU","text":"<p>For Ascend NPU, install:</p> <ul> <li>Ascend NPU Driver</li> <li>Ascend Docker Runtime</li> </ul>"},{"location":"installation/requirements/#hygon-dcu","title":"Hygon DCU","text":"<p>To use Hygon DCU, install:</p> <ul> <li>Hygon DCU Driver</li> <li>Hygon DTK Toolkit</li> </ul>"},{"location":"installation/requirements/#mthreads-gpu","title":"MThreads GPU","text":"<p>To use MThreads GPU, install:</p> <ul> <li>MThreads GPU Driver</li> <li>MThreads Container Toolkit</li> </ul>"},{"location":"installation/requirements/#iluvatar-gpu","title":"Iluvatar GPU","text":"<p>To use Iluvatar GPU, install:</p> <ul> <li>Iluvatar GPU Driver</li> <li>Iluvatar Container Toolkit</li> </ul>"},{"location":"installation/requirements/#metax-gpu","title":"MetaX GPU","text":"<p>To use MetaX GPU, install:</p> <ul> <li>MetaX GPU Driver</li> <li>MetaX MACA SDK</li> </ul>"},{"location":"installation/requirements/#cambricon-mlu","title":"Cambricon MLU","text":"<p>To use Cambricon MLU, install:</p> <ul> <li>Cambricon MLU Driver</li> <li>Cambricon NeuWare Toolkit</li> </ul>"},{"location":"installation/requirements/#networking-requirements","title":"Networking Requirements","text":""},{"location":"installation/requirements/#connectivity-requirements","title":"Connectivity Requirements","text":"<p>The following network connectivity is required for GPUStack to function properly:</p> <p>Server-to-Worker: The server must be able to reach workers to proxy inference requests.</p> <p>Worker-to-Server: Workers must be able to reach the server to register and send updates.</p> <p>Worker-to-Worker: Required for distributed inference across multiple workers.</p>"},{"location":"installation/requirements/#port-requirements","title":"Port Requirements","text":"<p>GPUStack uses these ports for communication:</p>"},{"location":"installation/requirements/#server-ports","title":"Server Ports","text":"Port Description TCP 80 Default port for GPUStack UI and API endpoints TCP 443 Default port for GPUStack UI and API endpoints (TLS enabled) TCP 10161 Default port for server metrics endpoint TCP 8080 Default port for GPUStack server internal API TCP 5432 Default port for embedded Postgres Database"},{"location":"installation/requirements/#worker-ports","title":"Worker Ports","text":"Port Description TCP 10150 Default port for GPUStack worker TCP 10151 Default port for worker metrics endpoint TCP 8080 Default port for GPUStack worker internal API TCP 40000-40063 Port range for inference services TCP 41000-41999 Port range for Ray services(vLLM distributed deployment using)"},{"location":"installation/requirements/#distributed-vllm-with-ray-ports","title":"Distributed vLLM with Ray Ports","text":"<p>When using distributed vLLM, GPUStack will parse the above port range for Ray services, and assign them in order as below:</p> <ol> <li>GCS server port (the first port of the range)</li> <li>Client Server port</li> <li>Dashboard port</li> <li>Dashboard gRPC port (no longer used since Ray 2.45.0, kept for backward compatibility)</li> <li>Dashboard agent gRPC port</li> <li>Dashboard agent listen port</li> <li>Metrics export port</li> <li>Node Manager port</li> <li>Object Manager port</li> <li>Raylet runtime env agent port</li> <li>Minimum port number for the worker</li> <li>Maximum port number for the worker (the last port of the range)</li> </ol> <p>For more details on Ray ports, see the Ray documentation.</p>"},{"location":"installation/requirements/#embedded-gateway-ports","title":"Embedded Gateway Ports","text":"<p>The embedded gateway for both server and worker uses the following ports for internal communications.</p> Port Host Description TCP 18443 127.0.0.1 Port for the file-based APIServer serving via HTTPS TCP 15000 127.0.0.1 Management port for the Envoy gateway TCP 15021 0.0.0.0 Health check port for the Envoy gateway TCP 15090 0.0.0.0 Metrics port for the Envoy gateway TCP 9876 127.0.0.1 Introspection port for the Pilot-discovery TCP 15010 127.0.0.1 Port for Pilot-discovery serving XDS via HTTP/gRPC TCP 15012 127.0.0.1 Port for Pilot-discovery serving XDS via secure gRPC TCP 15020 0.0.0.0 Metrics port for Pilot-agent TCP 8888 127.0.0.1 Port for Controller serving XDS via HTTP TCP 15051 127.0.0.1 Port for Controller serving XDS via gRPC"},{"location":"installation/uninstallation/","title":"Uninstallation","text":"<p>GPUStack is typically installed using containerization,  so uninstallation mainly involves removing the container and any associated data volumes.</p> <p>For example, if GPUStack is running in a Docker container named <code>gpustack</code>, run:</p> <pre><code>docker rm -f gpustack\n</code></pre> <p>To optionally remove associated data volumes, use:</p> <pre><code>docker volume rm &lt;data_volume_name&gt;\n</code></pre>"},{"location":"installation/amd/installation/","title":"AMD Installation","text":""},{"location":"installation/amd/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> AMD GPUs</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li> Linux ARM64</li> <li>Available Inference Backends</li> <li> vLLM</li> <li> Custom Engines</li> </ul> <p>Note</p> <ol> <li> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device.    Please verify compatibility with your target devices using ROCm Compatibility before proceeding.</p> </li> <li> <p>Default container images, such as vLLM, are provided by the GPUStack runner.</p> </li> </ol>"},{"location":"installation/amd/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/amd/installation/#amd-gpu-driver","title":"AMD GPU Driver","text":"<p>Ensure your system has an AMD GPU Driver that supports AMD ROCm 6.4 or higher. Verify installation with:</p> <pre><code>sudo amd-smi static\n</code></pre>"},{"location":"installation/amd/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker with the AMD Container Runtime:</p> <pre><code>sudo docker info 2&gt;/dev/null | grep -q \"amd\" \\\n    &amp;&amp; echo \"AMD Container Toolkit OK\" \\\n    || (echo \"AMD Container Toolkit not configured\"; exit 1)\n</code></pre>"},{"location":"installation/amd/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/amd/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --volume /opt/rocm:/opt/rocm:ro \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime amd \\\n    gpustack/gpustack\n</code></pre> <ul> <li>The <code>--privileged</code> flag is required for device vendor-agnostic access.</li> <li>To restrict GPU access, remove <code>--privileged</code> flag and set the <code>AMD_VISIBLE_DEVICES</code> environment variable.   See AMD Container Runtime - Migration Guide.</li> <li>If the <code>/opt/rocm</code> directory does not exist, please create a symbolic link pointing to the ROCm installed path: <code>ln -s /path/to/rocm /opt/rocm</code>.</li> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/amd/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/amd/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     --runtime amd \\\n     ...\n</code></pre>"},{"location":"installation/amd/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     --runtime amd \\\n     ...\n</code></pre>"},{"location":"installation/amd/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --runtime amd \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/amd/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/amd/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/ascend/installation/","title":"Ascend Installation","text":""},{"location":"installation/ascend/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> Ascend NPU 910C series</li> <li> Ascend NPU 910B series (910B1 ~ 910B4)</li> <li> Ascend NPU 310P3</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li> Linux ARM64</li> <li>Available Inference Backends</li> <li> vLLM</li> <li> SGLang</li> <li> MindIE</li> <li> Custom Engines</li> </ul> <p>Note</p> <ol> <li> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device.    Please verify compatibility with your target devices</p> </li> <li> <p>Default container images, such as vLLM, SGLang and MindIE, are provided by the GPUStack runner.</p> </li> </ol>"},{"location":"installation/ascend/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/ascend/installation/#ascend-npu-driver","title":"Ascend NPU Driver","text":"<p>Ensure your system has an Ascend NPU Driver that supports Ascend CANN 8.2 or higher. Verify installation with:</p> <pre><code>sudo npu-smi info\n</code></pre>"},{"location":"installation/ascend/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker with the Ascend Docker Runtime:</p> <pre><code>sudo docker info 2&gt;/dev/null | grep -q \"ascend\" \\\n    &amp;&amp; echo \"Ascend Container Toolkit OK\" \\\n    || (echo \"Ascend Container Toolkit not configured\"; exit 1)\n</code></pre>"},{"location":"installation/ascend/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/ascend/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --env \"ASCEND_VISIBLE_DEVICES=$(sudo ls /dev/davinci* | head -1 | grep -o '[0-9]\\+' || echo \"0\")\" \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime ascend \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict NPU access, remove <code>--privileged</code> flag and set the <code>ASCEND_VISIBLE_DEVICES</code> environment variable.   See MindCluster - Docker Client Usage.   When GPUStack detects device indexes that don't start from 0 or are not consecutive, it performs automatic alignment.    This is useful for virtual machine scenarios where only some devices are pass-through.    However, if the host's devices are consecutive, automatic alignment can be counterproductive.    Therefore, automatic alignment can be turned off to avoid wrong device mapping.</li> </ul> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n-    --privileged \\\n+    --env \"ASCEND_VISIBLE_DEVICES=&lt;expected_devices_comma_seperated_list&gt;\" \\\n+    --env \"GPUSTACK_RUNTIME_DEPLOY_BACKEND_VISIBLE_DEVICES_VALUE_ALIGNMENT=\" \\\n     ...\n</code></pre> <ul> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/ascend/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/ascend/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     --runtime ascend \\\n     ...\n</code></pre>"},{"location":"installation/ascend/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     --runtime ascend \\\n     ...\n</code></pre>"},{"location":"installation/ascend/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --runtime ascend \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/ascend/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/ascend/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/cambricon/installation/","title":"Cambricon Installation (Experimental)","text":""},{"location":"installation/cambricon/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> Cambricon MLUs</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li>Available Inference Backends</li> <li> Custom Engines</li> </ul> <p>Note</p> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device. Please verify compatibility with your target devices.</p>"},{"location":"installation/cambricon/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/cambricon/installation/#cambricon-mlu-driver-and-neuware-toolkit","title":"Cambricon MLU Driver and NeuWare Toolkit","text":"<p>Ensure your system has an Cambricon MLU Driver that supports Cambricon NeuWare. Verify installation with:</p> <pre><code>sudo cnmon\n</code></pre>"},{"location":"installation/cambricon/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker.</p>"},{"location":"installation/cambricon/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/cambricon/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --volume /usr/local/neuware:/usr/local/neuware:ro \\\n    --volume /usr/bin/cnmon:/usr/bin/cnmon \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict MLU access, remove <code>--privileged</code> flag and set <code>--device</code> options accordingly.</li> <li>If the <code>/usr/local/neuware</code> directory does not exist, please create a symbolic link pointing to the Cambricon installed path: <code>ln -s /path/to/neuware /usr/local/neuware</code>.</li> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/cambricon/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/cambricon/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     gpustack/gpustack\n</code></pre>"},{"location":"installation/cambricon/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     gpustack/gpustack\n</code></pre>"},{"location":"installation/cambricon/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/cambricon/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/cambricon/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/hygon/installation/","title":"Hygon Installation (Experimental)","text":""},{"location":"installation/hygon/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> Hygon DCUs (K100_AI (Verified), Z100/Z100L/K100(Not Verified))</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li>Available Inference Backends</li> <li> vLLM</li> <li> Custom Engines</li> </ul> <p>Note</p> <ol> <li> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device.    Please verify compatibility with your target devices.</p> </li> <li> <p>Default container images, such as vLLM, are provided by the GPUStack runner.</p> </li> </ol>"},{"location":"installation/hygon/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/hygon/installation/#hygon-dcu-driver-and-dtk-toolkit","title":"Hygon DCU Driver and DTK Toolkit","text":"<p>Ensure your system has an Hygon DCU Driver that supports Hygon DTK 25.04 or higher. Verify installation with:</p> <pre><code>sudo hy-smi\n</code></pre>"},{"location":"installation/hygon/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker.</p>"},{"location":"installation/hygon/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/hygon/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --volume /opt/hyhal:/opt/hyhal:ro \\\n    --volume /opt/dtk:/opt/dtk:ro \\\n    --env ROCM_PATH=/opt/dtk \\\n    --env ROCM_SMI_LIB_PATH=/opt/hyhal/lib \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict DCU access, remove <code>--privileged</code> flag and set <code>--device</code> options accordingly.</li> <li>If the <code>/opt/hyhal</code> directory does not exist, please create a symbolic link pointing to the Hygon installed path: <code>ln -s /path/to/hyhal /opt/hyhal</code>.   Same as <code>/opt/dtk</code> directory.</li> <li>If failed to detect devices, please try to remove <code>--env ROCM_SMI_LIB_PATH=/opt/hyhal/lib</code>.</li> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/hygon/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/hygon/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     gpustack/gpustack\n</code></pre>"},{"location":"installation/hygon/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     gpustack/gpustack\n</code></pre>"},{"location":"installation/hygon/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/hygon/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/hygon/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/iluvatar/installation/","title":"Iluvatar Installation (Experimental)","text":""},{"location":"installation/iluvatar/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> Iluvatar GPUs</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li>Available Inference Backends</li> <li> vLLM</li> <li> Custom Engines</li> </ul> <p>Note</p> <ol> <li> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device.    Please verify compatibility with your target devices.</p> </li> <li> <p>Default container images, such as vLLM, are provided by the GPUStack runner.</p> </li> </ol>"},{"location":"installation/iluvatar/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/iluvatar/installation/#iluvatar-gpu-driver","title":"Iluvatar GPU Driver","text":"<p>Ensure your system has an Iluvatar GPU Driver. Verify installation with:</p> <pre><code>sudo ixsmi\n</code></pre>"},{"location":"installation/iluvatar/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker with the Iluvatar Container Toolkit:</p> <pre><code>sudo docker info 2&gt;/dev/null | grep -q \"iluvatar\" \\\n    &amp;&amp; echo \"Iluvatar Container Toolkit OK\" \\\n    || (echo \"Iluvatar Container Toolkit not configured\"; exit 1)\n</code></pre>"},{"location":"installation/iluvatar/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/iluvatar/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime iluvatar \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict GPU access, remove <code>--privileged</code> flag and set the <code>IX_VISIBLE_DEVICES</code> environment variable.   See Iluvatar Container Toolkit - GPU Enumeration.</li> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/iluvatar/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/iluvatar/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     --runtime iluvatar \\\n     ...\n</code></pre>"},{"location":"installation/iluvatar/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     --runtime iluvatar \\\n     ...\n</code></pre>"},{"location":"installation/iluvatar/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --runtime iluvatar \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/iluvatar/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/iluvatar/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/metax/installation/","title":"MetaX Installation (Experimental)","text":""},{"location":"installation/metax/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> MetaX GPUs</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li>Available Inference Backends</li> <li> vLLM</li> <li> Custom Engines</li> </ul> <p>Note</p> <ol> <li> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device.    Please verify compatibility with your target devices.</p> </li> <li> <p>Default container images, such as vLLM, are provided by the GPUStack runner.</p> </li> </ol>"},{"location":"installation/metax/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/metax/installation/#metax-gpu-driver-and-maca-toolkit","title":"MetaX GPU Driver and MACA Toolkit","text":"<p>Ensure your system has an MetaX GPU Driver that supports MetaX MACA 3.0 or higher. Verify installation with:</p> <pre><code>sudo mx-smi\n</code></pre>"},{"location":"installation/metax/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker.</p>"},{"location":"installation/metax/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/metax/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --volume /opt/mxdriver:/opt/mxdriver:ro \\\n    --volume /opt/maca:/opt/maca:ro \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict GPU access, remove <code>--privileged</code> flag and set <code>--device</code> options accordingly.</li> <li>If the <code>/opt/mxdriver</code> directory does not exist, please create a symbolic link pointing to the MetaX installed path: <code>ln -s /path/to/metax /opt/mxdriver</code>.   Same as <code>/opt/maca</code> directory.</li> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/metax/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/metax/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     gpustack/gpustack\n</code></pre>"},{"location":"installation/metax/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     gpustack/gpustack\n</code></pre>"},{"location":"installation/metax/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/metax/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/metax/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/mthreads/installation/","title":"MThreads Installation (Experimental)","text":""},{"location":"installation/mthreads/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> MThreads GPUs</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li>Available Inference Backends</li> <li> Custom Engines</li> </ul> <p>Note</p> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) provides support for that device. Please verify compatibility with your target devices.</p>"},{"location":"installation/mthreads/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/mthreads/installation/#mthreads-gpu-driver","title":"MThreads GPU Driver","text":"<p>Ensure your system has an MThreads GPU Driver. Verify installation with:</p> <pre><code>sudo mthreads-gmi\n</code></pre>"},{"location":"installation/mthreads/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker with the KUAE Cloud Native Toolkits:</p> <pre><code>sudo docker info 2&gt;/dev/null | grep -q \"mthreads\" \\\n    &amp;&amp; echo \"MThreads Container Toolkit OK\" \\\n    || (echo \"MThreads Container Toolkit not configured\"; exit 1)\n</code></pre>"},{"location":"installation/mthreads/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/mthreads/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker (host network mode is recommended):</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime mthreads \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict GPU access, remove <code>--privileged</code> flag and set the <code>MTHREADS_VISIBLE_DEVICES</code> environment variable.   See MThreads Container Toolkit - GPU Enumeration.</li> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/mthreads/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/mthreads/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     --runtime mthreads \\\n     ..\n</code></pre>"},{"location":"installation/mthreads/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     --runtime mthreads \\\n     ...\n</code></pre>"},{"location":"installation/mthreads/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --runtime mthreads \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/mthreads/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/mthreads/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"installation/nvidia/installation/","title":"NVIDIA Installation","text":""},{"location":"installation/nvidia/installation/#supported","title":"Supported","text":"<ul> <li>Target Devices</li> <li> NVIDIA GPUs</li> <li>Operating Systems</li> <li> Linux AMD64</li> <li> Linux ARM64</li> <li>Available Inference Backends</li> <li> vLLM</li> <li> SGLang</li> <li> VoxBox</li> <li> Custom Engines</li> </ul> <p>Note</p> <ol> <li> <p>Whether a target device can run a specific inference backend depends on whether the corresponding version of the inference backend (container image) supports that device.    Please verify compatibility with your target devices using NVIDIA Compute Compatibility before proceeding.</p> </li> <li> <p>Default container images, such as vLLM, SGLang, and VoxBox, are provided by the GPUStack runner.</p> </li> </ol>"},{"location":"installation/nvidia/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/nvidia/installation/#nvidia-gpu-driver","title":"NVIDIA GPU Driver","text":"<p>Ensure your system has an NVIDIA GPU Driver that supports NVIDIA CUDA 12.4 or higher. Verify installation with:</p> <pre><code>sudo nvidia-smi\n</code></pre>"},{"location":"installation/nvidia/installation/#container-running-environment","title":"Container Running Environment","text":"<p>It is recommended to use Docker with the NVIDIA Container Toolkit:</p> <pre><code>sudo docker info 2&gt;/dev/null | grep -q \"nvidia\" \\\n    &amp;&amp; echo \"NVIDIA Container Toolkit OK\" \\\n    || (echo \"NVIDIA Container Toolkit not configured\"; exit 1)\n</code></pre>"},{"location":"installation/nvidia/installation/#port-requirements","title":"Port Requirements","text":"<p>Ensure that each node meets the port requirements.</p>"},{"location":"installation/nvidia/installation/#installation","title":"Installation","text":"<p>Run the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime nvidia \\\n    gpustack/gpustack\n</code></pre> <ul> <li>To restrict GPU access, it's usually to remove <code>--privileged</code> flag and set the <code>NVIDIA_VISIBLE_DEVICES</code> environment variable as NVIDIA Container Toolkit - GPU Enumeration described.   However, disabling privileged mode and limiting GPU visibility may cause the GPUStack built-in worker to lose access to GPUs with the error: \"Failed to initialize NVML: Unknown Error\".   A workaround (without restarting Docker) is to explicitly inject the device into the container using the <code>--device</code> declaration as below example,   for more details, please refer to the NVIDIA Container Toolkit - Troubleshooting.</li> </ul> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n-    --privileged \\\n+    --env NVIDIA_VISIBLE_DEVICES=0,1 \\\n+    --device /dev/nvidiactl \\\n+    --device /dev/nvidia-uvm \\\n+    --device /dev/nvidia-uvm-tools \\\n+    --device /dev/nvidia-modeset \\\n+    --device /dev/nvidia0 \\\n+    --device /dev/nvidia1 \\\n     ...\n</code></pre> <ul> <li>The <code>--network=host</code> option is necessary for port awareness.</li> <li>Mounting <code>/var/run/docker.sock</code> allows GPUStack to manage Docker containers for inference engines.</li> </ul>"},{"location":"installation/nvidia/installation/#reusing-model-files","title":"Reusing Model Files","text":"<p>You can reuse model files stored on the host in two ways.</p>"},{"location":"installation/nvidia/installation/#bind-mount-recommended","title":"Bind Mount (Recommended)","text":"<p>Mount pre-downloaded model files into the container so they can be deployed from a local path without re-downloading:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/path/to/model_files:ro \\\n     --runtime nvidia \\\n     ...\n</code></pre>"},{"location":"installation/nvidia/installation/#override-cache-directory","title":"Override Cache Directory","text":"<p>Mount a dedicated directory for storing downloaded models rather than relying on the default Docker volume:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --volume gpustack-data:/var/lib/gpustack \\\n+    --volume /path/to/model_files:/var/lib/gpustack/cache \\\n     --runtime nvidia \\\n     ...\n</code></pre>"},{"location":"installation/nvidia/installation/#customizing-serving-port","title":"Customizing Serving Port","text":"<p>By default, GPUStack listens on port 80. You can change this with the <code>--port</code> parameter:</p> <pre><code> sudo docker run -d --name gpustack \\\n     ...\n     --runtime nvidia \\\n     gpustack/gpustack \\\n+    --port 9090\n</code></pre> <p>For more options or to resolve port conflicts, refer to the CLI Reference.</p>"},{"location":"installation/nvidia/installation/#startup","title":"Startup","text":"<p>Check the GPUStack container logs:</p> <pre><code>sudo docker logs -f gpustack\n</code></pre> <p>If everything is normal, open <code>http://your_host_ip</code> in a browser to access the GPUStack UI.</p> <p>Log in with username <code>admin</code> and the default password. Retrieve the initial password with:</p> <pre><code>sudo docker exec -it gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/nvidia/installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more nodes to GPUStack to form a cluster.</p> <p>Please navigate to the Workers page in the GPUStack UI to get the command for adding workers.</p>"},{"location":"integrations/integrate-with-cherrystudio/","title":"Integrate with CherryStudio","text":"<p>CherryStudio integrates with GPUStack to leverage locally hosted LLMs, embeddings and reranking capabilities.</p>"},{"location":"integrations/integrate-with-cherrystudio/#deploying-models","title":"Deploying Models","text":"<ol> <li> <p>In GPUStack UI, navigate to the <code>Deployments</code> page and click on <code>Deploy Model</code> to deploy the models you need. Here are some example models:</p> <ul> <li>qwen3-instruct-2507</li> <li>qwen2.5-vl-7b</li> <li>bge-m3</li> <li>bge-reranker-v2-m3</li> </ul> </li> </ol> <p></p> <ol> <li>In the model\u2019s Operations, open <code>API Access Info</code> to see how to integrate with this model:</li> </ol> <p></p>"},{"location":"integrations/integrate-with-cherrystudio/#create-an-api-key","title":"Create an API Key","text":"<ol> <li> <p>Hover over the user avatar and navigate to the <code>API Keys</code> page, then click on <code>New API Key</code>.</p> </li> <li> <p>Fill in the name, then click <code>Save</code>.</p> </li> <li> <p>Copy the API key and save it for later use.</p> </li> </ol>"},{"location":"integrations/integrate-with-cherrystudio/#integrating-gpustack-into-cherrystudio","title":"Integrating GPUStack into CherryStudio","text":"<ol> <li> <p>Open CherryStudio, go to <code>Settings</code> \u2192 <code>Model Provider</code>, find GPUStack, enable it, and configure it as shown:</p> <ul> <li> <p><code>API Key</code>: Input the API key you copied from previous steps.</p> </li> <li> <p><code>API Host</code>: <code>Access URL</code> in the <code>API Access Info</code> panel.</p> </li> </ul> </li> </ol> <p></p> <ol> <li>In the GPUStack provider configuration, click \"Manage\" and enable the models you need:</li> </ol> <p></p> <p></p> <ol> <li>(Optional) Test the API:</li> </ol> <p></p> <p>After configuration, return to the CherryStudio home page and start using your models.</p>"},{"location":"integrations/integrate-with-cherrystudio/#using-llms","title":"Using LLMs","text":""},{"location":"integrations/integrate-with-cherrystudio/#using-multimodal-models","title":"Using Multimodal Models","text":"<ol> <li>Select a multimodal model:</li> </ol> <ol> <li>Ask multimodal questions:</li> </ol>"},{"location":"integrations/integrate-with-cherrystudio/#use-embeddings-and-reranking-to-improve-knowledge-base-qa","title":"Use Embeddings and Reranking to Improve Knowledge Base Q&amp;A","text":"<ol> <li>Open the Knowledge Base configuration page:</li> </ol> <ol> <li>Add a knowledge base:</li> </ol> <ol> <li>Add content to the knowledge base (using \u201cNotes\u201d as an example):</li> </ol> <ol> <li>Return to the home page and use knowledge base Q&amp;A:</li> </ol>"},{"location":"integrations/integrate-with-dify/","title":"Integrate with Dify","text":"<p>Dify can integrate with GPUStack to leverage locally deployed LLMs, embeddings, reranking, image generation, Speech-to-Text and Text-to-Speech capabilities.</p>"},{"location":"integrations/integrate-with-dify/#deploying-models","title":"Deploying Models","text":"<ol> <li>In GPUStack UI, navigate to the <code>Deployments</code> page and click on <code>Deploy Model</code> to deploy the models you need. Here are some example models:</li> </ol> <ul> <li>qwen3-8b</li> <li>qwen2.5-vl-3b-instruct</li> <li>bge-m3</li> <li>bge-reranker-v2-m3</li> </ul> <ol> <li>In the model\u2019s Operations, open <code>API Access Info</code> to see how to integrate with this model.</li> </ol>"},{"location":"integrations/integrate-with-dify/#create-an-api-key","title":"Create an API Key","text":"<ol> <li> <p>Hover over the user avatar and navigate to the <code>API Keys</code> page, then click on <code>New API Key</code>.</p> </li> <li> <p>Fill in the name, then click <code>Save</code>.</p> </li> <li> <p>Copy the API key and save it for later use.</p> </li> </ol>"},{"location":"integrations/integrate-with-dify/#integrating-gpustack-into-dify","title":"Integrating GPUStack into Dify","text":"<ol> <li>Access the Dify UI, go to the top right corner and click on <code>PLUGINS</code>, select <code>Install from Marketplace</code>, search for the GPUStack plugin, and choose to install it.</li> </ol> <ol> <li>After installed, go to <code>Settings &gt; Model Provider &gt; GPUStack</code>, then select <code>Add Model</code> and fill in:</li> </ol> <ul> <li> <p>Model Type: Select the model type based on the model.</p> </li> <li> <p>Model Name: The name must match the model name deployed on GPUStack.</p> </li> <li> <p>Server URL: <code>http://your-gpustack-url</code>, do not use <code>localhost</code>, as it refers to the container\u2019s internal network. If you\u2019re using a custom port, make sure to include it. Also, ensure the URL is accessible from inside the Dify container (you can test this with <code>curl</code>).</p> </li> <li> <p>API Key: Input the API key you copied from previous steps.</p> </li> </ul> <p>Click <code>Save</code> to add the model:</p> <p></p> <p>Add other models as needed, then select the added models in the <code>System Model Settings</code> and save:</p> <p></p> <p>You can now use the models in the <code>Studio</code> and <code>Knowledge</code>, here is a simple case:</p> <ol> <li>Go to <code>Knowledge</code> to create a knowledge, and upload your documents:</li> </ol> <p></p> <ol> <li>Configure the Chunk Settings and Retrieval Settings. Use the embedding model to generate document embeddings, and the rerank model to perform retrieval ranking.</li> </ol> <p></p> <ol> <li>After successfully importing the documents, create an application in the <code>Studio</code>, add the previously created knowledge, select the chat model, and interact with it:</li> </ol> <p></p> <ol> <li>Switch the model to <code>qwen2.5-vl-3b-instruct</code>, remove the previously added knowledge base, enable <code>Vision</code>, and upload an image in the chat to activate multimodal input:</li> </ol> <p></p>"},{"location":"integrations/integrate-with-ragflow/","title":"Integrate with RAGFlow","text":"<p>RAGFlow can integrate with GPUStack to leverage locally deployed LLMs, embeddings, reranking, Speech-to-Text and Text-to-Speech capabilities.</p>"},{"location":"integrations/integrate-with-ragflow/#deploying-models","title":"Deploying Models","text":"<ol> <li>In GPUStack UI, navigate to the <code>Deployments</code> page and click on <code>Deploy Model</code> to deploy the models you need. Here are some example models:</li> </ol> <ul> <li>qwen3-8b</li> <li>qwen2.5-vl-3b-instruct</li> <li>bge-m3</li> <li>bge-reranker-v2-m3</li> </ul> <ol> <li>In the model\u2019s Operations, open <code>API Access Info</code> to see how to integrate with this model.</li> </ol>"},{"location":"integrations/integrate-with-ragflow/#create-an-api-key","title":"Create an API Key","text":"<ol> <li> <p>Hover over the user avatar and navigate to the <code>API Keys</code> page, then click on <code>New API Key</code>.</p> </li> <li> <p>Fill in the name, then click <code>Save</code>.</p> </li> <li> <p>Copy the API key and save it for later use.</p> </li> </ol>"},{"location":"integrations/integrate-with-ragflow/#integrating-gpustack-into-ragflow","title":"Integrating GPUStack into RAGFlow","text":"<ol> <li>Access the RAGFlow UI, go to the top right corner and click the avatar, select <code>Model Providers &gt; GPUStack</code>, then select <code>Add the model</code> and fill in:</li> </ol> <ul> <li> <p>Model type: Select the model type based on the model.</p> </li> <li> <p>Model name: The name must match the model name deployed on GPUStack.</p> </li> <li> <p>Base URL: <code>http://your-gpustack-url</code>, the URL should not include the path and do not use <code>localhost</code>, as it refers to the container\u2019s internal network. If you\u2019re using a custom port, make sure to include it. Also, ensure the URL is accessible from inside the RAGFlow container (you can test this with <code>curl</code>).</p> </li> <li> <p>API-Key: Input the API key you copied from previous steps.</p> </li> <li> <p>Max Tokens: Input the max tokens supported by current model configuration.</p> </li> </ul> <p>Click <code>OK</code> to add the model:</p> <p></p> <ol> <li>Add other models as needed, then select the added models in the <code>Set default models</code> and save:</li> </ol> <p></p> <p>You can now use the models in the <code>Chat</code> and <code>Knowledge Base</code>, here is a simple case:</p> <ol> <li>Go to <code>Knowledge base</code> to create a new knowledge base and add your file:</li> </ol> <p></p> <ol> <li>Navigate to <code>Retrieval testing</code> and set the rerank model to <code>bge-reranker-v2-m3</code>:</li> </ol> <p></p> <ol> <li>In <code>Chat</code>, create an assistant, link the previously created knowledge base, and select a chat model:</li> </ol> <p></p> <ol> <li>Create a chat session \u2014 you can now interact with the model and query the knowledge base:</li> </ol> <p></p> <ol> <li>Edit the assistant and switch the model to <code>qwen2.5-vl-3b-instruct</code>. After saving, create a new chat and upload an image to enable multimodal input:</li> </ol> <p></p>"},{"location":"integrations/openai-compatible-apis/","title":"OpenAI-Compatible APIs","text":"<p>GPUStack serves OpenAI-compatible APIs using the <code>/v1</code> path.</p> <p>For all applications and frameworks that support the OpenAI-compatible API, you can integrate and use the models deployed on GPUStack through the OpenAI-compatible API provided by GPUStack.</p>"},{"location":"integrations/openai-compatible-apis/#supported-endpoints","title":"Supported Endpoints","text":"<p>The following API endpoints are supported:</p> <ul> <li> List Models</li> <li> Create Completion</li> <li> Create Chat Completion</li> <li> Create Embeddings</li> <li> Create Image</li> <li> Create Image Edit</li> <li> Create Speech</li> <li> Create Transcription</li> </ul>"},{"location":"integrations/openai-compatible-apis/#rerank-api","title":"Rerank API","text":"<p>In the context of Retrieval-Augmented Generation (RAG), reranking refers to the process of selecting the most relevant information from retrieved documents or knowledge sources before presenting them to the user or utilizing them for answer generation.</p> <p>It is important to note that the OpenAI-compatible APIs does not provide a <code>rerank</code> API, so GPUStack serves Jina compatible Rerank API using the <code>/v1/rerank</code> path.</p>"},{"location":"integrations/openai-compatible-apis/#non-openai-compatible-apis","title":"Non-OpenAI-Compatible APIs","text":"<p>For other non-OpenAI-compatible APIs, GPUStack allows you to enable the Generic Proxy when deploying a model.</p> <p>With the Generic Proxy enabled, GPUStack determines which model to forward the request to by checking either of the following:</p> <ul> <li> <p>the \"model\" field in the JSON body</p> </li> <li> <p>the <code>X-GPUStack-Model</code> header</p> </li> </ul> <p>After enabling the Generic Proxy, GPUStack can forward API requests to the target model via the <code>/model/proxy</code> endpoint. For example:</p> <pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/model/proxy/embed \\\n-X POST \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n-H \"X-GPUStack-Model: bge-m3\" \\\n-d '{\"inputs\":[\"What is Deep Learning?\", \"Deep Learning is not...\"]}'\n</code></pre> <p>For more details, see Enable Generic Proxy.</p>"},{"location":"performance-lab/overview/","title":"Inference Performance Tuning Overview","text":""},{"location":"performance-lab/overview/#overview","title":"Overview","text":"<p>Open-source inference engines like vLLM and SGLang deliver excellent inference performance, but the performance gap between a tuned deployment and an untuned one might be larger than you think. The most effective validation method is to run benchmarks with your actual traffic on your target devices. Nonetheless, we have conducted numerous experiments across different inference engines, GPU devices, models, and configuration parameter combinations. Some general observations from these experiments can offer initial guidance before you dive into deep optimization.</p>"},{"location":"performance-lab/overview/#observations","title":"Observations","text":"<p>The following observations are based on the scope of our current experiments and may be updated or supplemented as more testing is done or as the community makes progress. For optimization methods and conclusions related to specific models on specific GPUs, please refer to the corresponding experimental documentation.</p>"},{"location":"performance-lab/overview/#inference-engines","title":"Inference Engines","text":"<p>The choice of inference engine is crucial. Inference optimization often involves meticulous engine-specific tuning for particular scenarios, such as specific models, specific quantization schemes, specific GPUs, etc. Consequently, whether an engine is optimized for a given scenario makes a significant difference. For instance, vLLM runs gpt-oss-20b more than ten times faster than SGLang/TensorRT-LLM on an A100 GPU(see details). However, we cannot simply state that Engine A is universally better than Engine B. In our experimental results, vLLM, SGLang, and TensorRT-LLM each achieved the best performance in specific scenarios.</p> <ol> <li>vLLM stands out for its excellent user experience, strong compatibility, and comprehensive, timely model support. It consistently delivers high-quality inference performance, making it a well-rounded and reliable choice across a wide range of scenarios.</li> <li>SGLang also provides an excellent user experience and competitive inference performance. While its compatibility and model support breadth trail slightly behind vLLM, its model support timeliness remains among the best. It particularly excels in Speculative Decoding and in running large FP8 MoE models on Hopper GPUs.</li> <li>TensorRT-LLM offers highly optimized inference performance and demonstrates exceptional results in certain experimental settings. However, its applicability is more specialized, with tighter vendor coupling and relatively slower support for newly released models.</li> </ol>"},{"location":"performance-lab/overview/#quantization","title":"Quantization","text":"<ol> <li>Quantization is a crucial technique for maximizing throughput.</li> <li>Weight-Activation quantization also helps reduce latency.</li> </ol>"},{"location":"performance-lab/overview/#speculative-decoding","title":"Speculative Decoding","text":"<p>Speculative decoding is an effective method for optimizing latency. However, its effectiveness degrades significantly as the batch size increases. Therefore, it is not suitable for improving throughput.</p>"},{"location":"performance-lab/overview/#parallelism-strategies","title":"Parallelism Strategies","text":"<p>Parallelism strategies are essential for multi-GPU distributed inference.</p>"},{"location":"performance-lab/overview/#kernels","title":"Kernels","text":"<p>vLLM/SGLang typically provide reasonable default selections based on the hardware environment. In most scenarios, the default attention backend is the most appropriate. Kernel optimizations like DeepGEMM are applicable to specific precisions/GPUs. While often enabled by default, there might be cases where disabling them is more suitable.</p>"},{"location":"performance-lab/overview/#generalizing","title":"Generalizing","text":"<p>A deployment configuration that performs well generally shows positive performance improvements across different Input Sequence Lengths (ISL) and Output Sequence Lengths (OSL), though the ratios can differ significantly.</p>"},{"location":"performance-lab/overview/#deeper-tuning","title":"Deeper Tuning","text":"<p>Some parameters require tuning based on the actual inference request patterns, such as ISL/OSL, prefix repetition in the data, concurrency, etc. These include:</p> <ol> <li>Max batch size (Related to concurrency)</li> <li>Scheduling config (e.g., API server scale-out [vLLM], async scheduling [vLLM], schedule conservativeness [SGLang], schedule policy [SGLang]) (Related to various factors)</li> <li>Extended KV Cache (Related to sequence length and prefix repetition)</li> <li>CUDA graph (Related to concurrency)</li> <li>Torch compile (Related to model architecture and concurrency)</li> </ol>"},{"location":"performance-lab/deepseek-r1/h200/","title":"Optimizing DeepSeek-R1 Throughput on NVIDIA H200 GPUs","text":""},{"location":"performance-lab/deepseek-r1/h200/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of DeepSeek-R1 on a single H200 node:</p> Serving Command <pre><code>python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-0528 --tp 8 --enable-dp-attention\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 5482.59Mean TPOT(ms): 227.45 Total TPS: 7143.30 (+30.3%)Mean TPOT(ms): 304.71 Short Prompt Total TPS: 5931.25Mean TPOT(ms): 351.83 Total TPS: 20448.42 (+244.8%)Mean TPOT(ms): 802.48 Medium Prompt Total TPS: 17690.89Mean TPOT(ms): 237.02 Total TPS: 19024.33 (+7.5%)Mean TPOT(ms): 154.49 Long Prompt Total TPS: 15808.55Mean TPOT(ms): 164.23 Total TPS: 16096.80 (+1.8%)Mean TPOT(ms): 107.91 Very Long Prompt Total TPS: 15514.57Mean TPOT(ms): 365.79 Total TPS: 16091.68 (+3.7%)Mean TPOT(ms): 198.62 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/deepseek-r1/h200/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/deepseek-r1/h200/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/deepseek-r1/h200/#model","title":"Model","text":"<p>deepseek-ai/DeepSeek-R1-0528</p>"},{"location":"performance-lab/deepseek-r1/h200/#hardware","title":"Hardware","text":"<p>NVIDIA H200 GPUs</p>"},{"location":"performance-lab/deepseek-r1/h200/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.3</li> <li>TensorRT-LLM: v1.0.0</li> </ul>"},{"location":"performance-lab/deepseek-r1/h200/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/deepseek-r1/h200/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model deepseek-ai/DeepSeek-R1-0528 --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model deepseek-ai/DeepSeek-R1-0528 --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/deepseek-r1/h200/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/deepseek-r1/h200/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve deepseek-ai/DeepSeek-R1-0528 -tp 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  76.59\nTotal input tokens:                      219171\nTotal generated tokens:                  200752\nRequest throughput (req/s):              13.06\nOutput token throughput (tok/s):         2621.05\nPeak output token throughput (tok/s):    7451.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5482.59\n---------------Time to First Token----------------\nMean TTFT (ms):                          17743.08\nMedian TTFT (ms):                        17698.39\nP99 TTFT (ms):                           24371.87\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          227.45\nMedian TPOT (ms):                        177.90\nP99 TPOT (ms):                           568.18\n---------------Inter-token Latency----------------\nMean ITL (ms):                           119.29\nMedian ITL (ms):                         66.09\nP99 ITL (ms):                            384.38\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-0528 --host 0.0.0.0 --port 8000 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  63.28\nTotal input tokens:                      219171\nTotal generated tokens:                  200767\nRequest throughput (req/s):              15.80\nOutput token throughput (tok/s):         3172.46\nPeak output token throughput (tok/s):    9699.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6635.73\n---------------Time to First Token----------------\nMean TTFT (ms):                          6948.08\nMedian TTFT (ms):                        6793.80\nP99 TTFT (ms):                           13674.05\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          420.18\nMedian TPOT (ms):                        216.55\nP99 TPOT (ms):                           2473.09\n---------------Inter-token Latency----------------\nMean ITL (ms):                           138.99\nMedian ITL (ms):                         51.66\nP99 ITL (ms):                            1159.64\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve /workspace/DeepSeek-R1-0528 --tp_size 8 --ep_size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  124.04\nTotal input tokens:                      219171\nTotal generated tokens:                  200813\nRequest throughput (req/s):              8.06\nOutput token throughput (tok/s):         1618.94\nPeak output token throughput (tok/s):    7636.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3385.88\n---------------Time to First Token----------------\nMean TTFT (ms):                          20253.90\nMedian TTFT (ms):                        20524.78\nP99 TTFT (ms):                           23743.46\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          221.31\nMedian TPOT (ms):                        167.15\nP99 TPOT (ms):                           726.85\n---------------Inter-token Latency----------------\nMean ITL (ms):                           155.18\nMedian ITL (ms):                         111.57\nP99 ITL (ms):                            1518.10\n==================================================\n</code></pre> <p>Result: SGLang(6635.73 tok/s) &gt; vLLM (5482.59 tok/s) &gt; TensorRT-LLM (3385.88 tok/s)</p>"},{"location":"performance-lab/deepseek-r1/h200/#2-parallelism-in-sglang","title":"2. Parallelism in SGLang","text":"<p>TP+DP Attention</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-0528 --host 0.0.0.0 --port 8000 --tp-size 8 --enable-dp-attention\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  58.79\nTotal input tokens:                      219171\nTotal generated tokens:                  200752\nRequest throughput (req/s):              17.01\nOutput token throughput (tok/s):         3414.99\nPeak output token throughput (tok/s):    9601.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7143.30\n---------------Time to First Token----------------\nMean TTFT (ms):                          5992.00\nMedian TTFT (ms):                        5976.11\nP99 TTFT (ms):                           9650.89\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          304.71\nMedian TPOT (ms):                        188.40\nP99 TPOT (ms):                           1704.65\n---------------Inter-token Latency----------------\nMean ITL (ms):                           121.64\nMedian ITL (ms):                         51.15\nP99 ITL (ms):                            464.61\n==================================================\n</code></pre>"},{"location":"performance-lab/deepseek-r1/h200/#3-torchcompile-in-sglang","title":"3. Torch.compile in SGLang","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-0528 --host 0.0.0.0 --port 8000 --tp-size 8 --enable-dp-attention --enable-torch-compile \n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  61.72\nTotal input tokens:                      219171\nTotal generated tokens:                  200813\nRequest throughput (req/s):              16.20\nOutput token throughput (tok/s):         3253.37\nPeak output token throughput (tok/s):    9287.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6804.17\n---------------Time to First Token----------------\nMean TTFT (ms):                          6978.36\nMedian TTFT (ms):                        6965.90\nP99 TTFT (ms):                           10888.39\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          306.37\nMedian TPOT (ms):                        203.10\nP99 TPOT (ms):                           1807.98\n---------------Inter-token Latency----------------\nMean ITL (ms):                           127.42\nMedian ITL (ms):                         51.90\nP99 ITL (ms):                            596.33\n==================================================\n</code></pre>"},{"location":"performance-lab/deepseek-r1/h200/#4-mtp-in-sglang","title":"4. MTP in SGLang","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-0528 --host 0.0.0.0 --port 8000 --tp-size 8 --enable-dp-attention --enable-torch-compile --speculative-algorithm EAGLE --speculative-num-steps 1 --speculative-eagle-topk 1 --speculative-num-draft-tokens 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  189.43\nTotal input tokens:                      219171\nTotal generated tokens:                  200813\nRequest throughput (req/s):              5.28\nOutput token throughput (tok/s):         1060.10\nPeak output token throughput (tok/s):    1121.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2217.12\n---------------Time to First Token----------------\nMean TTFT (ms):                          95444.76\nMedian TTFT (ms):                        98104.26\nP99 TTFT (ms):                           176418.52\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          42.24\nMedian TPOT (ms):                        42.11\nP99 TPOT (ms):                           82.73\n---------------Inter-token Latency----------------\nMean ITL (ms):                           74.27\nMedian ITL (ms):                         36.60\nP99 ITL (ms):                            200.54\n==================================================\n</code></pre>"},{"location":"performance-lab/deepseek-r1/h200/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +21.0% Parallelism +7.6% Torch Compile - MTP -"},{"location":"performance-lab/deepseek-r1/h200/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve deepseek-ai/DeepSeek-R1-0528 -tp 8\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  206.90\nTotal input tokens:                      3199900\nTotal generated tokens:                  10000\nRequest throughput (req/s):              0.48\nOutput token throughput (tok/s):         48.33\nPeak output token throughput (tok/s):    498.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          15514.57\n---------------Time to First Token----------------\nMean TTFT (ms):                          101351.99\nMedian TTFT (ms):                        101163.80\nP99 TTFT (ms):                           200267.68\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          365.79\nMedian TPOT (ms):                        400.72\nP99 TPOT (ms):                           404.78\n---------------Inter-token Latency----------------\nMean ITL (ms):                           362.20\nMedian ITL (ms):                         459.96\nP99 ITL (ms):                            614.75\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  132.68\nTotal input tokens:                      1997530\nTotal generated tokens:                  100000\nRequest throughput (req/s):              3.77\nOutput token throughput (tok/s):         753.67\nPeak output token throughput (tok/s):    2922.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          15808.55\n---------------Time to First Token----------------\nMean TTFT (ms):                          63058.71\nMedian TTFT (ms):                        61952.74\nP99 TTFT (ms):                           124112.96\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          164.23\nMedian TPOT (ms):                        177.37\nP99 TPOT (ms):                           195.80\n---------------Inter-token Latency----------------\nMean ITL (ms):                           163.74\nMedian ITL (ms):                         57.87\nP99 ITL (ms):                            383.79\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  59.26\nTotal input tokens:                      998393\nTotal generated tokens:                  50000\nRequest throughput (req/s):              8.44\nOutput token throughput (tok/s):         843.71\nPeak output token throughput (tok/s):    4036.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          17690.89\n---------------Time to First Token----------------\nMean TTFT (ms):                          28911.90\nMedian TTFT (ms):                        27559.21\nP99 TTFT (ms):                           53497.07\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          237.02\nMedian TPOT (ms):                        276.56\nP99 TPOT (ms):                           322.76\n---------------Inter-token Latency----------------\nMean ITL (ms):                           235.10\nMedian ITL (ms):                         357.74\nP99 ITL (ms):                            379.46\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  22.07\nTotal input tokens:                      126882\nTotal generated tokens:                  4000\nRequest throughput (req/s):              45.32\nOutput token throughput (tok/s):         181.27\nPeak output token throughput (tok/s):    2082.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5931.25\n---------------Time to First Token----------------\nMean TTFT (ms):                          19392.11\nMedian TTFT (ms):                        18992.42\nP99 TTFT (ms):                           21440.63\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          351.83\nMedian TPOT (ms):                        389.74\nP99 TPOT (ms):                           432.12\n---------------Inter-token Latency----------------\nMean ITL (ms):                           263.87\nMedian ITL (ms):                         368.43\nP99 ITL (ms):                            470.22\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  108.42\nTotal input tokens:                      22836\nTotal generated tokens:                  21338\nRequest throughput (req/s):              0.92\nOutput token throughput (tok/s):         196.81\nPeak output token throughput (tok/s):    216.00\nPeak concurrent requests:                9.00\nTotal Token throughput (tok/s):          407.45\n---------------Time to First Token----------------\nMean TTFT (ms):                          123.30\nMedian TTFT (ms):                        117.53\nP99 TTFT (ms):                           258.12\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          19.36\nMedian TPOT (ms):                        19.13\nP99 TPOT (ms):                           22.67\n---------------Inter-token Latency----------------\nMean ITL (ms):                           19.41\nMedian ITL (ms):                         18.84\nP99 ITL (ms):                            48.61\n==================================================\n</code></pre> Optimized serving script <pre><code>python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-0528 --tp 8 --enable-dp-attention\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  199.47\nTotal input tokens:                      3199900\nTotal generated tokens:                  9952\nRequest throughput (req/s):              0.50\nOutput token throughput (tok/s):         49.89\nPeak output token throughput (tok/s):    521.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          16091.68\n---------------Time to First Token----------------\nMean TTFT (ms):                          98133.46\nMedian TTFT (ms):                        97539.39\nP99 TTFT (ms):                           196094.04\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          198.62\nMedian TPOT (ms):                        198.45\nP99 TPOT (ms):                           365.31\n---------------Inter-token Latency----------------\nMean ITL (ms):                           209.45\nMedian ITL (ms):                         35.27\nP99 ITL (ms):                            1680.63\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  130.31\nTotal input tokens:                      1997530\nTotal generated tokens:                  100000\nRequest throughput (req/s):              3.84\nOutput token throughput (tok/s):         767.42\nPeak output token throughput (tok/s):    3227.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          16096.80\n---------------Time to First Token----------------\nMean TTFT (ms):                          62808.37\nMedian TTFT (ms):                        61113.90\nP99 TTFT (ms):                           122763.87\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          107.91\nMedian TPOT (ms):                        105.31\nP99 TPOT (ms):                           179.79\n---------------Inter-token Latency----------------\nMean ITL (ms):                           115.81\nMedian ITL (ms):                         46.40\nP99 ITL (ms):                            155.40\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  55.11\nTotal input tokens:                      998393\nTotal generated tokens:                  50000\nRequest throughput (req/s):              9.07\nOutput token throughput (tok/s):         907.31\nPeak output token throughput (tok/s):    5778.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          19024.33\n---------------Time to First Token----------------\nMean TTFT (ms):                          27248.86\nMedian TTFT (ms):                        25136.93\nP99 TTFT (ms):                           49876.53\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          154.49\nMedian TPOT (ms):                        148.01\nP99 TPOT (ms):                           294.80\n---------------Inter-token Latency----------------\nMean ITL (ms):                           159.34\nMedian ITL (ms):                         48.78\nP99 ITL (ms):                            801.82\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  6.40\nTotal input tokens:                      126882\nTotal generated tokens:                  4000\nRequest throughput (req/s):              156.24\nOutput token throughput (tok/s):         624.94\nPeak output token throughput (tok/s):    2769.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          20448.42\n---------------Time to First Token----------------\nMean TTFT (ms):                          3626.25\nMedian TTFT (ms):                        3413.13\nP99 TTFT (ms):                           4879.78\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          802.48\nMedian TPOT (ms):                        940.27\nP99 TPOT (ms):                           1182.88\n---------------Inter-token Latency----------------\nMean ITL (ms):                           481.68\nMedian ITL (ms):                         139.56\nP99 ITL (ms):                            3031.24\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/a100/","title":"Optimizing GLM-4.6-Air/GLM-4.5-Air Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/glm-4.5-air/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GLM-4.5-Air on A100 GPUs:</p> Serving Command <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air \\\n--tp-size 4 --ep-size 4 --tool-call-parser glm --reasoning-parser glm45\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized (scaled to same GPU count) ShareGPT Total TPS: 3903.44Mean TPOT(ms): 104.59 Total TPS: 6245.77 (+60.0%)Mean TPOT(ms): 309.31 Short Prompt Total TPS: 6271.24Mean TPOT(ms): 153.00 Total TPS: 13012.20  (+107.5%)Mean TPOT(ms): 1246.57 Medium Prompt Total TPS: 8643.75Mean TPOT(ms): 204.31 Total TPS: 10956.49  (+26.8%)Mean TPOT(ms): 165.34 Long Prompt Total TPS: 6975.10Mean TPOT(ms): 267.38 Total TPS: 9100.71 (+30.5%)Mean TPOT(ms): 124.68 Very Long Prompt Total TPS: 6435.97Mean TPOT(ms): 305.47 Total TPS: 7945.66 (+23.5%)Mean TPOT(ms): 276.41 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/glm-4.5-air/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/glm-4.5-air/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/glm-4.5-air/a100/#model","title":"Model","text":"<p>zai-org/GLM-4.5-Air</p>"},{"location":"performance-lab/glm-4.5-air/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/glm-4.5-air/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.4.post2</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/glm-4.5-air/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/glm-4.5-air/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model zai-org/GLM-4.5-Air --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model zai-org/GLM-4.5-Air --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/a100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/glm-4.5-air/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve zai-org/GLM-4.5-Air -tp 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  106.15\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              9.42\nOutput token throughput (tok/s):         1883.14\nPeak output token throughput (tok/s):    3380.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3903.44\n---------------Time to First Token----------------\nMean TTFT (ms):                          38971.22\nMedian TTFT (ms):                        37502.20\nP99 TTFT (ms):                           76635.16\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          104.59\nMedian TPOT (ms):                        101.18\nP99 TPOT (ms):                           210.38\n---------------Inter-token Latency----------------\nMean ITL (ms):                           91.67\nMedian ITL (ms):                         87.56\nP99 ITL (ms):                            178.94\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air --tp-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  67.25\nTotal input tokens:                      214465\nTotal generated tokens:                  199873\nRequest throughput (req/s):              14.87\nOutput token throughput (tok/s):         2971.88\nPeak output token throughput (tok/s):    11709.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6160.72\n---------------Time to First Token----------------\nMean TTFT (ms):                          14772.52\nMedian TTFT (ms):                        14625.90\nP99 TTFT (ms):                           22760.64\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          328.21\nMedian TPOT (ms):                        129.31\nP99 TPOT (ms):                           2061.59\n---------------Inter-token Latency----------------\nMean ITL (ms):                           109.79\nMedian ITL (ms):                         67.14\nP99 ITL (ms):                            411.08\n==================================================\n</code></pre> <p>Result: SGLang(6160.72 tok/s) &gt; vLLM (3903.44 tok/s) &gt; TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/glm-4.5-air/a100/#2-quantization-in-sglang","title":"2. Quantization in SGLang","text":"<p>FP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.5-Air-FP8 --tp-size 4\n</code></pre> Benchmark result <pre><code># Crash. FP8 not supported on A100\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/a100/#3-parallelism-in-sglang","title":"3. Parallelism in SGLang","text":"<p>PP4</p> Serving script <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.5-Air --pp-size 8\n</code></pre> Benchmark result <pre><code># Crash. PP not supported for this setup.\n</code></pre> <p>TP4+EP4</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --tp-size 4 --ep-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  66.34\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              15.07\nOutput token throughput (tok/s):         3013.15\nPeak output token throughput (tok/s):    9933.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6245.77\n---------------Time to First Token----------------\nMean TTFT (ms):                          8445.43\nMedian TTFT (ms):                        8321.65\nP99 TTFT (ms):                           15245.84\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          309.31\nMedian TPOT (ms):                        130.15\nP99 TPOT (ms):                           1969.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           112.44\nMedian ITL (ms):                         75.24\nP99 ITL (ms):                            484.22\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/a100/#4-attention-backend-in-sglang","title":"4. Attention Backend in SGLang","text":"<p>The default attention backend in SGLang for this setup is flashinfer.</p> <p>triton</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --tp-size 4 --ep-size 4 --attention-backend triton\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  68.84\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              14.53\nOutput token throughput (tok/s):         2904.08\nPeak output token throughput (tok/s):    9443.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6019.69\n---------------Time to First Token----------------\nMean TTFT (ms):                          9348.81\nMedian TTFT (ms):                        9050.60\nP99 TTFT (ms):                           17294.26\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          373.94\nMedian TPOT (ms):                        141.96\nP99 TPOT (ms):                           2701.08\n---------------Inter-token Latency----------------\nMean ITL (ms):                           120.58\nMedian ITL (ms):                         74.19\nP99 ITL (ms):                            413.30\n==================================================\n</code></pre> <p>fa3</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --tp-size 4 --ep-size 4 --attention-backend fa3\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  67.66\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              14.78\nOutput token throughput (tok/s):         2954.39\nPeak output token throughput (tok/s):    8239.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6123.98\n---------------Time to First Token----------------\nMean TTFT (ms):                          8260.21\nMedian TTFT (ms):                        8222.20\nP99 TTFT (ms):                           15115.68\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          310.40\nMedian TPOT (ms):                        133.38\nP99 TPOT (ms):                           2082.45\n---------------Inter-token Latency----------------\nMean ITL (ms):                           114.41\nMedian ITL (ms):                         78.46\nP99 ITL (ms):                            397.40\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/a100/#5-cuda-graph-tuning-in-sglang","title":"5. CUDA Graph Tuning in SGLang","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --tp-size 4 --ep-size 4 --cuda-graph-max-bs 512\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  66.38\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              15.06\nOutput token throughput (tok/s):         3011.43\nPeak output token throughput (tok/s):    10146.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6242.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          8458.72\nMedian TTFT (ms):                        8006.49\nP99 TTFT (ms):                           15322.87\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          311.99\nMedian TPOT (ms):                        131.91\nP99 TPOT (ms):                           1946.11\n---------------Inter-token Latency----------------\nMean ITL (ms):                           112.67\nMedian ITL (ms):                         73.66\nP99 ITL (ms):                            516.14\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +57.8% Quantization - Parallelism +1.4% Attention Backend - CUDA Graph Tuning -"},{"location":"performance-lab/glm-4.5-air/a100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve zai-org/GLM-4.5-Air -tp 4\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  498.76\nTotal input tokens:                      3200000\nTotal generated tokens:                  9987\nRequest throughput (req/s):              0.20\nOutput token throughput (tok/s):         20.02\nPeak output token throughput (tok/s):    218.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          6435.97\n---------------Time to First Token----------------\nMean TTFT (ms):                          248013.29\nMedian TTFT (ms):                        248015.97\nP99 TTFT (ms):                           491884.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          305.47\nMedian TPOT (ms):                        317.93\nP99 TPOT (ms):                           319.14\n---------------Inter-token Latency----------------\nMean ITL (ms):                           303.40\nMedian ITL (ms):                         309.84\nP99 ITL (ms):                            406.26\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  300.98\nTotal input tokens:                      1999787\nTotal generated tokens:                  99547\nRequest throughput (req/s):              1.66\nOutput token throughput (tok/s):         330.75\nPeak output token throughput (tok/s):    1743.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          6975.10\n---------------Time to First Token----------------\nMean TTFT (ms):                          142573.31\nMedian TTFT (ms):                        141422.49\nP99 TTFT (ms):                           289825.31\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          267.38\nMedian TPOT (ms):                        295.72\nP99 TPOT (ms):                           300.72\n---------------Inter-token Latency----------------\nMean ITL (ms):                           266.48\nMedian ITL (ms):                         295.37\nP99 ITL (ms):                            307.18\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  121.41\nTotal input tokens:                      999429\nTotal generated tokens:                  50000\nRequest throughput (req/s):              4.12\nOutput token throughput (tok/s):         411.83\nPeak output token throughput (tok/s):    1796.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          8643.75\n---------------Time to First Token----------------\nMean TTFT (ms):                          60300.00\nMedian TTFT (ms):                        60039.46\nP99 TTFT (ms):                           116422.28\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          204.31\nMedian TPOT (ms):                        224.99\nP99 TPOT (ms):                           225.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           202.57\nMedian ITL (ms):                         224.72\nP99 ITL (ms):                            230.94\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  21.03\nTotal input tokens:                      127881\nTotal generated tokens:                  4000\nRequest throughput (req/s):              47.55\nOutput token throughput (tok/s):         190.21\nPeak output token throughput (tok/s):    952.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6271.24\n---------------Time to First Token----------------\nMean TTFT (ms):                          16583.77\nMedian TTFT (ms):                        16009.09\nP99 TTFT (ms):                           20726.61\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          153.00\nMedian TPOT (ms):                        153.83\nP99 TPOT (ms):                           173.39\n---------------Inter-token Latency----------------\nMean ITL (ms):                           114.74\nMedian ITL (ms):                         153.44\nP99 ITL (ms):                            192.15\n==================================================\n</code></pre> Optimized serving script <pre><code>python3 -m sglang.launch_server --model GLM-4.5-Air --tp-size 4 --ep-size 4 \\\n    --tool-call-parser glm --reasoning-parser glm45\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  403.99\nTotal input tokens:                      3200000\nTotal generated tokens:                  9931\nRequest throughput (req/s):              0.25\nOutput token throughput (tok/s):         24.58\nPeak output token throughput (tok/s):    332.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          7945.66\n---------------Time to First Token----------------\nMean TTFT (ms):                          200446.56\nMedian TTFT (ms):                        199894.57\nP99 TTFT (ms):                           397275.29\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          276.41\nMedian TPOT (ms):                        295.02\nP99 TPOT (ms):                           410.95\n---------------Inter-token Latency----------------\nMean ITL (ms):                           271.13\nMedian ITL (ms):                         32.39\nP99 ITL (ms):                            3555.22\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  230.67\nTotal input tokens:                      1999787\nTotal generated tokens:                  99437\nRequest throughput (req/s):              2.17\nOutput token throughput (tok/s):         431.09\nPeak output token throughput (tok/s):    1680.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          9100.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          109316.41\nMedian TTFT (ms):                        113614.39\nP99 TTFT (ms):                           222368.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          124.68\nMedian TPOT (ms):                        124.45\nP99 TPOT (ms):                           179.33\n---------------Inter-token Latency----------------\nMean ITL (ms):                           123.34\nMedian ITL (ms):                         59.70\nP99 ITL (ms):                            294.40\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  95.78\nTotal input tokens:                      999429\nTotal generated tokens:                  49964\nRequest throughput (req/s):              5.22\nOutput token throughput (tok/s):         521.66\nPeak output token throughput (tok/s):    3160.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          10956.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          45224.78\nMedian TTFT (ms):                        44492.00\nP99 TTFT (ms):                           91909.36\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          165.34\nMedian TPOT (ms):                        163.74\nP99 TPOT (ms):                           281.83\n---------------Inter-token Latency----------------\nMean ITL (ms):                           162.06\nMedian ITL (ms):                         63.35\nP99 ITL (ms):                            166.19\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  10.14\nTotal input tokens:                      127881\nTotal generated tokens:                  4000\nRequest throughput (req/s):              98.67\nOutput token throughput (tok/s):         394.66\nPeak output token throughput (tok/s):    2166.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          13012.20\n---------------Time to First Token----------------\nMean TTFT (ms):                          5882.75\nMedian TTFT (ms):                        5678.55\nP99 TTFT (ms):                           9321.13\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1246.57\nMedian TPOT (ms):                        1420.02\nP99 TPOT (ms):                           2379.65\n---------------Inter-token Latency----------------\nMean ITL (ms):                           747.94\nMedian ITL (ms):                         59.22\nP99 ITL (ms):                            6782.02\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/h100/","title":"Optimizing GLM-4.6-Air/GLM-4.5-Air Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/glm-4.5-air/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GLM-4.5-Air on H100 GPUs:</p> Serving Command <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.5-Air-FP8 \\\n--tp-size 4 --ep-size 4 --tool-call-parser glm --reasoning-parser glm45\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized (scaled to same GPU count) ShareGPT Total TPS: 7909.53Mean TPOT(ms): 134.47 Total TPS: 12411.25 (+56.9%)Mean TPOT(ms): 153.80 Short Prompt Total TPS: 7066.67Mean TPOT(ms): 214.38 Total TPS: 28621.61  (+305.0%)Mean TPOT(ms): 327.73 Medium Prompt Total TPS: 22582.76Mean TPOT(ms): 125.44 Total TPS: 35741.37  (+58.3%)Mean TPOT(ms): 106.94 Long Prompt Total TPS: 21705.53Mean TPOT(ms): 82.53 Total TPS: 33207.02 (+53.0%)Mean TPOT(ms): 71.49 Very Long Prompt Total TPS: 20732.85Mean TPOT(ms): 176.99 Total TPS: 26683.88 (+28.7%)Mean TPOT(ms): 200.08 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/glm-4.5-air/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/glm-4.5-air/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/glm-4.5-air/h100/#model","title":"Model","text":"<p>zai-org/GLM-4.5-Air</p>"},{"location":"performance-lab/glm-4.5-air/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/glm-4.5-air/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.4.post2</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/glm-4.5-air/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/glm-4.5-air/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model zai-org/GLM-4.5-Air --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model zai-org/GLM-4.5-Air --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/h100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/glm-4.5-air/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve zai-org/GLM-4.5-Air -tp 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  52.39\nTotal input tokens:                      214465\nTotal generated tokens:                  199895\nRequest throughput (req/s):              19.09\nOutput token throughput (tok/s):         3815.71\nPeak output token throughput (tok/s):    10558.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7909.53\n---------------Time to First Token----------------\nMean TTFT (ms):                          16383.88\nMedian TTFT (ms):                        16164.27\nP99 TTFT (ms):                           19975.18\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          134.47\nMedian TPOT (ms):                        105.53\nP99 TPOT (ms):                           276.78\n---------------Inter-token Latency----------------\nMean ITL (ms):                           74.75\nMedian ITL (ms):                         47.07\nP99 ITL (ms):                            265.69\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air --tp-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  44.10\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              22.67\nOutput token throughput (tok/s):         4532.66\nPeak output token throughput (tok/s):    12110.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          9395.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          4371.21\nMedian TTFT (ms):                        4175.32\nP99 TTFT (ms):                           8363.43\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          222.86\nMedian TPOT (ms):                        132.57\nP99 TPOT (ms):                           1165.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           91.03\nMedian ITL (ms):                         40.24\nP99 ITL (ms):                            525.39\n==================================================\n</code></pre> <p>Result: SGLang(9395.49 tok/s) &gt; vLLM (7909.53 tok/s) &gt; TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/glm-4.5-air/h100/#2-quantization-in-sglang","title":"2. Quantization in SGLang","text":"<p>FP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.5-Air-FP8 --tp-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  34.54\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              28.95\nOutput token throughput (tok/s):         5787.27\nPeak output token throughput (tok/s):    20354.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          11996.08\n---------------Time to First Token----------------\nMean TTFT (ms):                          4436.82\nMedian TTFT (ms):                        4318.99\nP99 TTFT (ms):                           6248.30\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          179.03\nMedian TPOT (ms):                        104.28\nP99 TPOT (ms):                           806.18\n---------------Inter-token Latency----------------\nMean ITL (ms):                           72.38\nMedian ITL (ms):                         25.48\nP99 ITL (ms):                            438.37\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/h100/#3-parallelism-in-sglang","title":"3. Parallelism in SGLang","text":"<p>TP4+EP4</p> Serving script <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.5-Air --tp-size 4 --ep-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  33.39\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              29.95\nOutput token throughput (tok/s):         5987.56\nPeak output token throughput (tok/s):    17690.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          12411.25\n---------------Time to First Token----------------\nMean TTFT (ms):                          4086.62\nMedian TTFT (ms):                        4050.40\nP99 TTFT (ms):                           6097.74\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          153.80\nMedian TPOT (ms):                        105.20\nP99 TPOT (ms):                           721.74\n---------------Inter-token Latency----------------\nMean ITL (ms):                           67.59\nMedian ITL (ms):                         26.23\nP99 ITL (ms):                            378.32\n==================================================\n</code></pre> <p>TP2</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --tp-size 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  66.93\nTotal input tokens:                      214465\nTotal generated tokens:                  199824\nRequest throughput (req/s):              14.94\nOutput token throughput (tok/s):         2985.45\nPeak output token throughput (tok/s):    6512.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6189.64\n---------------Time to First Token----------------\nMean TTFT (ms):                          12314.75\nMedian TTFT (ms):                        4776.06\nP99 TTFT (ms):                           44680.63\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          145.35\nMedian TPOT (ms):                        142.17\nP99 TPOT (ms):                           377.52\n---------------Inter-token Latency----------------\nMean ITL (ms):                           113.40\nMedian ITL (ms):                         93.47\nP99 ITL (ms):                            333.86\n==================================================\n</code></pre> <p>TP2+EP2</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --tp-size 2 --ep-size 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  71.98\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              13.89\nOutput token throughput (tok/s):         2777.39\nPeak output token throughput (tok/s):    6953.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5757.08\n---------------Time to First Token----------------\nMean TTFT (ms):                          14606.79\nMedian TTFT (ms):                        7321.02\nP99 TTFT (ms):                           50010.14\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          158.73\nMedian TPOT (ms):                        150.07\nP99 TPOT (ms):                           394.02\n---------------Inter-token Latency----------------\nMean ITL (ms):                           119.76\nMedian ITL (ms):                         99.62\nP99 ITL (ms):                            266.91\n==================================================\n</code></pre> <p>PP4</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air  --pp-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  118.50\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              8.44\nOutput token throughput (tok/s):         1686.89\nPeak output token throughput (tok/s):    5980.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3496.65\n---------------Time to First Token----------------\nMean TTFT (ms):                          4772.90\nMedian TTFT (ms):                        4737.52\nP99 TTFT (ms):                           6786.89\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          243.24\nMedian TPOT (ms):                        187.96\nP99 TPOT (ms):                           872.62\n---------------Inter-token Latency----------------\nMean ITL (ms):                           173.87\nMedian ITL (ms):                         170.67\nP99 ITL (ms):                            227.82\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.5-air/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +18.8% Quantization +27.7% Parallelism +3.5%"},{"location":"performance-lab/glm-4.5-air/h100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve zai-org/GLM-4.5-Air -tp 4\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  498.76\nTotal input tokens:                      3200000\nTotal generated tokens:                  9987\nRequest throughput (req/s):              0.20\nOutput token throughput (tok/s):         20.02\nPeak output token throughput (tok/s):    218.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          6435.97\n---------------Time to First Token----------------\nMean TTFT (ms):                          248013.29\nMedian TTFT (ms):                        248015.97\nP99 TTFT (ms):                           491884.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          305.47\nMedian TPOT (ms):                        317.93\nP99 TPOT (ms):                           319.14\n---------------Inter-token Latency----------------\nMean ITL (ms):                           303.40\nMedian ITL (ms):                         309.84\nP99 ITL (ms):                            406.26\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  300.98\nTotal input tokens:                      1999787\nTotal generated tokens:                  99547\nRequest throughput (req/s):              1.66\nOutput token throughput (tok/s):         330.75\nPeak output token throughput (tok/s):    1743.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          6975.10\n---------------Time to First Token----------------\nMean TTFT (ms):                          142573.31\nMedian TTFT (ms):                        141422.49\nP99 TTFT (ms):                           289825.31\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          267.38\nMedian TPOT (ms):                        295.72\nP99 TPOT (ms):                           300.72\n---------------Inter-token Latency----------------\nMean ITL (ms):                           266.48\nMedian ITL (ms):                         295.37\nP99 ITL (ms):                            307.18\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  121.41\nTotal input tokens:                      999429\nTotal generated tokens:                  50000\nRequest throughput (req/s):              4.12\nOutput token throughput (tok/s):         411.83\nPeak output token throughput (tok/s):    1796.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          8643.75\n---------------Time to First Token----------------\nMean TTFT (ms):                          60300.00\nMedian TTFT (ms):                        60039.46\nP99 TTFT (ms):                           116422.28\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          204.31\nMedian TPOT (ms):                        224.99\nP99 TPOT (ms):                           225.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           202.57\nMedian ITL (ms):                         224.72\nP99 ITL (ms):                            230.94\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  21.03\nTotal input tokens:                      127881\nTotal generated tokens:                  4000\nRequest throughput (req/s):              47.55\nOutput token throughput (tok/s):         190.21\nPeak output token throughput (tok/s):    952.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6271.24\n---------------Time to First Token----------------\nMean TTFT (ms):                          16583.77\nMedian TTFT (ms):                        16009.09\nP99 TTFT (ms):                           20726.61\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          153.00\nMedian TPOT (ms):                        153.83\nP99 TPOT (ms):                           173.39\n---------------Inter-token Latency----------------\nMean ITL (ms):                           114.74\nMedian ITL (ms):                         153.44\nP99 ITL (ms):                            192.15\n==================================================\n</code></pre> Optimized serving script <pre><code>python3 -m sglang.launch_server --model GLM-4.5-Air --tp-size 4 --ep-size 4 \\\n    --tool-call-parser glm --reasoning-parser glm45\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  403.99\nTotal input tokens:                      3200000\nTotal generated tokens:                  9931\nRequest throughput (req/s):              0.25\nOutput token throughput (tok/s):         24.58\nPeak output token throughput (tok/s):    332.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          7945.66\n---------------Time to First Token----------------\nMean TTFT (ms):                          200446.56\nMedian TTFT (ms):                        199894.57\nP99 TTFT (ms):                           397275.29\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          276.41\nMedian TPOT (ms):                        295.02\nP99 TPOT (ms):                           410.95\n---------------Inter-token Latency----------------\nMean ITL (ms):                           271.13\nMedian ITL (ms):                         32.39\nP99 ITL (ms):                            3555.22\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  230.67\nTotal input tokens:                      1999787\nTotal generated tokens:                  99437\nRequest throughput (req/s):              2.17\nOutput token throughput (tok/s):         431.09\nPeak output token throughput (tok/s):    1680.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          9100.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          109316.41\nMedian TTFT (ms):                        113614.39\nP99 TTFT (ms):                           222368.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          124.68\nMedian TPOT (ms):                        124.45\nP99 TPOT (ms):                           179.33\n---------------Inter-token Latency----------------\nMean ITL (ms):                           123.34\nMedian ITL (ms):                         59.70\nP99 ITL (ms):                            294.40\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  95.78\nTotal input tokens:                      999429\nTotal generated tokens:                  49964\nRequest throughput (req/s):              5.22\nOutput token throughput (tok/s):         521.66\nPeak output token throughput (tok/s):    3160.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          10956.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          45224.78\nMedian TTFT (ms):                        44492.00\nP99 TTFT (ms):                           91909.36\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          165.34\nMedian TPOT (ms):                        163.74\nP99 TPOT (ms):                           281.83\n---------------Inter-token Latency----------------\nMean ITL (ms):                           162.06\nMedian ITL (ms):                         63.35\nP99 ITL (ms):                            166.19\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  10.14\nTotal input tokens:                      127881\nTotal generated tokens:                  4000\nRequest throughput (req/s):              98.67\nOutput token throughput (tok/s):         394.66\nPeak output token throughput (tok/s):    2166.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          13012.20\n---------------Time to First Token----------------\nMean TTFT (ms):                          5882.75\nMedian TTFT (ms):                        5678.55\nP99 TTFT (ms):                           9321.13\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1246.57\nMedian TPOT (ms):                        1420.02\nP99 TPOT (ms):                           2379.65\n---------------Inter-token Latency----------------\nMean ITL (ms):                           747.94\nMedian ITL (ms):                         59.22\nP99 ITL (ms):                            6782.02\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.6/a100/","title":"Optimizing GLM-4.6/GLM-4.5 Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/glm-4.6/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GLM-4.6 on A100 GPUs:</p> Serving Command <pre><code>vllm serve zai-org/GLM-4.6-FP8 -tp 8\n</code></pre> <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/glm-4.6/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/glm-4.6/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/glm-4.6/a100/#model","title":"Model","text":"<p>zai-org/GLM-4.6</p>"},{"location":"performance-lab/glm-4.6/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/glm-4.6/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.3</li> <li>TensorRT-LLM: v1.0.0</li> </ul>"},{"location":"performance-lab/glm-4.6/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/glm-4.6/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model zai-org/GLM-4.6 --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model zai-org/GLM-4.6 --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/glm-4.6/a100/#experiment-results","title":"Experiment Results","text":"<p>The original BF16 precision model cannot be served on a single server with NVIDIA A100 GPUs due to memory limitations. Therefore, we focus on serving the FP8 quantization model.</p>"},{"location":"performance-lab/glm-4.6/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve zai-org/GLM-4.6-FP8 -tp 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  148.61\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              6.73\nOutput token throughput (tok/s):         1345.12\nPeak output token throughput (tok/s):    2500.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2788.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          51022.80\nMedian TTFT (ms):                        48308.20\nP99 TTFT (ms):                           106768.28\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          154.20\nMedian TPOT (ms):                        144.84\nP99 TPOT (ms):                           318.65\n---------------Inter-token Latency----------------\nMean ITL (ms):                           134.46\nMedian ITL (ms):                         117.84\nP99 ITL (ms):                            326.23\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 8\n</code></pre> Benchmark result <pre><code>ValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")\n</code></pre> <p>Result: vLLM (2788.21 tok/s) &gt; SGLang(Not supported) = TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/glm-4.6/a100/#2-parallelism-in-vllm","title":"2. Parallelism in vLLM","text":"<p>TP+EP</p> Serving script <pre><code>vllm serve zai-org/GLM-4.6-FP8 -tp 8 --enable-expert-parallel\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  153.11\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              6.53\nOutput token throughput (tok/s):         1305.62\nPeak output token throughput (tok/s):    2311.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2706.34\n---------------Time to First Token----------------\nMean TTFT (ms):                          51031.51\nMedian TTFT (ms):                        48265.76\nP99 TTFT (ms):                           108400.10\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          158.11\nMedian TPOT (ms):                        149.37\nP99 TPOT (ms):                           315.61\n---------------Inter-token Latency----------------\nMean ITL (ms):                           139.37\nMedian ITL (ms):                         127.75\nP99 ITL (ms):                            321.92\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.6/a100/#3-max-number-of-batched-tokens-in-vllm","title":"3. Max Number of Batched Tokens in vLLM","text":"Serving script <pre><code>vllm serve zai-org/GLM-4.6-FP8 -tp 8 --max-num-batched-tokens 8192\n</code></pre> Benchmark result <pre><code># --max-num-batched-tokens 4096\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  146.98\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              6.80\nOutput token throughput (tok/s):         1360.05\nPeak output token throughput (tok/s):    2502.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2819.17\n---------------Time to First Token----------------\nMean TTFT (ms):                          49578.97\nMedian TTFT (ms):                        47246.86\nP99 TTFT (ms):                           104952.43\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          159.13\nMedian TPOT (ms):                        144.06\nP99 TPOT (ms):                           547.87\n---------------Inter-token Latency----------------\nMean ITL (ms):                           134.40\nMedian ITL (ms):                         117.57\nP99 ITL (ms):                            458.51\n==================================================\n\n# --max-num-batched-tokens 8192\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  146.69\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              6.82\nOutput token throughput (tok/s):         1362.78\nPeak output token throughput (tok/s):    2385.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2824.82\n---------------Time to First Token----------------\nMean TTFT (ms):                          49591.68\nMedian TTFT (ms):                        47020.36\nP99 TTFT (ms):                           104829.14\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          160.28\nMedian TPOT (ms):                        143.24\nP99 TPOT (ms):                           559.15\n---------------Inter-token Latency----------------\nMean ITL (ms):                           134.08\nMedian ITL (ms):                         118.83\nP99 ITL (ms):                            320.13\n==================================================\n\n# --max-num-batched-tokens 16384\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  147.02\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              6.80\nOutput token throughput (tok/s):         1359.74\nPeak output token throughput (tok/s):    2435.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2818.53\n---------------Time to First Token----------------\nMean TTFT (ms):                          49736.96\nMedian TTFT (ms):                        47034.75\nP99 TTFT (ms):                           105084.77\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          160.50\nMedian TPOT (ms):                        143.84\nP99 TPOT (ms):                           582.59\n---------------Inter-token Latency----------------\nMean ITL (ms):                           134.22\nMedian ITL (ms):                         118.12\nP99 ITL (ms):                            325.43\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.6/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection - Parallelism - Max Number of Batched Tokens +1.3%"},{"location":"performance-lab/glm-4.6/h100/","title":"Optimizing GLM-4.6/GLM-4.5 Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/glm-4.6/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GLM-4.6 on H100 GPUs:</p> Serving Command <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.6-FP8 --tp-size 8 --ep-size 8 --tool-call-parser glm --reasoning-parser glm45\n</code></pre> <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/glm-4.6/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/glm-4.6/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/glm-4.6/h100/#model","title":"Model","text":"<p>zai-org/GLM-4.6</p>"},{"location":"performance-lab/glm-4.6/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/glm-4.6/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.3</li> <li>TensorRT-LLM: v1.0.0</li> </ul>"},{"location":"performance-lab/glm-4.6/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/glm-4.6/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model zai-org/GLM-4.6 --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model zai-org/GLM-4.6 --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/glm-4.6/h100/#experiment-results","title":"Experiment Results","text":"<p>The original BF16 precision model cannot be served on a single server with NVIDIA H100 GPUs due to memory limitations. Therefore, we focus on serving the FP8 quantization model.</p>"},{"location":"performance-lab/glm-4.6/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve zai-org/GLM-4.6-FP8 -tp 8 --gpu-memory-utilization 0.85\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  130.81\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              7.64\nOutput token throughput (tok/s):         1528.16\nPeak output token throughput (tok/s):    3935.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3167.63\n---------------Time to First Token----------------\nMean TTFT (ms):                          26751.30\nMedian TTFT (ms):                        26848.82\nP99 TTFT (ms):                           45890.42\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          508.88\nMedian TPOT (ms):                        273.93\nP99 TPOT (ms):                           1362.12\n---------------Inter-token Latency----------------\nMean ITL (ms):                           222.86\nMedian ITL (ms):                         149.69\nP99 ITL (ms):                            1365.56\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  109.16\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              9.16\nOutput token throughput (tok/s):         1831.30\nPeak output token throughput (tok/s):    5614.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3796.00\n---------------Time to First Token----------------\nMean TTFT (ms):                          17555.93\nMedian TTFT (ms):                        17493.70\nP99 TTFT (ms):                           32706.51\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          684.10\nMedian TPOT (ms):                        252.95\nP99 TPOT (ms):                           4790.60\n---------------Inter-token Latency----------------\nMean ITL (ms):                           208.76\nMedian ITL (ms):                         129.94\nP99 ITL (ms):                            514.51\n==================================================\n</code></pre> <p>Result: SGLang(3796.00 tok/s) &gt; vLLM (3167.63 tok/s) &gt; TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/glm-4.6/h100/#2-parallelism-in-sglang","title":"2. Parallelism in SGLang","text":"<p>TP8EP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 8 --ep-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  107.44\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              9.31\nOutput token throughput (tok/s):         1860.58\nPeak output token throughput (tok/s):    6492.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3856.69\n---------------Time to First Token----------------\nMean TTFT (ms):                          18994.85\nMedian TTFT (ms):                        18941.03\nP99 TTFT (ms):                           34437.71\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          677.30\nMedian TPOT (ms):                        237.99\nP99 TPOT (ms):                           4817.75\n---------------Inter-token Latency----------------\nMean ITL (ms):                           200.86\nMedian ITL (ms):                         123.73\nP99 ITL (ms):                            502.84\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.6/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +19.8% Parallelism +1.6%"},{"location":"performance-lab/glm-4.6/h200/","title":"Optimizing GLM-4.6/GLM-4.5 Throughput on NVIDIA H200 GPUs","text":""},{"location":"performance-lab/glm-4.6/h200/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GLM-4.6 on H200 GPUs:</p> Serving Command <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.6-FP8 --tp 4 --tool-call-parser glm --reasoning-parser glm45\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized (scaled to same GPU count) ShareGPT Total TPS: 6030.90Mean TPOT(ms): 197.85 Total TPS: 7081.34x2 (+134.8%)Mean TPOT(ms): 256.43 Short Prompt Total TPS: 9497.19Mean TPOT(ms): 351.26 Total TPS: 21234.81x2  (+347.2%)Mean TPOT(ms): 800.25 Medium Prompt Total TPS: 17214.85Mean TPOT(ms): 269.75 Total TPS: 17363.22x2  (+101.7%)Mean TPOT(ms): 144.82 Long Prompt Total TPS: 16714.91Mean TPOT(ms): 205.69 Total TPS: 14979.90x2 (+79.2%)Mean TPOT(ms): 97.74 Very Long Prompt Total TPS: 15066.09Mean TPOT(ms): 479.17 Total TPS: 12223.53x2 (+62.3%)Mean TPOT(ms): 191.61 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/glm-4.6/h200/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/glm-4.6/h200/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/glm-4.6/h200/#model","title":"Model","text":"<p>zai-org/GLM-4.6</p>"},{"location":"performance-lab/glm-4.6/h200/#hardware","title":"Hardware","text":"<p>NVIDIA H200 GPUs</p>"},{"location":"performance-lab/glm-4.6/h200/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.3</li> <li>TensorRT-LLM: v1.0.0</li> </ul>"},{"location":"performance-lab/glm-4.6/h200/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/glm-4.6/h200/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model zai-org/GLM-4.6 --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model zai-org/GLM-4.6 --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/glm-4.6/h200/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/glm-4.6/h200/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve zai-org/GLM-4.6 -tp 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  68.67\nTotal input tokens:                      214465\nTotal generated tokens:                  199702\nRequest throughput (req/s):              14.56\nOutput token throughput (tok/s):         2907.97\nPeak output token throughput (tok/s):    7158.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6030.90\n---------------Time to First Token----------------\nMean TTFT (ms):                          15339.53\nMedian TTFT (ms):                        15377.50\nP99 TTFT (ms):                           21078.26\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          197.85\nMedian TPOT (ms):                        145.75\nP99 TPOT (ms):                           411.50\n---------------Inter-token Latency----------------\nMean ITL (ms):                           109.47\nMedian ITL (ms):                         68.98\nP99 ITL (ms):                            413.20\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6 --host 0.0.0.0 --port 8000 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  59.37\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              16.84\nOutput token throughput (tok/s):         3366.90\nPeak output token throughput (tok/s):    7570.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6979.05\n---------------Time to First Token----------------\nMean TTFT (ms):                          6044.62\nMedian TTFT (ms):                        5851.20\nP99 TTFT (ms):                           10817.86\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          282.87\nMedian TPOT (ms):                        148.97\nP99 TPOT (ms):                           1610.70\n---------------Inter-token Latency----------------\nMean ITL (ms):                           113.29\nMedian ITL (ms):                         64.03\nP99 ITL (ms):                            369.42\n==================================================\n</code></pre> <p>Result: SGLang(6979.05 tok/s) &gt; vLLM (6030.90 tok/s) &gt; TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/glm-4.6/h200/#2-quantization-in-sglang","title":"2. Quantization in SGLang","text":"<p>FP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  52.72\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              18.97\nOutput token throughput (tok/s):         3791.55\nPeak output token throughput (tok/s):    8987.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7859.29\n---------------Time to First Token----------------\nMean TTFT (ms):                          5085.56\nMedian TTFT (ms):                        4847.02\nP99 TTFT (ms):                           10068.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          271.63\nMedian TPOT (ms):                        144.47\nP99 TPOT (ms):                           1542.73\n---------------Inter-token Latency----------------\nMean ITL (ms):                           105.34\nMedian ITL (ms):                         52.76\nP99 ITL (ms):                            321.65\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.6/h200/#3-parallelism-in-sglang","title":"3. Parallelism in SGLang","text":"<p>TP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  52.72\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              18.97\nOutput token throughput (tok/s):         3791.55\nPeak output token throughput (tok/s):    8987.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7859.29\n---------------Time to First Token----------------\nMean TTFT (ms):                          5085.56\nMedian TTFT (ms):                        4847.02\nP99 TTFT (ms):                           10068.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          271.63\nMedian TPOT (ms):                        144.47\nP99 TPOT (ms):                           1542.73\n---------------Inter-token Latency----------------\nMean ITL (ms):                           105.34\nMedian ITL (ms):                         52.76\nP99 ITL (ms):                            321.65\n==================================================\n</code></pre> <p>TP4</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  58.52\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              17.09\nOutput token throughput (tok/s):         3416.25\nPeak output token throughput (tok/s):    7602.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7081.34\n---------------Time to First Token----------------\nMean TTFT (ms):                          5786.78\nMedian TTFT (ms):                        5769.81\nP99 TTFT (ms):                           10134.80\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          256.43\nMedian TPOT (ms):                        144.11\nP99 TPOT (ms):                           1451.02\n---------------Inter-token Latency----------------\nMean ITL (ms):                           109.76\nMedian ITL (ms):                         62.64\nP99 ITL (ms):                            330.68\n==================================================\n</code></pre> <p>TP4+EP4</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path zai-org/GLM-4.6-FP8 --host 0.0.0.0 --port 8000 --tp-size 4 --ep-size 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  62.06\nTotal input tokens:                      214465\nTotal generated tokens:                  199904\nRequest throughput (req/s):              16.11\nOutput token throughput (tok/s):         3221.11\nPeak output token throughput (tok/s):    8209.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6676.84\n---------------Time to First Token----------------\nMean TTFT (ms):                          6632.87\nMedian TTFT (ms):                        6433.53\nP99 TTFT (ms):                           11461.66\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          284.20\nMedian TPOT (ms):                        157.89\nP99 TPOT (ms):                           1629.92\n---------------Inter-token Latency----------------\nMean ITL (ms):                           117.77\nMedian ITL (ms):                         61.25\nP99 ITL (ms):                            406.32\n==================================================\n</code></pre>"},{"location":"performance-lab/glm-4.6/h200/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +15.7% Quantization +12.6% Parallelism +80.2%"},{"location":"performance-lab/glm-4.6/h200/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve zai-org/GLM-4.6 -tp 8\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  213.02\nTotal input tokens:                      3200000\nTotal generated tokens:                  9406\nRequest throughput (req/s):              0.47\nOutput token throughput (tok/s):         44.16\nPeak output token throughput (tok/s):    489.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          15066.09\n---------------Time to First Token----------------\nMean TTFT (ms):                          105089.84\nMedian TTFT (ms):                        104737.57\nP99 TTFT (ms):                           208225.57\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          479.17\nMedian TPOT (ms):                        541.92\nP99 TPOT (ms):                           544.17\n---------------Inter-token Latency----------------\nMean ITL (ms):                           474.58\nMedian ITL (ms):                         529.63\nP99 ITL (ms):                            637.86\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  125.61\nTotal input tokens:                      1999787\nTotal generated tokens:                  99801\nRequest throughput (req/s):              3.98\nOutput token throughput (tok/s):         794.52\nPeak output token throughput (tok/s):    3810.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          16714.91\n---------------Time to First Token----------------\nMean TTFT (ms):                          57737.75\nMedian TTFT (ms):                        59379.10\nP99 TTFT (ms):                           116713.01\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          205.69\nMedian TPOT (ms):                        241.12\nP99 TPOT (ms):                           260.89\n---------------Inter-token Latency----------------\nMean ITL (ms):                           205.05\nMedian ITL (ms):                         59.67\nP99 ITL (ms):                            424.92\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  60.96\nTotal input tokens:                      999429\nTotal generated tokens:                  50000\nRequest throughput (req/s):              8.20\nOutput token throughput (tok/s):         820.20\nPeak output token throughput (tok/s):    5415.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          17214.85\n---------------Time to First Token----------------\nMean TTFT (ms):                          30215.66\nMedian TTFT (ms):                        29924.42\nP99 TTFT (ms):                           55334.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          269.75\nMedian TPOT (ms):                        283.63\nP99 TPOT (ms):                           407.32\n---------------Inter-token Latency----------------\nMean ITL (ms):                           267.55\nMedian ITL (ms):                         402.36\nP99 ITL (ms):                            417.66\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  13.89\nTotal input tokens:                      127881\nTotal generated tokens:                  4000\nRequest throughput (req/s):              72.01\nOutput token throughput (tok/s):         288.05\nPeak output token throughput (tok/s):    2063.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          9497.19\n---------------Time to First Token----------------\nMean TTFT (ms):                          11467.27\nMedian TTFT (ms):                        10707.53\nP99 TTFT (ms):                           13722.66\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          351.26\nMedian TPOT (ms):                        397.14\nP99 TPOT (ms):                           409.34\n---------------Inter-token Latency----------------\nMean ITL (ms):                           263.45\nMedian ITL (ms):                         389.46\nP99 ITL (ms):                            443.88\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  103.11\nTotal input tokens:                      22992\nTotal generated tokens:                  21794\nRequest throughput (req/s):              0.97\nOutput token throughput (tok/s):         211.38\nPeak output token throughput (tok/s):    232.00\nPeak concurrent requests:                9.00\nTotal Token throughput (tok/s):          434.37\n---------------Time to First Token----------------\nMean TTFT (ms):                          70.16\nMedian TTFT (ms):                        62.63\nP99 TTFT (ms):                           148.47\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          18.11\nMedian TPOT (ms):                        17.80\nP99 TPOT (ms):                           23.44\n---------------Inter-token Latency----------------\nMean ITL (ms):                           17.85\nMedian ITL (ms):                         17.55\nP99 ITL (ms):                            26.92\n==================================================\n</code></pre> Optimized serving script <pre><code>python3 -m sglang.launch_server --model zai-org/GLM-4.6-FP8 --tp 4 \\\n    --tool-call-parser glm --reasoning-parser glm45\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  262.58\nTotal input tokens:                      3200000\nTotal generated tokens:                  9703\nRequest throughput (req/s):              0.38\nOutput token throughput (tok/s):         36.95\nPeak output token throughput (tok/s):    390.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          12223.53\n---------------Time to First Token----------------\nMean TTFT (ms):                          129699.21\nMedian TTFT (ms):                        129839.73\nP99 TTFT (ms):                           257123.03\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          191.61\nMedian TPOT (ms):                        181.54\nP99 TPOT (ms):                           350.40\n---------------Inter-token Latency----------------\nMean ITL (ms):                           189.68\nMedian ITL (ms):                         40.53\nP99 ITL (ms):                            71.16\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  140.16\nTotal input tokens:                      1999787\nTotal generated tokens:                  99801\nRequest throughput (req/s):              3.57\nOutput token throughput (tok/s):         712.05\nPeak output token throughput (tok/s):    2688.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          14979.90\n---------------Time to First Token----------------\nMean TTFT (ms):                          65196.25\nMedian TTFT (ms):                        67840.59\nP99 TTFT (ms):                           131861.02\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          97.74\nMedian TPOT (ms):                        94.74\nP99 TPOT (ms):                           149.34\n---------------Inter-token Latency----------------\nMean ITL (ms):                           97.55\nMedian ITL (ms):                         47.60\nP99 ITL (ms):                            314.57\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  60.44\nTotal input tokens:                      999429\nTotal generated tokens:                  50000\nRequest throughput (req/s):              8.27\nOutput token throughput (tok/s):         827.27\nPeak output token throughput (tok/s):    4460.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          17363.22\n---------------Time to First Token----------------\nMean TTFT (ms):                          27600.49\nMedian TTFT (ms):                        29627.26\nP99 TTFT (ms):                           56644.81\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          144.82\nMedian TPOT (ms):                        142.76\nP99 TPOT (ms):                           250.18\n---------------Inter-token Latency----------------\nMean ITL (ms):                           143.46\nMedian ITL (ms):                         57.80\nP99 ITL (ms):                            145.06\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  6.21\nTotal input tokens:                      127881\nTotal generated tokens:                  4000\nRequest throughput (req/s):              161.01\nOutput token throughput (tok/s):         644.06\nPeak output token throughput (tok/s):    1924.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          21234.81\n---------------Time to First Token----------------\nMean TTFT (ms):                          3551.50\nMedian TTFT (ms):                        3313.50\nP99 TTFT (ms):                           5528.41\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          800.25\nMedian TPOT (ms):                        936.39\nP99 TPOT (ms):                           1413.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           600.19\nMedian ITL (ms):                         0.01\nP99 ITL (ms):                            3985.12\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/a100/","title":"Optimizing GPT-OSS-120B Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/gpt-oss-120b/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GPT-OSS-120B on A100 GPUs:</p> Serving Command <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling --max-num-batched-tokens 4096\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 4768.75Mean TPOT(ms): 97.57 Total TPS: 5055.82 (+6.0%)Mean TPOT(ms): 94.55 Short Prompt Total TPS: 9353.12Mean TPOT(ms): 182.75 Total TPS: 10785.60 (+15.3%)Mean TPOT(ms): 301.66 Medium Prompt Total TPS: 8456.65Mean TPOT(ms): 131.94 Total TPS: 9249.89 (+9.4%)Mean TPOT(ms): 117.85 Long Prompt Total TPS: 7671.00Mean TPOT(ms): 112.18 Total TPS: 8468.21 (+10.4%)Mean TPOT(ms): 99.77 Very Long Prompt Total TPS: 4337.21Mean TPOT(ms): 224.58 Total TPS: 4605.91 (+6.2%)Mean TPOT(ms): 201.61 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/gpt-oss-120b/a100/#model","title":"Model","text":"<p>GPT-OSS-120B</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.3</li> <li>TensorRT-LLM: v1.2.0rc0</li> </ul>"},{"location":"performance-lab/gpt-oss-120b/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/gpt-oss-120b/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model openai/gpt-oss-120b --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model openai/gpt-oss-120b --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/a100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/gpt-oss-120b/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  86.39\nTotal input tokens:                      215312\nTotal generated tokens:                  196656\nRequest throughput (req/s):              11.58\nOutput token throughput (tok/s):         2276.40\nPeak output token throughput (tok/s):    3572.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4768.75\n---------------Time to First Token----------------\nMean TTFT (ms):                          25918.79\nMedian TTFT (ms):                        24822.43\nP99 TTFT (ms):                           59731.55\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          97.57\nMedian TPOT (ms):                        90.70\nP99 TPOT (ms):                           206.31\n---------------Inter-token Latency----------------\nMean ITL (ms):                           86.67\nMedian ITL (ms):                         70.10\nP99 ITL (ms):                            210.76\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --host 0.0.0.0 --port 8000 \n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  597.93\nTotal input tokens:                      215312\nTotal generated tokens:                  197133\nRequest throughput (req/s):              1.67\nOutput token throughput (tok/s):         329.69\nPeak output token throughput (tok/s):    1865.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          689.79\n---------------Time to First Token----------------\nMean TTFT (ms):                          275030.66\nMedian TTFT (ms):                        270126.72\nP99 TTFT (ms):                           550929.50\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          765.78\nMedian TPOT (ms):                        486.18\nP99 TPOT (ms):                           6656.59\n---------------Inter-token Latency----------------\nMean ITL (ms):                           458.97\nMedian ITL (ms):                         112.65\nP99 ITL (ms):                            6667.80\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve openai/gpt-oss-120b --max_seq_len 32768\n</code></pre> Benchmark result <pre><code>NotImplementedError: WFP4A16 MoE is unsupported on SM80.\n</code></pre> <p>Result: vLLM (4768.75 tok/s) &gt; SGLang(689.79 tok/s) &gt; TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#2-async-scheduling-in-vllm","title":"2. Async scheduling in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  82.35\nTotal input tokens:                      215312\nTotal generated tokens:                  195312\nRequest throughput (req/s):              12.14\nOutput token throughput (tok/s):         2371.67\nPeak output token throughput (tok/s):    3615.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4986.20\n---------------Time to First Token----------------\nMean TTFT (ms):                          24787.19\nMedian TTFT (ms):                        23575.09\nP99 TTFT (ms):                           56631.39\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          92.70\nMedian TPOT (ms):                        86.19\nP99 TPOT (ms):                           203.05\n---------------Inter-token Latency----------------\nMean ITL (ms):                           82.31\nMedian ITL (ms):                         65.76\nP99 ITL (ms):                            205.68\n==================================================\n</code></pre> <p>Result: Throughput improved from 4768.75 tok/s to 4986.20 tok/s by enabling async scheduling.</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#3-parallelism-in-vllm","title":"3. Parallelism in vLLM","text":"<p>TP2</p> Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling -tp 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  52.10\nTotal input tokens:                      215312\nTotal generated tokens:                  196156\nRequest throughput (req/s):              19.19\nOutput token throughput (tok/s):         3765.02\nPeak output token throughput (tok/s):    5687.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7897.72\n---------------Time to First Token----------------\nMean TTFT (ms):                          16190.50\nMedian TTFT (ms):                        15435.84\nP99 TTFT (ms):                           36099.41\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          57.98\nMedian TPOT (ms):                        53.39\nP99 TPOT (ms):                           132.88\n---------------Inter-token Latency----------------\nMean ITL (ms):                           51.20\nMedian ITL (ms):                         40.51\nP99 ITL (ms):                            134.62\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/a100/#4-attention-backend-in-vllm","title":"4. Attention Backend in vLLM","text":"<p>FlashAttention is the default.</p> <p>FlashInfer</p> Serving script <pre><code>VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Benchmark result <pre><code>RuntimeError: Worker failed with error 'FlashInfer backend currently does not support attention sinks, please use trtllm on blackwell or flash attention on earlier GPUs.', please check the stack trace above for the root cause\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/a100/#5-max-number-of-batched-tokens-in-vllm","title":"5. Max Number of Batched Tokens in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling --max-num-batched-tokens 4096\n</code></pre> Benchmark result <pre><code># --max-num-batched-tokens 4096\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  81.24\nTotal input tokens:                      215312\nTotal generated tokens:                  195400\nRequest throughput (req/s):              12.31\nOutput token throughput (tok/s):         2405.35\nPeak output token throughput (tok/s):    3767.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5055.82\n---------------Time to First Token----------------\nMean TTFT (ms):                          24102.33\nMedian TTFT (ms):                        23200.31\nP99 TTFT (ms):                           55834.89\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          94.55\nMedian TPOT (ms):                        84.96\nP99 TPOT (ms):                           332.44\n---------------Inter-token Latency----------------\nMean ITL (ms):                           81.69\nMedian ITL (ms):                         65.37\nP99 ITL (ms):                            263.32\n==================================================\n# --max-num-batched-tokens 8192\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  81.65\nTotal input tokens:                      215312\nTotal generated tokens:                  196734\nRequest throughput (req/s):              12.25\nOutput token throughput (tok/s):         2409.38\nPeak output token throughput (tok/s):    4073.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5046.28\n---------------Time to First Token----------------\nMean TTFT (ms):                          24118.79\nMedian TTFT (ms):                        22914.78\nP99 TTFT (ms):                           56155.47\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          95.68\nMedian TPOT (ms):                        85.26\nP99 TPOT (ms):                           379.93\n---------------Inter-token Latency----------------\nMean ITL (ms):                           81.81\nMedian ITL (ms):                         65.77\nP99 ITL (ms):                            232.43\n==================================================\n</code></pre> <p>Result: Throughput improved from 4986.20 tok/s to 5046.28 tok/s when increasing max-num-batched-tokens.</p>"},{"location":"performance-lab/gpt-oss-120b/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection - Async Scheduling +4.6% Parallelism - Attention Backend - Max Number of Batched Tokens +1.2%"},{"location":"performance-lab/gpt-oss-120b/a100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  740.10\nTotal input tokens:                      3200000\nTotal generated tokens:                  9988\nRequest throughput (req/s):              0.14\nOutput token throughput (tok/s):         13.50\nPeak output token throughput (tok/s):    176.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          4337.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          368856.64\nMedian TTFT (ms):                        368713.30\nP99 TTFT (ms):                           730947.33\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          224.58\nMedian TPOT (ms):                        228.19\nP99 TPOT (ms):                           231.28\n---------------Inter-token Latency----------------\nMean ITL (ms):                           233.21\nMedian ITL (ms):                         24.23\nP99 ITL (ms):                            676.79\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  269.39\nTotal input tokens:                      1999249\nTotal generated tokens:                  67259\nRequest throughput (req/s):              1.86\nOutput token throughput (tok/s):         249.67\nPeak output token throughput (tok/s):    1146.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          7671.00\n---------------Time to First Token----------------\nMean TTFT (ms):                          133561.13\nMedian TTFT (ms):                        134006.62\nP99 TTFT (ms):                           263106.36\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          112.18\nMedian TPOT (ms):                        114.77\nP99 TPOT (ms):                           136.26\n---------------Inter-token Latency----------------\nMean ITL (ms):                           117.62\nMedian ITL (ms):                         27.99\nP99 ITL (ms):                            353.98\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  123.94\nTotal input tokens:                      999120\nTotal generated tokens:                  48981\nRequest throughput (req/s):              4.03\nOutput token throughput (tok/s):         395.21\nPeak output token throughput (tok/s):    1696.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          8456.65\n---------------Time to First Token----------------\nMean TTFT (ms):                          61328.02\nMedian TTFT (ms):                        61764.97\nP99 TTFT (ms):                           120670.44\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          131.94\nMedian TPOT (ms):                        138.30\nP99 TPOT (ms):                           150.84\n---------------Inter-token Latency----------------\nMean ITL (ms):                           139.51\nMedian ITL (ms):                         214.02\nP99 ITL (ms):                            432.26\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  14.09\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              70.99\nOutput token throughput (tok/s):         283.95\nPeak output token throughput (tok/s):    273.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          9353.12\n---------------Time to First Token----------------\nMean TTFT (ms):                          7813.18\nMedian TTFT (ms):                        7750.58\nP99 TTFT (ms):                           13935.99\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          182.75\nMedian TPOT (ms):                        189.53\nP99 TPOT (ms):                           193.12\n---------------Inter-token Latency----------------\nMean ITL (ms):                           274.25\nMedian ITL (ms):                         196.02\nP99 ITL (ms):                            386.54\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  66.52\nTotal input tokens:                      22946\nTotal generated tokens:                  21691\nRequest throughput (req/s):              1.50\nOutput token throughput (tok/s):         326.09\nPeak output token throughput (tok/s):    368.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          671.05\n---------------Time to First Token----------------\nMean TTFT (ms):                          61.91\nMedian TTFT (ms):                        61.55\nP99 TTFT (ms):                           112.05\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          11.67\nMedian TPOT (ms):                        11.44\nP99 TPOT (ms):                           15.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.74\nMedian ITL (ms):                         11.14\nP99 ITL (ms):                            38.75\n==================================================  \n</code></pre> Optimized serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling --max-num-batched-tokens 4096\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  696.89\nTotal input tokens:                      3200000\nTotal generated tokens:                  9828\nRequest throughput (req/s):              0.14\nOutput token throughput (tok/s):         14.10\nPeak output token throughput (tok/s):    196.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          4605.91\n---------------Time to First Token----------------\nMean TTFT (ms):                          346799.49\nMedian TTFT (ms):                        346737.12\nP99 TTFT (ms):                           687309.14\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          201.61\nMedian TPOT (ms):                        203.58\nP99 TPOT (ms):                           271.48\n---------------Inter-token Latency----------------\nMean ITL (ms):                           209.02\nMedian ITL (ms):                         20.50\nP99 ITL (ms):                            1206.69\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  244.13\nTotal input tokens:                      1999249\nTotal generated tokens:                  68059\nRequest throughput (req/s):              2.05\nOutput token throughput (tok/s):         278.79\nPeak output token throughput (tok/s):    1289.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          8468.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          121158.11\nMedian TTFT (ms):                        121158.59\nP99 TTFT (ms):                           238837.53\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          99.77\nMedian TPOT (ms):                        102.46\nP99 TPOT (ms):                           122.61\n---------------Inter-token Latency----------------\nMean ITL (ms):                           104.98\nMedian ITL (ms):                         25.69\nP99 ITL (ms):                            397.17\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  113.32\nTotal input tokens:                      999120\nTotal generated tokens:                  49103\nRequest throughput (req/s):              4.41\nOutput token throughput (tok/s):         433.30\nPeak output token throughput (tok/s):    1893.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          9249.89\n---------------Time to First Token----------------\nMean TTFT (ms):                          55458.38\nMedian TTFT (ms):                        56144.82\nP99 TTFT (ms):                           110079.18\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          117.85\nMedian TPOT (ms):                        125.32\nP99 TPOT (ms):                           135.13\n---------------Inter-token Latency----------------\nMean ITL (ms):                           125.08\nMedian ITL (ms):                         31.45\nP99 ITL (ms):                            575.36\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  12.22\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              81.86\nOutput token throughput (tok/s):         327.44\nPeak output token throughput (tok/s):    360.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10785.60\n---------------Time to First Token----------------\nMean TTFT (ms):                          6900.44\nMedian TTFT (ms):                        6888.59\nP99 TTFT (ms):                           11990.00\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          301.66\nMedian TPOT (ms):                        320.19\nP99 TPOT (ms):                           323.93\n---------------Inter-token Latency----------------\nMean ITL (ms):                           452.71\nMedian ITL (ms):                         327.62\nP99 ITL (ms):                            652.15\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  62.98\nTotal input tokens:                      22946\nTotal generated tokens:                  21691\nRequest throughput (req/s):              1.59\nOutput token throughput (tok/s):         344.39\nPeak output token throughput (tok/s):    387.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          708.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          71.15\nMedian TTFT (ms):                        70.23\nP99 TTFT (ms):                           131.39\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          11.03\nMedian TPOT (ms):                        10.78\nP99 TPOT (ms):                           15.48\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.07\nMedian ITL (ms):                         10.49\nP99 ITL (ms):                            36.80\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/h100/","title":"Optimizing GPT-OSS-120B Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/gpt-oss-120b/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GPT-OSS-120B on H100 GPUs:</p> Serving Command <pre><code>vllm serve openai/gpt-oss-120b --async-scheduling -tp 2\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 6095.37Mean TPOT(ms): 54.10 Total TPS: 16042.19 (+31.6%/GPU)Mean TPOT(ms): 86.54 Short Prompt Total TPS: 17059.15Mean TPOT(ms): 304.80 Total TPS: 31517.47 (-7.6%/GPU)Mean TPOT(ms): 183.05 Medium Prompt Total TPS: 14385.69Mean TPOT(ms): 41.35 Total TPS: 34083.02 (+18.5%/GPU)Mean TPOT(ms): 139.32 Long Prompt Total TPS: 14247.47Mean TPOT(ms): 31.70 Total TPS: 34491.13 (+21.1%/GPU)Mean TPOT(ms): 164.73 Very Long Prompt Total TPS: 16136.57Mean TPOT(ms): 23.99 Total TPS: 32471.01 (+0.6%/GPU)Mean TPOT(ms): 220.32 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/gpt-oss-120b/h100/#model","title":"Model","text":"<p>GPT-OSS-120B</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.10.2</li> <li>SGLang: v0.5.3rc0</li> <li>TensorRT-LLM: v1.0.0</li> </ul>"},{"location":"performance-lab/gpt-oss-120b/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/gpt-oss-120b/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model openai/gpt-oss-120b --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model openai/gpt-oss-120b --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/h100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/gpt-oss-120b/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  67.40\nTotal input tokens:                      215312\nTotal generated tokens:                  195544\nRequest throughput (req/s):              14.84\nOutput token throughput (tok/s):         2901.05\nTotal Token throughput (tok/s):          6095.37\n---------------Time to First Token----------------\nMean TTFT (ms):                          24830.45\nMedian TTFT (ms):                        25973.43\nP99 TTFT (ms):                           54033.43\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          54.10\nMedian TPOT (ms):                        47.57\nP99 TPOT (ms):                           193.25\n---------------Inter-token Latency----------------\nMean ITL (ms):                           46.65\nMedian ITL (ms):                         37.98\nP99 ITL (ms):                            121.49\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path openai/gpt-oss-120b --host 0.0.0.0 --port 8000 \n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  227.00\nTotal input tokens:                      215312\nTotal generated tokens:                  196540\nRequest throughput (req/s):              4.41\nOutput token throughput (tok/s):         865.81\nTotal Token throughput (tok/s):          1814.32\n---------------Time to First Token----------------\nMean TTFT (ms):                          132612.71\nMedian TTFT (ms):                        139831.95\nP99 TTFT (ms):                           216875.56\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          57.34\nMedian TPOT (ms):                        39.11\nP99 TPOT (ms):                           345.30\n---------------Inter-token Latency----------------\nMean ITL (ms):                           51.23\nMedian ITL (ms):                         21.25\nP99 ITL (ms):                            1328.36\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve openai/gpt-oss-120b --max_seq_len 32768\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  191.25\nTotal input tokens:                      215312\nTotal generated tokens:                  193874\nRequest throughput (req/s):              5.23\nOutput token throughput (tok/s):         1013.72\nTotal Token throughput (tok/s):          2139.54\n---------------Time to First Token----------------\nMean TTFT (ms):                          55896.64\nMedian TTFT (ms):                        49891.09\nP99 TTFT (ms):                           138440.71\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          233.20\nMedian TPOT (ms):                        213.25\nP99 TPOT (ms):                           790.11\n---------------Inter-token Latency----------------\nMean ITL (ms):                           101.13\nMedian ITL (ms):                         53.81\nP99 ITL (ms):                            389.65\n==================================================\n</code></pre> <p>Result: vLLM (6095.37 tok/s) &gt; TensorRT-LLM (2139.54 tok/s) &gt; SGLang(1814.32 tok/s)</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#2-async-scheduling-in-vllm","title":"2. Async scheduling in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  63.65\nTotal input tokens:                      215312\nTotal generated tokens:                  187289\nRequest throughput (req/s):              15.71\nOutput token throughput (tok/s):         2942.57\nTotal Token throughput (tok/s):          6325.41\n---------------Time to First Token----------------\nMean TTFT (ms):                          23629.95\nMedian TTFT (ms):                        24595.25\nP99 TTFT (ms):                           51018.93\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          52.66\nMedian TPOT (ms):                        46.51\nP99 TPOT (ms):                           182.00\n---------------Inter-token Latency----------------\nMean ITL (ms):                           45.31\nMedian ITL (ms):                         36.40\nP99 ITL (ms):                            111.51\n==================================================\n</code></pre> <p>Result: Throughput improved from 6095.37 tok/s to 6325.41 tok/s by enabling async scheduling.</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#3-parallelism-in-vllm","title":"3. Parallelism in vLLM","text":"<p>TP2</p> Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling -tp 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  25.65\nTotal input tokens:                      215312\nTotal generated tokens:                  196205\nRequest throughput (req/s):              38.98\nOutput token throughput (tok/s):         7648.67\nTotal Token throughput (tok/s):          16042.19\n---------------Time to First Token----------------\nMean TTFT (ms):                          4612.68\nMedian TTFT (ms):                        4600.39\nP99 TTFT (ms):                           7587.07\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          86.54\nMedian TPOT (ms):                        51.57\nP99 TPOT (ms):                           215.21\n---------------Inter-token Latency----------------\nMean ITL (ms):                           43.02\nMedian ITL (ms):                         31.37\nP99 ITL (ms):                            223.98\n==================================================\n</code></pre> <p>TP4</p> Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling -tp 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  17.32\nTotal input tokens:                      215312\nTotal generated tokens:                  196174\nRequest throughput (req/s):              57.74\nOutput token throughput (tok/s):         11327.22\nTotal Token throughput (tok/s):          23759.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          2952.04\nMedian TTFT (ms):                        2871.45\nP99 TTFT (ms):                           4867.46\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          55.77\nMedian TPOT (ms):                        34.49\nP99 TPOT (ms):                           136.33\n---------------Inter-token Latency----------------\nMean ITL (ms):                           28.57\nMedian ITL (ms):                         21.75\nP99 ITL (ms):                            149.71\n==================================================\n</code></pre> <p>PP2</p> Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling -pp 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  36.74\nTotal input tokens:                      215312\nTotal generated tokens:                  196089\nRequest throughput (req/s):              27.22\nOutput token throughput (tok/s):         5337.65\nTotal Token throughput (tok/s):          11198.56\n---------------Time to First Token----------------\nMean TTFT (ms):                          4471.76\nMedian TTFT (ms):                        4392.85\nP99 TTFT (ms):                           6960.63\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          115.37\nMedian TPOT (ms):                        69.62\nP99 TPOT (ms):                           371.01\n---------------Inter-token Latency----------------\nMean ITL (ms):                           60.87\nMedian ITL (ms):                         50.83\nP99 ITL (ms):                            374.02\n==================================================\n</code></pre> <p>Result: TP2 improves throughput from 6325.41 tok/s to 16042.19 tok/s, achieving 26.8% higher token throughput per GPU.</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#4-attention-backend-in-vllm","title":"4. Attention Backend in vLLM","text":"<p>FlashAttention is the default.</p> <p>FlashInfer</p> Serving script <pre><code>VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Benchmark result <pre><code>RuntimeError: Worker failed with error 'FlashInfer backend currently does not support attention sinks, please use trtllm on blackwell or flash attention on earlier GPUs.', please check the stack trace above for the root cause\n</code></pre> <p>XFormers</p> Serving script <pre><code>VLLM_ATTENTION_BACKEND=XFORMERS vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Benchmark result <pre><code>TypeError: XFormersImpl.__init__() got an unexpected keyword argument 'sinks'\n</code></pre>"},{"location":"performance-lab/gpt-oss-120b/h100/#5-max-number-of-batched-tokens-in-vllm","title":"5. Max Number of Batched Tokens in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768 --async-scheduling --max-num-batched-tokens 16384\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  65.28\nTotal input tokens:                      215312\nTotal generated tokens:                  187636\nRequest throughput (req/s):              15.32\nOutput token throughput (tok/s):         2874.47\nTotal Token throughput (tok/s):          6172.93\n---------------Time to First Token----------------\nMean TTFT (ms):                          23977.21\nMedian TTFT (ms):                        24694.18\nP99 TTFT (ms):                           51339.38\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          51.18\nMedian TPOT (ms):                        45.25\nP99 TPOT (ms):                           164.63\n---------------Inter-token Latency----------------\nMean ITL (ms):                           44.26\nMedian ITL (ms):                         35.89\nP99 ITL (ms):                            109.05\n==================================================\n</code></pre> <p>Result: Throughput slightly decreased when increasing max-num-batched-tokens.</p>"},{"location":"performance-lab/gpt-oss-120b/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection - Async Scheduling +3.8% Parallelism +26.8% Attention Backend - Max Number of Batched Tokens -"},{"location":"performance-lab/gpt-oss-120b/h100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve openai/gpt-oss-120b --max-model-len 32768\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  198.92\nTotal input tokens:                      3200000\nTotal generated tokens:                  9877\nRequest throughput (req/s):              0.50\nOutput token throughput (tok/s):         49.65\nTotal Token throughput (tok/s):          16136.57\n---------------Time to First Token----------------\nMean TTFT (ms):                          99444.04\nMedian TTFT (ms):                        99439.56\nP99 TTFT (ms):                           196214.22\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          23.99\nMedian TPOT (ms):                        23.69\nP99 TPOT (ms):                           31.32\n---------------Inter-token Latency----------------\nMean ITL (ms):                           25.13\nMedian ITL (ms):                         7.26\nP99 ITL (ms):                            454.68\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  145.04\nTotal input tokens:                      1999249\nTotal generated tokens:                  67205\nRequest throughput (req/s):              3.45\nOutput token throughput (tok/s):         463.35\nTotal Token throughput (tok/s):          14247.47\n---------------Time to First Token----------------\nMean TTFT (ms):                          72954.68\nMedian TTFT (ms):                        73023.24\nP99 TTFT (ms):                           141744.03\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          31.70\nMedian TPOT (ms):                        32.02\nP99 TPOT (ms):                           37.43\n---------------Inter-token Latency----------------\nMean ITL (ms):                           33.25\nMedian ITL (ms):                         12.37\nP99 ITL (ms):                            191.40\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  72.87\nTotal input tokens:                      999120\nTotal generated tokens:                  49197\nRequest throughput (req/s):              6.86\nOutput token throughput (tok/s):         675.11\nTotal Token throughput (tok/s):          14385.69\n---------------Time to First Token----------------\nMean TTFT (ms):                          36234.76\nMedian TTFT (ms):                        35662.58\nP99 TTFT (ms):                           70634.63\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          41.35\nMedian TPOT (ms):                        42.86\nP99 TPOT (ms):                           48.36\n---------------Inter-token Latency----------------\nMean ITL (ms):                           43.61\nMedian ITL (ms):                         14.92\nP99 ITL (ms):                            309.43\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  7.72\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              129.48\nOutput token throughput (tok/s):         517.91\nTotal Token throughput (tok/s):          17059.15\n---------------Time to First Token----------------\nMean TTFT (ms):                          4850.46\nMedian TTFT (ms):                        4987.41\nP99 TTFT (ms):                           7372.98\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          304.80\nMedian TPOT (ms):                        341.94\nP99 TPOT (ms):                           360.40\n---------------Inter-token Latency----------------\nMean ITL (ms):                           467.00\nMedian ITL (ms):                         352.58\nP99 ITL (ms):                            728.75\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  47.88\nTotal input tokens:                      22946\nTotal generated tokens:                  21691\nRequest throughput (req/s):              2.09\nOutput token throughput (tok/s):         453.04\nTotal Token throughput (tok/s):          932.29\n---------------Time to First Token----------------\nMean TTFT (ms):                          49.00\nMedian TTFT (ms):                        47.49\nP99 TTFT (ms):                           107.46\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          8.31\nMedian TPOT (ms):                        8.24\nP99 TPOT (ms):                           10.12\n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.40\nMedian ITL (ms):                         7.99\nP99 ITL (ms):                            23.32\n==================================================\n</code></pre> Optimized serving script <pre><code>vllm serve openai/gpt-oss-120b --async-scheduling -tp 2\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  98.86\nTotal input tokens:                      3200000\nTotal generated tokens:                  9955\nRequest throughput (req/s):              1.01\nOutput token throughput (tok/s):         100.70\nTotal Token throughput (tok/s):          32471.01\n---------------Time to First Token----------------\nMean TTFT (ms):                          48826.05\nMedian TTFT (ms):                        48671.00\nP99 TTFT (ms):                           96651.74\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          220.32\nMedian TPOT (ms):                        252.27\nP99 TPOT (ms):                           252.79\n---------------Inter-token Latency----------------\nMean ITL (ms):                           228.81\nMedian ITL (ms):                         246.94\nP99 ITL (ms):                            481.43\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  59.94\nTotal input tokens:                      1999249\nTotal generated tokens:                  68046\nRequest throughput (req/s):              8.34\nOutput token throughput (tok/s):         1135.29\nTotal Token throughput (tok/s):          34491.13\n---------------Time to First Token----------------\nMean TTFT (ms):                          29487.34\nMedian TTFT (ms):                        29235.61\nP99 TTFT (ms):                           56201.04\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          164.73\nMedian TPOT (ms):                        199.03\nP99 TPOT (ms):                           220.23\n---------------Inter-token Latency----------------\nMean ITL (ms):                           171.11\nMedian ITL (ms):                         215.59\nP99 ITL (ms):                            431.23\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  30.75\nTotal input tokens:                      999120\nTotal generated tokens:                  49059\nRequest throughput (req/s):              16.26\nOutput token throughput (tok/s):         1595.22\nTotal Token throughput (tok/s):          34083.02\n---------------Time to First Token----------------\nMean TTFT (ms):                          15291.77\nMedian TTFT (ms):                        15170.31\nP99 TTFT (ms):                           28437.70\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          139.32\nMedian TPOT (ms):                        148.04\nP99 TPOT (ms):                           212.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           147.37\nMedian ITL (ms):                         208.40\nP99 ITL (ms):                            421.37\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  4.18\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              239.21\nOutput token throughput (tok/s):         956.85\nTotal Token throughput (tok/s):          31517.47\n---------------Time to First Token----------------\nMean TTFT (ms):                          2800.15\nMedian TTFT (ms):                        2412.28\nP99 TTFT (ms):                           3856.95\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          183.05\nMedian TPOT (ms):                        208.66\nP99 TPOT (ms):                           232.15\n---------------Inter-token Latency----------------\nMean ITL (ms):                           274.85\nMedian ITL (ms):                         267.83\nP99 ITL (ms):                            506.27\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  33.60\nTotal input tokens:                      22946\nTotal generated tokens:                  21691\nRequest throughput (req/s):              2.98\nOutput token throughput (tok/s):         645.55\nTotal Token throughput (tok/s):          1328.44\n---------------Time to First Token----------------\nMean TTFT (ms):                          37.51\nMedian TTFT (ms):                        36.68\nP99 TTFT (ms):                           58.54\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          5.75\nMedian TPOT (ms):                        5.74\nP99 TPOT (ms):                           6.91\n---------------Inter-token Latency----------------\nMean ITL (ms):                           5.89\nMedian ITL (ms):                         5.54\nP99 ITL (ms):                            23.43\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/a100/","title":"Optimizing GPT-OSS-20B Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/gpt-oss-20b/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GPT-OSS-20B on A100 GPUs:</p> Serving Command <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 9743.28Mean TPOT(ms): 48.57 Total TPS: 10919.92 (+12.1%)Mean TPOT(ms): 43.25 Short Prompt Total TPS: 24223.22Mean TPOT(ms): 62.10 Total TPS: 25172.82 (+4.0%)Mean TPOT(ms): 55.31 Medium Prompt Total TPS: 18021.34Mean TPOT(ms): 100.32 Total TPS: 18920.16 (+5.0%)Mean TPOT(ms): 95.80 Long Prompt Total TPS: 15938.87Mean TPOT(ms): 123.35 Total TPS: 16550.81 (+3.8%)Mean TPOT(ms): 119.37 Very Long Prompt Total TPS: 7280.33Mean TPOT(ms): 268.03 Total TPS: 7363.09 (+1.1%)Mean TPOT(ms): 264.93 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/gpt-oss-20b/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/gpt-oss-20b/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/gpt-oss-20b/a100/#model","title":"Model","text":"<p>GPT-OSS-20B</p>"},{"location":"performance-lab/gpt-oss-20b/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/gpt-oss-20b/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.4.post2</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/gpt-oss-20b/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/gpt-oss-20b/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model openai/gpt-oss-20b --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model openai/gpt-oss-20b --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/a100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/gpt-oss-20b/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve openai/gpt-oss-20b\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  42.03\nTotal input tokens:                      215312\nTotal generated tokens:                  194171\nRequest throughput (req/s):              23.79\nOutput token throughput (tok/s):         4620.12\nPeak output token throughput (tok/s):    7484.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          9743.28\n---------------Time to First Token----------------\nMean TTFT (ms):                          13654.59\nMedian TTFT (ms):                        13063.93\nP99 TTFT (ms):                           30306.72\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          48.57\nMedian TPOT (ms):                        45.15\nP99 TPOT (ms):                           110.58\n---------------Inter-token Latency----------------\nMean ITL (ms):                           41.95\nMedian ITL (ms):                         33.96\nP99 ITL (ms):                            112.27\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path openai/gpt-oss-20b\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  735.43\nTotal input tokens:                      215312\nTotal generated tokens:                  195617\nRequest throughput (req/s):              1.36\nOutput token throughput (tok/s):         265.99\nPeak output token throughput (tok/s):    5848.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          558.76\n---------------Time to First Token----------------\nMean TTFT (ms):                          140888.75\nMedian TTFT (ms):                        137990.65\nP99 TTFT (ms):                           274130.28\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          6243.98\nMedian TPOT (ms):                        2508.45\nP99 TPOT (ms):                           46225.15\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1889.78\nMedian ITL (ms):                         1305.64\nP99 ITL (ms):                            4248.44\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve openai/gpt-oss-120b --max_seq_len 32768\n</code></pre> Benchmark result <pre><code>NotImplementedError: WFP4A16 MoE is unsupported on SM80.\n</code></pre> <p>Result: vLLM (9743.28 tok/s) &gt; SGLang(558.76 tok/s) &gt; TensorRT-LLM (Not supported)</p>"},{"location":"performance-lab/gpt-oss-20b/a100/#2-async-scheduling-in-vllm","title":"2. Async scheduling in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  37.51\nTotal input tokens:                      215312\nTotal generated tokens:                  194303\nRequest throughput (req/s):              26.66\nOutput token throughput (tok/s):         5179.92\nPeak output token throughput (tok/s):    8545.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10919.92\n---------------Time to First Token----------------\nMean TTFT (ms):                          12371.62\nMedian TTFT (ms):                        11647.98\nP99 TTFT (ms):                           27040.92\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          43.25\nMedian TPOT (ms):                        39.47\nP99 TPOT (ms):                           105.22\n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.85\nMedian ITL (ms):                         28.41\nP99 ITL (ms):                            106.78\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/a100/#3-tuning-number-of-batched-tokens-in-vllm","title":"3. Tuning number of batched tokens in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling --max-num-batched-tokens 8192\n</code></pre> Benchmark result <pre><code># --max-num-batched-tokens 4096\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  37.43\nTotal input tokens:                      215312\nTotal generated tokens:                  194145\nRequest throughput (req/s):              26.72\nOutput token throughput (tok/s):         5186.81\nPeak output token throughput (tok/s):    8601.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10939.12\n---------------Time to First Token----------------\nMean TTFT (ms):                          12298.27\nMedian TTFT (ms):                        11609.21\nP99 TTFT (ms):                           27015.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          44.99\nMedian TPOT (ms):                        39.55\nP99 TPOT (ms):                           195.62\n---------------Inter-token Latency----------------\nMean ITL (ms):                           37.00\nMedian ITL (ms):                         28.57\nP99 ITL (ms):                            118.90\n==================================================\n# --max-num-batched-tokens 8192\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  37.58\nTotal input tokens:                      215312\nTotal generated tokens:                  193986\nRequest throughput (req/s):              26.61\nOutput token throughput (tok/s):         5161.85\nPeak output token throughput (tok/s):    8663.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10891.17\n---------------Time to First Token----------------\nMean TTFT (ms):                          12479.83\nMedian TTFT (ms):                        11828.63\nP99 TTFT (ms):                           27160.43\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          45.94\nMedian TPOT (ms):                        39.39\nP99 TPOT (ms):                           234.47\n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.95\nMedian ITL (ms):                         28.33\nP99 ITL (ms):                            101.21\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection - Async Scheduling +12.1% Batched Tokens -"},{"location":"performance-lab/gpt-oss-20b/a100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve openai/gpt-oss-20b\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  440.85\nTotal input tokens:                      3200000\nTotal generated tokens:                  9533\nRequest throughput (req/s):              0.23\nOutput token throughput (tok/s):         21.62\nPeak output token throughput (tok/s):    203.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          7280.33\n---------------Time to First Token----------------\nMean TTFT (ms):                          219739.77\nMedian TTFT (ms):                        219663.48\nP99 TTFT (ms):                           435080.24\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          268.03\nMedian TPOT (ms):                        278.17\nP99 TPOT (ms):                           282.40\n---------------Inter-token Latency----------------\nMean ITL (ms):                           276.43\nMedian ITL (ms):                         274.32\nP99 ITL (ms):                            438.91\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  128.79\nTotal input tokens:                      1999249\nTotal generated tokens:                  53530\nRequest throughput (req/s):              3.88\nOutput token throughput (tok/s):         415.64\nPeak output token throughput (tok/s):    1983.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          15938.87\n---------------Time to First Token----------------\nMean TTFT (ms):                          62947.13\nMedian TTFT (ms):                        61941.78\nP99 TTFT (ms):                           125429.94\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          123.35\nMedian TPOT (ms):                        129.79\nP99 TPOT (ms):                           131.42\n---------------Inter-token Latency----------------\nMean ITL (ms):                           130.61\nMedian ITL (ms):                         129.50\nP99 ITL (ms):                            262.38\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  58.06\nTotal input tokens:                      999120\nTotal generated tokens:                  47224\nRequest throughput (req/s):              8.61\nOutput token throughput (tok/s):         813.35\nPeak output token throughput (tok/s):    4701.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          18021.34\n---------------Time to First Token----------------\nMean TTFT (ms):                          31669.55\nMedian TTFT (ms):                        32083.12\nP99 TTFT (ms):                           55891.47\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          100.32\nMedian TPOT (ms):                        119.20\nP99 TPOT (ms):                           119.71\n---------------Inter-token Latency----------------\nMean ITL (ms):                           106.87\nMedian ITL (ms):                         119.09\nP99 ITL (ms):                            239.78\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  5.44\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              183.85\nOutput token throughput (tok/s):         735.40\nPeak output token throughput (tok/s):    932.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          24223.22\n---------------Time to First Token----------------\nMean TTFT (ms):                          2774.46\nMedian TTFT (ms):                        1971.28\nP99 TTFT (ms):                           5298.60\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          62.10\nMedian TPOT (ms):                        65.44\nP99 TPOT (ms):                           106.62\n---------------Inter-token Latency----------------\nMean ITL (ms):                           100.20\nMedian ITL (ms):                         105.43\nP99 ITL (ms):                            213.73\n==================================================\n</code></pre> Optimized serving script <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  435.88\nTotal input tokens:                      3200000\nTotal generated tokens:                  9394\nRequest throughput (req/s):              0.23\nOutput token throughput (tok/s):         21.55\nPeak output token throughput (tok/s):    239.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          7363.09\n---------------Time to First Token----------------\nMean TTFT (ms):                          217408.40\nMedian TTFT (ms):                        217388.75\nP99 TTFT (ms):                           430331.45\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          264.93\nMedian TPOT (ms):                        275.04\nP99 TPOT (ms):                           278.94\n---------------Inter-token Latency----------------\nMean ITL (ms):                           274.36\nMedian ITL (ms):                         274.48\nP99 ITL (ms):                            433.63\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  123.99\nTotal input tokens:                      1999249\nTotal generated tokens:                  52857\nRequest throughput (req/s):              4.03\nOutput token throughput (tok/s):         426.31\nPeak output token throughput (tok/s):    1699.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          16550.81\n---------------Time to First Token----------------\nMean TTFT (ms):                          60587.64\nMedian TTFT (ms):                        59590.47\nP99 TTFT (ms):                           121079.49\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          119.37\nMedian TPOT (ms):                        125.63\nP99 TPOT (ms):                           126.57\n---------------Inter-token Latency----------------\nMean ITL (ms):                           126.46\nMedian ITL (ms):                         125.29\nP99 ITL (ms):                            253.57\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  55.29\nTotal input tokens:                      999120\nTotal generated tokens:                  46961\nRequest throughput (req/s):              9.04\nOutput token throughput (tok/s):         849.37\nPeak output token throughput (tok/s):    4961.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          18920.16\n---------------Time to First Token----------------\nMean TTFT (ms):                          30030.76\nMedian TTFT (ms):                        30306.12\nP99 TTFT (ms):                           53186.39\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          95.80\nMedian TPOT (ms):                        114.20\nP99 TPOT (ms):                           114.34\n---------------Inter-token Latency----------------\nMean ITL (ms):                           102.24\nMedian ITL (ms):                         114.01\nP99 ITL (ms):                            229.30\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  5.23\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              191.06\nOutput token throughput (tok/s):         764.23\nPeak output token throughput (tok/s):    1302.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          25172.82\n---------------Time to First Token----------------\nMean TTFT (ms):                          2682.01\nMedian TTFT (ms):                        1944.72\nP99 TTFT (ms):                           5078.68\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          55.31\nMedian TPOT (ms):                        46.59\nP99 TPOT (ms):                           102.22\n---------------Inter-token Latency----------------\nMean ITL (ms):                           85.84\nMedian ITL (ms):                         100.82\nP99 ITL (ms):                            205.12\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/h100/","title":"Optimizing GPT-OSS-20B Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/gpt-oss-20b/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of GPT-OSS-20B on H100 GPUs:</p> Serving Command <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 16701.31Mean TPOT(ms): 91.33 Total TPS: 19128.79 (+14.5%)Mean TPOT(ms): 86.81 Short Prompt Total TPS: 28032.23Mean TPOT(ms): 178.91 Total TPS: 32611.91 (+16.3%)Mean TPOT(ms): 75.14 Medium Prompt Total TPS: 32113.65Mean TPOT(ms): 149.76 Total TPS: 33149.54 (+3.2%)Mean TPOT(ms): 146.63 Long Prompt Total TPS: 33236.17Mean TPOT(ms): 184.26 Total TPS: 33823.34 (+1.8%)Mean TPOT(ms): 182.69 Very Long Prompt Total TPS: 29730.30Mean TPOT(ms): 239.44 Total TPS: 29718.21 (-0.0%)Mean TPOT(ms): 239.49 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/gpt-oss-20b/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/gpt-oss-20b/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/gpt-oss-20b/h100/#model","title":"Model","text":"<p>GPT-OSS-20B</p>"},{"location":"performance-lab/gpt-oss-20b/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/gpt-oss-20b/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.4.post2</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/gpt-oss-20b/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/gpt-oss-20b/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model openai/gpt-oss-20b --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model openai/gpt-oss-20b --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/h100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/gpt-oss-20b/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve openai/gpt-oss-20b\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  24.81\nTotal input tokens:                      215312\nTotal generated tokens:                  193979\nRequest throughput (req/s):              40.30\nOutput token throughput (tok/s):         7817.03\nPeak output token throughput (tok/s):    15953.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          16493.75\n---------------Time to First Token----------------\nMean TTFT (ms):                          4881.62\nMedian TTFT (ms):                        4791.97\nP99 TTFT (ms):                           8166.01\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          91.00\nMedian TPOT (ms):                        55.52\nP99 TPOT (ms):                           237.48\n---------------Inter-token Latency----------------\nMean ITL (ms):                           44.98\nMedian ITL (ms):                         31.92\nP99 ITL (ms):                            248.25\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path openai/gpt-oss-20b\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  73.05\nTotal input tokens:                      215312\nTotal generated tokens:                  195734\nRequest throughput (req/s):              13.69\nOutput token throughput (tok/s):         2679.49\nPeak output token throughput (tok/s):    13804.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5626.99\n---------------Time to First Token----------------\nMean TTFT (ms):                          10412.75\nMedian TTFT (ms):                        10326.68\nP99 TTFT (ms):                           11920.92\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          314.53\nMedian TPOT (ms):                        215.95\nP99 TPOT (ms):                           1052.78\n---------------Inter-token Latency----------------\nMean ITL (ms):                           161.29\nMedian ITL (ms):                         47.78\nP99 ITL (ms):                            1929.13\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve openai/gpt-oss-20b\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  53.03\nTotal input tokens:                      215312\nTotal generated tokens:                  191726\nRequest throughput (req/s):              18.86\nOutput token throughput (tok/s):         3615.45\nPeak output token throughput (tok/s):    18641.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7675.67\n---------------Time to First Token----------------\nMean TTFT (ms):                          12248.87\nMedian TTFT (ms):                        12024.20\nP99 TTFT (ms):                           22142.67\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          224.49\nMedian TPOT (ms):                        134.32\nP99 TPOT (ms):                           552.52\n---------------Inter-token Latency----------------\nMean ITL (ms):                           51.52\nMedian ITL (ms):                         0.07\nP99 ITL (ms):                            963.17\n==================================================\n</code></pre> <p>Result: vLLM (16493.75 tok/s) &gt; TensorRT-LLM (7675.67 tok/s) &gt; SGLang(5626.99 tok/s)</p>"},{"location":"performance-lab/gpt-oss-20b/h100/#2-async-scheduling-in-vllm","title":"2. Async scheduling in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  21.39\nTotal input tokens:                      215312\nTotal generated tokens:                  193919\nRequest throughput (req/s):              46.74\nOutput token throughput (tok/s):         9064.40\nPeak output token throughput (tok/s):    22786.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          19128.79\n---------------Time to First Token----------------\nMean TTFT (ms):                          4933.91\nMedian TTFT (ms):                        4761.74\nP99 TTFT (ms):                           8029.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          86.81\nMedian TPOT (ms):                        52.23\nP99 TPOT (ms):                           222.47\n---------------Inter-token Latency----------------\nMean ITL (ms):                           38.92\nMedian ITL (ms):                         27.02\nP99 ITL (ms):                            236.29\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/h100/#5-tuning-number-of-batched-tokens-in-vllm","title":"5. Tuning number of batched tokens in vLLM","text":"Serving script <pre><code>vllm serve openai/gpt-oss-120b --async-scheduling --max-num-batched-tokens 16384\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  21.56\nTotal input tokens:                      215312\nTotal generated tokens:                  194591\nRequest throughput (req/s):              46.39\nOutput token throughput (tok/s):         9027.14\nPeak output token throughput (tok/s):    21433.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          19015.54\n---------------Time to First Token----------------\nMean TTFT (ms):                          4957.11\nMedian TTFT (ms):                        4698.76\nP99 TTFT (ms):                           7875.07\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          104.05\nMedian TPOT (ms):                        46.28\nP99 TPOT (ms):                           443.08\n---------------Inter-token Latency----------------\nMean ITL (ms):                           38.42\nMedian ITL (ms):                         24.01\nP99 ITL (ms):                            448.17\n==================================================\n</code></pre>"},{"location":"performance-lab/gpt-oss-20b/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection - Async Scheduling +16.0% Batched Tokens -"},{"location":"performance-lab/gpt-oss-20b/h100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve openai/gpt-oss-20b\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  107.95\nTotal input tokens:                      3200000\nTotal generated tokens:                  9350\nRequest throughput (req/s):              0.93\nOutput token throughput (tok/s):         86.62\nTotal Token throughput (tok/s):          29730.30\n---------------Time to First Token----------------\nMean TTFT (ms):                          53687.69\nMedian TTFT (ms):                        53539.25\nP99 TTFT (ms):                           105655.63\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          239.44\nMedian TPOT (ms):                        273.86\nP99 TPOT (ms):                           275.66\n---------------Inter-token Latency----------------\nMean ITL (ms):                           247.74\nMedian ITL (ms):                         266.37\nP99 ITL (ms):                            513.87\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  61.73\nTotal input tokens:                      1999249\nTotal generated tokens:                  52309\nRequest throughput (req/s):              8.10\nOutput token throughput (tok/s):         847.43\nTotal Token throughput (tok/s):          33236.17\n---------------Time to First Token----------------\nMean TTFT (ms):                          31245.12\nMedian TTFT (ms):                        30820.97\nP99 TTFT (ms):                           58702.17\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          184.26\nMedian TPOT (ms):                        225.55\nP99 TPOT (ms):                           230.22\n---------------Inter-token Latency----------------\nMean ITL (ms):                           192.92\nMedian ITL (ms):                         227.42\nP99 ITL (ms):                            458.94\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  32.58\nTotal input tokens:                      999120\nTotal generated tokens:                  47022\nRequest throughput (req/s):              15.35\nOutput token throughput (tok/s):         1443.44\nTotal Token throughput (tok/s):          32113.65\n---------------Time to First Token----------------\nMean TTFT (ms):                          16515.86\nMedian TTFT (ms):                        16363.96\nP99 TTFT (ms):                           30359.34\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          149.76\nMedian TPOT (ms):                        159.20\nP99 TPOT (ms):                           225.79\n---------------Inter-token Latency----------------\nMean ITL (ms):                           159.21\nMedian ITL (ms):                         220.69\nP99 ITL (ms):                            450.04\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  4.70\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              212.76\nOutput token throughput (tok/s):         851.04\nTotal Token throughput (tok/s):          28032.23\n---------------Time to First Token----------------\nMean TTFT (ms):                          3174.14\nMedian TTFT (ms):                        2892.24\nP99 TTFT (ms):                           4415.25\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          178.91\nMedian TPOT (ms):                        208.03\nP99 TPOT (ms):                           246.08\n---------------Inter-token Latency----------------\nMean ITL (ms):                           274.82\nMedian ITL (ms):                         228.45\nP99 ITL (ms):                            509.42\n==================================================\n</code></pre> Optimized serving script <pre><code>vllm serve openai/gpt-oss-20b --async-scheduling\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  107.99\nTotal input tokens:                      3200000\nTotal generated tokens:                  9404\nRequest throughput (req/s):              0.93\nOutput token throughput (tok/s):         87.08\nPeak output token throughput (tok/s):    1196.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          29718.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          53565.41\nMedian TTFT (ms):                        53436.03\nP99 TTFT (ms):                           105793.87\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          239.49\nMedian TPOT (ms):                        275.30\nP99 TPOT (ms):                           276.86\n---------------Inter-token Latency----------------\nMean ITL (ms):                           247.68\nMedian ITL (ms):                         269.91\nP99 ITL (ms):                            515.55\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  60.67\nTotal input tokens:                      1999249\nTotal generated tokens:                  52966\nRequest throughput (req/s):              8.24\nOutput token throughput (tok/s):         872.95\nPeak output token throughput (tok/s):    7931.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          33823.34\n---------------Time to First Token----------------\nMean TTFT (ms):                          30513.46\nMedian TTFT (ms):                        30091.29\nP99 TTFT (ms):                           58206.28\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          182.69\nMedian TPOT (ms):                        224.54\nP99 TPOT (ms):                           229.32\n---------------Inter-token Latency----------------\nMean ITL (ms):                           191.65\nMedian ITL (ms):                         226.01\nP99 ITL (ms):                            458.93\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  31.55\nTotal input tokens:                      999120\nTotal generated tokens:                  46814\nRequest throughput (req/s):              15.85\nOutput token throughput (tok/s):         1483.71\nPeak output token throughput (tok/s):    12476.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          33149.54\n---------------Time to First Token----------------\nMean TTFT (ms):                          16044.42\nMedian TTFT (ms):                        16014.33\nP99 TTFT (ms):                           29736.08\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          146.63\nMedian TPOT (ms):                        154.39\nP99 TPOT (ms):                           223.69\n---------------Inter-token Latency----------------\nMean ITL (ms):                           155.89\nMedian ITL (ms):                         216.15\nP99 ITL (ms):                            447.46\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  4.04\nTotal input tokens:                      127755\nTotal generated tokens:                  4000\nRequest throughput (req/s):              247.52\nOutput token throughput (tok/s):         990.08\nPeak output token throughput (tok/s):    1364.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          32611.91\n---------------Time to First Token----------------\nMean TTFT (ms):                          2884.78\nMedian TTFT (ms):                        2641.45\nP99 TTFT (ms):                           3879.45\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          75.14\nMedian TPOT (ms):                        16.98\nP99 TPOT (ms):                           221.60\n---------------Inter-token Latency----------------\nMean ITL (ms):                           156.10\nMedian ITL (ms):                         151.36\nP99 ITL (ms):                            452.14\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-14b/a100/","title":"Optimizing Qwen3-14B Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/qwen3-14b/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-14B on A100 GPUs:</p> Serving Command <pre><code>trtllm-serve Qwen/Qwen3-14B --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 3922.41Mean TPOT(ms): 114.34 Total TPS: 5770.24 (+47.1%)Mean TPOT(ms): 260.46 Short Prompt Total TPS: 4916.63Mean TPOT(ms): 233.35 Total TPS: 8762.17 (+78.2%)Mean TPOT(ms): 715.19 Medium Prompt Total TPS: 5967.13Mean TPOT(ms): 301.73 Total TPS: 7474.00 (+25.3%)Mean TPOT(ms): 313.69 Long Prompt Total TPS: 5153.42Mean TPOT(ms): 254.61 Total TPS: 6418.65 (+24.6%)Mean TPOT(ms): 196.10 Very Long Prompt Total TPS: 4430.14Mean TPOT(ms): 445.58 Total TPS: 4413.82 (-0.4%)Mean TPOT(ms): 493.18 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-14b/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-14b/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-14b/a100/#model","title":"Model","text":"<p>Qwen3-14B</p>"},{"location":"performance-lab/qwen3-14b/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/qwen3-14b/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.5.post1</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/qwen3-14b/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-14b/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-14B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-14B --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-14b/a100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-14b/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-14B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  106.82\nTotal input tokens:                      217393\nTotal generated tokens:                  201605\nRequest throughput (req/s):              9.36\nOutput token throughput (tok/s):         1887.31\nPeak output token throughput (tok/s):    4616.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3922.41\n---------------Time to First Token----------------\nMean TTFT (ms):                          41875.09\nMedian TTFT (ms):                        39151.17\nP99 TTFT (ms):                           83576.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          114.34\nMedian TPOT (ms):                        106.15\nP99 TPOT (ms):                           262.52\n---------------Inter-token Latency----------------\nMean ITL (ms):                           95.04\nMedian ITL (ms):                         85.55\nP99 ITL (ms):                            271.69\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-14B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  79.35\nTotal input tokens:                      217393\nTotal generated tokens:                  201706\nRequest throughput (req/s):              12.60\nOutput token throughput (tok/s):         2542.06\nPeak output token throughput (tok/s):    6721.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5281.81\n---------------Time to First Token----------------\nMean TTFT (ms):                          20365.34\nMedian TTFT (ms):                        19226.41\nP99 TTFT (ms):                           37314.85\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          364.93\nMedian TPOT (ms):                        185.18\nP99 TPOT (ms):                           2181.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           141.30\nMedian ITL (ms):                         84.71\nP99 ITL (ms):                            566.50\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-14B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  72.71\nTotal input tokens:                      217393\nTotal generated tokens:                  201471\nRequest throughput (req/s):              13.75\nOutput token throughput (tok/s):         2770.99\nPeak output token throughput (tok/s):    7336.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5760.97\n---------------Time to First Token----------------\nMean TTFT (ms):                          15258.26\nMedian TTFT (ms):                        14210.29\nP99 TTFT (ms):                           37520.64\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          261.77\nMedian TPOT (ms):                        166.55\nP99 TPOT (ms):                           915.21\n---------------Inter-token Latency----------------\nMean ITL (ms):                           130.55\nMedian ITL (ms):                         85.40\nP99 ITL (ms):                            931.59\n==================================================\n</code></pre> <p>Result: TensorRT-LLM (5760.97 tok/s) &gt; SGLang (5281.81 tok/s) &gt; vLLM (3922.41 tok/s)</p>"},{"location":"performance-lab/qwen3-14b/a100/#2-quantization-in-tensorrt-llmvllm","title":"2. Quantization in TensorRT-LLM/vLLM","text":"<p>TensorRT-LLM FP8</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-14B-FP8\n</code></pre> Benchmark result <pre><code># RuntimeError: Unsupported SM version for FP8 block scaling GEMM\n</code></pre> <p>TensorRT-LLM AWQ</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-14B-AWQ\n</code></pre> Benchmark result <pre><code># KeyError: 'weight'\n</code></pre> <p>vLLM FP8</p> Serving script <pre><code>vllm serve Qwen/Qwen3-14B-FP8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  125.66\nTotal input tokens:                      217393\nTotal generated tokens:                  201655\nRequest throughput (req/s):              7.96\nOutput token throughput (tok/s):         1604.75\nPeak output token throughput (tok/s):    3823.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3334.74\n---------------Time to First Token----------------\nMean TTFT (ms):                          52287.47\nMedian TTFT (ms):                        48838.08\nP99 TTFT (ms):                           104986.53\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          146.36\nMedian TPOT (ms):                        132.69\nP99 TPOT (ms):                           372.46\n---------------Inter-token Latency----------------\nMean ITL (ms):                           118.59\nMedian ITL (ms):                         96.50\nP99 ITL (ms):                            380.19\n==================================================\n</code></pre> <p>vLLM AWQ</p> Serving script <pre><code>vllm serve Qwen/Qwen3-14B-AWQ\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  111.81\nTotal input tokens:                      217393\nTotal generated tokens:                  201461\nRequest throughput (req/s):              8.94\nOutput token throughput (tok/s):         1801.74\nPeak output token throughput (tok/s):    4069.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3745.97\n---------------Time to First Token----------------\nMean TTFT (ms):                          47136.93\nMedian TTFT (ms):                        44343.62\nP99 TTFT (ms):                           94568.79\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          129.77\nMedian TPOT (ms):                        120.87\nP99 TPOT (ms):                           331.28\n---------------Inter-token Latency----------------\nMean ITL (ms):                           106.86\nMedian ITL (ms):                         89.48\nP99 ITL (ms):                            338.24\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-14b/a100/#3-max-batched-token-numbers-in-tensorrt-llm","title":"3. Max Batched Token Numbers in TensorRT-LLM","text":"Serving script <pre><code>trtllm-serve Qwen/Qwen3-14B --max_num_tokens=16384\n</code></pre> Benchmark result <pre><code># --max_num_tokens=16384\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  72.54\nTotal input tokens:                      217393\nTotal generated tokens:                  201631\nRequest throughput (req/s):              13.79\nOutput token throughput (tok/s):         2779.60\nPeak output token throughput (tok/s):    5583.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5776.50\n---------------Time to First Token----------------\nMean TTFT (ms):                          15374.97\nMedian TTFT (ms):                        13585.74\nP99 TTFT (ms):                           38913.73\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          268.46\nMedian TPOT (ms):                        159.86\nP99 TPOT (ms):                           1618.81\n---------------Inter-token Latency----------------\nMean ITL (ms):                           129.13\nMedian ITL (ms):                         87.94\nP99 ITL (ms):                            1712.52\n==================================================\n\n# --max_num_tokens=32768\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  74.21\nTotal input tokens:                      217393\nTotal generated tokens:                  201567\nRequest throughput (req/s):              13.48\nOutput token throughput (tok/s):         2716.13\nPeak output token throughput (tok/s):    5559.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5645.51\n---------------Time to First Token----------------\nMean TTFT (ms):                          16776.11\nMedian TTFT (ms):                        15050.03\nP99 TTFT (ms):                           43000.86\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          234.79\nMedian TPOT (ms):                        150.33\nP99 TPOT (ms):                           1486.15\n---------------Inter-token Latency----------------\nMean ITL (ms):                           123.01\nMedian ITL (ms):                         87.61\nP99 ITL (ms):                            525.09\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-14b/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +47.1% Quantization - Max Batched Token Numbers -"},{"location":"performance-lab/qwen3-14b/a100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-14B\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  724.56\nTotal input tokens:                      3200000\nTotal generated tokens:                  9891\nRequest throughput (req/s):              0.14\nOutput token throughput (tok/s):         13.65\nPeak output token throughput (tok/s):    130.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          4430.14\n---------------Time to First Token----------------\nMean TTFT (ms):                          359648.08\nMedian TTFT (ms):                        359424.54\nP99 TTFT (ms):                           714338.15\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          445.58\nMedian TPOT (ms):                        462.54\nP99 TPOT (ms):                           463.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           441.08\nMedian ITL (ms):                         448.05\nP99 ITL (ms):                            594.59\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  407.05\nTotal input tokens:                      1997942\nTotal generated tokens:                  99783\nRequest throughput (req/s):              1.23\nOutput token throughput (tok/s):         245.13\nPeak output token throughput (tok/s):    1304.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          5153.42\n---------------Time to First Token----------------\nMean TTFT (ms):                          197625.03\nMedian TTFT (ms):                        196621.85\nP99 TTFT (ms):                           396056.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          254.61\nMedian TPOT (ms):                        272.93\nP99 TPOT (ms):                           283.50\n---------------Inter-token Latency----------------\nMean ITL (ms):                           253.82\nMedian ITL (ms):                         369.00\nP99 ITL (ms):                            377.42\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  175.66\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              2.85\nOutput token throughput (tok/s):         284.64\nPeak output token throughput (tok/s):    1722.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          5967.13\n---------------Time to First Token----------------\nMean TTFT (ms):                          86908.30\nMedian TTFT (ms):                        86575.09\nP99 TTFT (ms):                           170607.65\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          301.73\nMedian TPOT (ms):                        334.17\nP99 TPOT (ms):                           334.55\n---------------Inter-token Latency----------------\nMean ITL (ms):                           299.28\nMedian ITL (ms):                         333.99\nP99 ITL (ms):                            338.95\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  26.79\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              37.33\nOutput token throughput (tok/s):         149.33\nPeak output token throughput (tok/s):    618.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4916.63\n---------------Time to First Token----------------\nMean TTFT (ms):                          19451.46\nMedian TTFT (ms):                        19150.52\nP99 TTFT (ms):                           26523.59\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          233.35\nMedian TPOT (ms):                        238.61\nP99 TPOT (ms):                           251.05\n---------------Inter-token Latency----------------\nMean ITL (ms):                           175.01\nMedian ITL (ms):                         237.82\nP99 ITL (ms):                            253.18\n==================================================\n</code></pre> Optimized serving script <pre><code>trtllm-serve Qwen/Qwen3-14B --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  727.25\nTotal input tokens:                      3200000\nTotal generated tokens:                  9932\nRequest throughput (req/s):              0.14\nOutput token throughput (tok/s):         13.66\nPeak output token throughput (tok/s):    169.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          4413.82\n---------------Time to First Token----------------\nMean TTFT (ms):                          361107.28\nMedian TTFT (ms):                        361543.94\nP99 TTFT (ms):                           716851.40\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          493.18\nMedian TPOT (ms):                        500.49\nP99 TPOT (ms):                           590.17\n---------------Inter-token Latency----------------\nMean ITL (ms):                           488.09\nMedian ITL (ms):                         48.22\nP99 ITL (ms):                            2215.91\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  326.81\nTotal input tokens:                      1997942\nTotal generated tokens:                  99760\nRequest throughput (req/s):              1.53\nOutput token throughput (tok/s):         305.25\nPeak output token throughput (tok/s):    1224.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          6418.65\n---------------Time to First Token----------------\nMean TTFT (ms):                          155664.65\nMedian TTFT (ms):                        152609.98\nP99 TTFT (ms):                           316584.25\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          196.10\nMedian TPOT (ms):                        207.95\nP99 TPOT (ms):                           218.23\n---------------Inter-token Latency----------------\nMean ITL (ms):                           195.11\nMedian ITL (ms):                         55.84\nP99 ITL (ms):                            1038.71\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  140.24\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              3.57\nOutput token throughput (tok/s):         356.52\nPeak output token throughput (tok/s):    2128.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          7474.00\n---------------Time to First Token----------------\nMean TTFT (ms):                          67624.26\nMedian TTFT (ms):                        65786.34\nP99 TTFT (ms):                           134519.75\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          313.69\nMedian TPOT (ms):                        351.99\nP99 TPOT (ms):                           365.92\n---------------Inter-token Latency----------------\nMean ITL (ms):                           310.55\nMedian ITL (ms):                         63.96\nP99 ITL (ms):                            1002.01\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  15.03\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              66.53\nOutput token throughput (tok/s):         266.13\nPeak output token throughput (tok/s):    883.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          8762.17\n---------------Time to First Token----------------\nMean TTFT (ms):                          8166.48\nMedian TTFT (ms):                        7882.94\nP99 TTFT (ms):                           14856.34\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          715.19\nMedian TPOT (ms):                        853.12\nP99 TPOT (ms):                           1028.67\n---------------Inter-token Latency----------------\nMean ITL (ms):                           536.38\nMedian ITL (ms):                         638.54\nP99 ITL (ms):                            1280.94\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-14b/h100/","title":"Optimizing Qwen3-14B Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/qwen3-14b/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-14B on H100 GPUs:</p> Serving Command <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-14B-FP8\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 6595.66Mean TPOT(ms): 226.03 Total TPS: 10426.27 (+58.1%)Mean TPOT(ms): 251.67 Short Prompt Total TPS: 5796.73Mean TPOT(ms): 525.31 Total TPS: 16884.75 (+191.3%)Mean TPOT(ms): 927.88 Medium Prompt Total TPS: 10597.19Mean TPOT(ms): 206.51 Total TPS: 14764.26 (+39.3%)Mean TPOT(ms): 143.22 Long Prompt Total TPS: 9218.03Mean TPOT(ms): 132.45 Total TPS: 12257.62 (+33.0%)Mean TPOT(ms): 84.93 Very Long Prompt Total TPS: 8298.56Mean TPOT(ms): 277.44 Total TPS: 10014.29 (+20.7%)Mean TPOT(ms): 215.93 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-14b/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-14b/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-14b/h100/#model","title":"Model","text":"<p>Qwen3-14B</p>"},{"location":"performance-lab/qwen3-14b/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/qwen3-14b/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.5.post1</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/qwen3-14b/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-14b/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-14B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-14B --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-14b/h100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-14b/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-14B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  63.53\nTotal input tokens:                      217393\nTotal generated tokens:                  201648\nRequest throughput (req/s):              15.74\nOutput token throughput (tok/s):         3173.92\nPeak output token throughput (tok/s):    8890.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6595.66\n---------------Time to First Token----------------\nMean TTFT (ms):                          22141.21\nMedian TTFT (ms):                        21872.31\nP99 TTFT (ms):                           31156.86\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          226.03\nMedian TPOT (ms):                        117.46\nP99 TPOT (ms):                           621.87\n---------------Inter-token Latency----------------\nMean ITL (ms):                           92.83\nMedian ITL (ms):                         59.99\nP99 ITL (ms):                            627.72\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-14B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  49.86\nTotal input tokens:                      217393\nTotal generated tokens:                  201628\nRequest throughput (req/s):              20.06\nOutput token throughput (tok/s):         4043.89\nPeak output token throughput (tok/s):    12059.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          8403.97\n---------------Time to First Token----------------\nMean TTFT (ms):                          9230.85\nMedian TTFT (ms):                        8812.15\nP99 TTFT (ms):                           19581.07\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          235.33\nMedian TPOT (ms):                        118.67\nP99 TPOT (ms):                           1462.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           91.54\nMedian ITL (ms):                         51.39\nP99 ITL (ms):                            442.65\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-14B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  50.39\nTotal input tokens:                      217393\nTotal generated tokens:                  201612\nRequest throughput (req/s):              19.84\nOutput token throughput (tok/s):         4000.99\nPeak output token throughput (tok/s):    8281.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          8315.15\n---------------Time to First Token----------------\nMean TTFT (ms):                          10230.91\nMedian TTFT (ms):                        9088.74\nP99 TTFT (ms):                           24363.57\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          161.58\nMedian TPOT (ms):                        107.16\nP99 TPOT (ms):                           579.49\n---------------Inter-token Latency----------------\nMean ITL (ms):                           84.78\nMedian ITL (ms):                         0.12\nP99 ITL (ms):                            1440.77\n==================================================\n</code></pre> <p>Result: SGLang (8424.86 tok/s) &gt; TensorRT-LLM (8315.15 tok/s) &gt; vLLM (6595.66 tok/s)</p>"},{"location":"performance-lab/qwen3-14b/h100/#2-quantization-in-sglang","title":"2. Quantization in SGLang","text":"<p>FP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-14B-FP8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  40.21\nTotal input tokens:                      217393\nTotal generated tokens:                  201833\nRequest throughput (req/s):              24.87\nOutput token throughput (tok/s):         5019.64\nPeak output token throughput (tok/s):    14199.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10426.27\n---------------Time to First Token----------------\nMean TTFT (ms):                          7320.60\nMedian TTFT (ms):                        7110.49\nP99 TTFT (ms):                           13187.08\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          251.67\nMedian TPOT (ms):                        94.53\nP99 TPOT (ms):                           1678.80\n---------------Inter-token Latency----------------\nMean ITL (ms):                           77.06\nMedian ITL (ms):                         48.45\nP99 ITL (ms):                            383.61\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-14b/h100/#3-attention-backends-in-sglang","title":"3. Attention Backends in SGLang","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-14B-FP8 --attention-backend=fa3\n</code></pre> Benchmark result <pre><code># fa3\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  40.18\nTotal input tokens:                      217393\nTotal generated tokens:                  201833\nRequest throughput (req/s):              24.89\nOutput token throughput (tok/s):         5023.58\nPeak output token throughput (tok/s):    13005.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10434.44\n---------------Time to First Token----------------\nMean TTFT (ms):                          7304.47\nMedian TTFT (ms):                        7081.19\nP99 TTFT (ms):                           13629.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          242.72\nMedian TPOT (ms):                        94.43\nP99 TPOT (ms):                           1559.90\n---------------Inter-token Latency----------------\nMean ITL (ms):                           76.97\nMedian ITL (ms):                         47.55\nP99 ITL (ms):                            359.57\n================================================== \n\n# --attention-backend=flashinfer\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  45.91\nTotal input tokens:                      217393\nTotal generated tokens:                  201462\nRequest throughput (req/s):              21.78\nOutput token throughput (tok/s):         4388.17\nPeak output token throughput (tok/s):    15814.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          9123.34\n---------------Time to First Token----------------\nMean TTFT (ms):                          13440.31\nMedian TTFT (ms):                        13197.52\nP99 TTFT (ms):                           19982.69\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          246.57\nMedian TPOT (ms):                        95.09\nP99 TPOT (ms):                           1613.59\n---------------Inter-token Latency----------------\nMean ITL (ms):                           76.95\nMedian ITL (ms):                         48.29\nP99 ITL (ms):                            367.15\n==================================================\n\n# --attention-backend=triton\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  41.67\nTotal input tokens:                      217393\nTotal generated tokens:                  201602\nRequest throughput (req/s):              24.00\nOutput token throughput (tok/s):         4838.09\nPeak output token throughput (tok/s):    14024.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          10055.13\n---------------Time to First Token----------------\nMean TTFT (ms):                          7297.40\nMedian TTFT (ms):                        6973.27\nP99 TTFT (ms):                           14354.40\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          268.09\nMedian TPOT (ms):                        100.54\nP99 TPOT (ms):                           1810.20\n---------------Inter-token Latency----------------\nMean ITL (ms):                           82.50\nMedian ITL (ms):                         51.79\nP99 ITL (ms):                            379.98\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-14b/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +27.7% Quantization +23.8% Attention Backend -"},{"location":"performance-lab/qwen3-14b/h100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-14B\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  386.80\nTotal input tokens:                      3200000\nTotal generated tokens:                  9855\nRequest throughput (req/s):              0.26\nOutput token throughput (tok/s):         25.48\nPeak output token throughput (tok/s):    192.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          8298.56\n---------------Time to First Token----------------\nMean TTFT (ms):                          191024.43\nMedian TTFT (ms):                        191051.49\nP99 TTFT (ms):                           380407.97\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          277.44\nMedian TPOT (ms):                        284.52\nP99 TPOT (ms):                           324.14\n---------------Inter-token Latency----------------\nMean ITL (ms):                           274.38\nMedian ITL (ms):                         43.48\nP99 ITL (ms):                            1101.18\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  227.57\nTotal input tokens:                      1997942\nTotal generated tokens:                  99803\nRequest throughput (req/s):              2.20\nOutput token throughput (tok/s):         438.56\nPeak output token throughput (tok/s):    1536.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          9218.03\n---------------Time to First Token----------------\nMean TTFT (ms):                          112716.92\nMedian TTFT (ms):                        110734.54\nP99 TTFT (ms):                           219749.74\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          132.45\nMedian TPOT (ms):                        139.68\nP99 TPOT (ms):                           161.93\n---------------Inter-token Latency----------------\nMean ITL (ms):                           132.11\nMedian ITL (ms):                         43.93\nP99 ITL (ms):                            656.79\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  98.91\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              5.06\nOutput token throughput (tok/s):         505.51\nPeak output token throughput (tok/s):    2890.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          10597.19\n---------------Time to First Token----------------\nMean TTFT (ms):                          50716.13\nMedian TTFT (ms):                        49138.28\nP99 TTFT (ms):                           94614.51\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          206.51\nMedian TPOT (ms):                        231.62\nP99 TPOT (ms):                           236.67\n---------------Inter-token Latency----------------\nMean ITL (ms):                           204.84\nMedian ITL (ms):                         46.01\nP99 ITL (ms):                            642.06\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  22.72\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              44.02\nOutput token throughput (tok/s):         176.06\nPeak output token throughput (tok/s):    843.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5796.73\n---------------Time to First Token----------------\nMean TTFT (ms):                          18282.75\nMedian TTFT (ms):                        18061.13\nP99 TTFT (ms):                           22564.77\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          525.31\nMedian TPOT (ms):                        597.29\nP99 TPOT (ms):                           610.81\n---------------Inter-token Latency----------------\nMean ITL (ms):                           393.98\nMedian ITL (ms):                         581.28\nP99 ITL (ms):                            633.07\n==================================================\n</code></pre> Optimized serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-14B-FP8\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  320.54\nTotal input tokens:                      3200000\nTotal generated tokens:                  9964\nRequest throughput (req/s):              0.31\nOutput token throughput (tok/s):         31.09\nPeak output token throughput (tok/s):    250.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          10014.29\n---------------Time to First Token----------------\nMean TTFT (ms):                          158068.68\nMedian TTFT (ms):                        157591.30\nP99 TTFT (ms):                           313702.45\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          215.93\nMedian TPOT (ms):                        212.59\nP99 TPOT (ms):                           326.36\n---------------Inter-token Latency----------------\nMean ITL (ms):                           211.53\nMedian ITL (ms):                         41.38\nP99 ITL (ms):                            5675.37\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  171.14\nTotal input tokens:                      1997942\nTotal generated tokens:                  99814\nRequest throughput (req/s):              2.92\nOutput token throughput (tok/s):         583.23\nPeak output token throughput (tok/s):    1950.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          12257.62\n---------------Time to First Token----------------\nMean TTFT (ms):                          82073.47\nMedian TTFT (ms):                        85092.81\nP99 TTFT (ms):                           165401.79\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          84.93\nMedian TPOT (ms):                        83.43\nP99 TPOT (ms):                           131.23\n---------------Inter-token Latency----------------\nMean ITL (ms):                           84.30\nMedian ITL (ms):                         41.87\nP99 ITL (ms):                            89.35\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  70.99\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              7.04\nOutput token throughput (tok/s):         704.28\nPeak output token throughput (tok/s):    3496.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          14764.26\n---------------Time to First Token----------------\nMean TTFT (ms):                          33245.08\nMedian TTFT (ms):                        33169.68\nP99 TTFT (ms):                           68799.73\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          143.22\nMedian TPOT (ms):                        146.25\nP99 TPOT (ms):                           216.28\n---------------Inter-token Latency----------------\nMean ITL (ms):                           140.52\nMedian ITL (ms):                         45.98\nP99 ITL (ms):                            1987.27\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  7.80\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              128.21\nOutput token throughput (tok/s):         512.83\nPeak output token throughput (tok/s):    3784.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          16884.75\n---------------Time to First Token----------------\nMean TTFT (ms):                          4489.43\nMedian TTFT (ms):                        4057.99\nP99 TTFT (ms):                           7239.77\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          927.88\nMedian TPOT (ms):                        962.87\nP99 TPOT (ms):                           1793.61\n---------------Inter-token Latency----------------\nMean ITL (ms):                           556.73\nMedian ITL (ms):                         82.38\nP99 ITL (ms):                            5043.60\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/a100/","title":"Optimizing Qwen3-235B-A22B Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/qwen3-235b-a22b/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-235B-A22B-Instruct-2507 on A100 GPUs:</p> Serving Command <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 3792.50Mean TPOT(ms): 123.36 Total TPS: 3019.19x2 (+59.2%)Mean TPOT(ms): 150.72 Short Prompt Total TPS: 7449.43Mean TPOT(ms): 155.16 Total TPS: 4961.66x2 (+33.2%)Mean TPOT(ms): 265.87 Medium Prompt Total TPS: 9342.67Mean TPOT(ms): 190.79 Total TPS: 5333.91x2 (+14.2%)Mean TPOT(ms): 342.39 Long Prompt Total TPS: 8170.72Mean TPOT(ms): 205.64 Total TPS: 4633.41x2 (+13.4%)Mean TPOT(ms): 347.13 Very Long Prompt Total TPS: 6657.49Mean TPOT(ms): 298.86 Total TPS: 4132.51x2 (+24.1%)Mean TPOT(ms): 481.71 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-235b-a22b/a100/#model","title":"Model","text":"<p>Qwen3-235B-A22B-Instruct-2507</p>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.5.post1</li> </ul>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-235B-A22B-Instruct-2507 --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-235B-A22B-Instruct-2507 --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-235b-a22b/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 -tp 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  109.00\nTotal input tokens:                      217393\nTotal generated tokens:                  196004\nRequest throughput (req/s):              9.17\nOutput token throughput (tok/s):         1798.14\nPeak output token throughput (tok/s):    3297.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3792.50\n---------------Time to First Token----------------\nMean TTFT (ms):                          38065.91\nMedian TTFT (ms):                        37597.63\nP99 TTFT (ms):                           77082.58\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          123.36\nMedian TPOT (ms):                        103.68\nP99 TPOT (ms):                           448.36\n---------------Inter-token Latency----------------\nMean ITL (ms):                           100.76\nMedian ITL (ms):                         86.83\nP99 ITL (ms):                            185.41\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Instruct-2507 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  89.35\nTotal input tokens:                      217393\nTotal generated tokens:                  196763\nRequest throughput (req/s):              11.19\nOutput token throughput (tok/s):         2202.21\nPeak output token throughput (tok/s):    4926.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4635.31\n---------------Time to First Token----------------\nMean TTFT (ms):                          21130.67\nMedian TTFT (ms):                        15483.85\nP99 TTFT (ms):                           51960.77\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          235.10\nMedian TPOT (ms):                        163.74\nP99 TPOT (ms):                           1171.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           136.35\nMedian ITL (ms):                         86.55\nP99 ITL (ms):                            500.15\n==================================================\n</code></pre> <p>Result: SGLang(4635.31 tok/s) &gt; vLLM (3792.50 tok/s)</p>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#2-quantization-in-vllm","title":"2. Quantization in vLLM","text":"<p>Running SGLang with FP8 quantization on A100 GPUs does not work. vLLM supports running as w8a16 quantization on A100 GPUs via its Marlin kernel.</p> <p>vLLM FP8</p> Serving script <pre><code># TP8 only is not compatible for the model weight. Use EP as well.\nvllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 8 --enable-expert-parallel\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  110.68\nTotal input tokens:                      217393\nTotal generated tokens:                  196648\nRequest throughput (req/s):              9.04\nOutput token throughput (tok/s):         1776.73\nPeak output token throughput (tok/s):    3106.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3740.89\n---------------Time to First Token----------------\nMean TTFT (ms):                          37366.87\nMedian TTFT (ms):                        35525.98\nP99 TTFT (ms):                           78616.18\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          115.74\nMedian TPOT (ms):                        109.21\nP99 TPOT (ms):                           228.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           101.28\nMedian ITL (ms):                         91.72\nP99 ITL (ms):                            235.47\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#3-parallelism-in-vllm","title":"3. Parallelism in vLLM","text":"<p>TP4</p> Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  136.64\nTotal input tokens:                      217393\nTotal generated tokens:                  195140\nRequest throughput (req/s):              7.32\nOutput token throughput (tok/s):         1428.16\nPeak output token throughput (tok/s):    2852.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3019.19\n---------------Time to First Token----------------\nMean TTFT (ms):                          45879.29\nMedian TTFT (ms):                        43216.75\nP99 TTFT (ms):                           99903.30\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          150.72\nMedian TPOT (ms):                        143.39\nP99 TPOT (ms):                           297.17\n---------------Inter-token Latency----------------\nMean ITL (ms):                           131.17\nMedian ITL (ms):                         119.69\nP99 ITL (ms):                            311.46\n==================================================\n</code></pre> <p>TP4 EP4</p> Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4 --enable-expert-parallel\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  146.74\nTotal input tokens:                      217393\nTotal generated tokens:                  195590\nRequest throughput (req/s):              6.81\nOutput token throughput (tok/s):         1332.94\nPeak output token throughput (tok/s):    2501.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2814.47\n---------------Time to First Token----------------\nMean TTFT (ms):                          48930.06\nMedian TTFT (ms):                        45498.27\nP99 TTFT (ms):                           106740.39\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          161.49\nMedian TPOT (ms):                        154.84\nP99 TPOT (ms):                           329.32\n---------------Inter-token Latency----------------\nMean ITL (ms):                           140.35\nMedian ITL (ms):                         128.38\nP99 ITL (ms):                            340.60\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#4-max-batched-token-numbers-in-vllm","title":"4. Max Batched Token Numbers in vLLM","text":"Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4 --max-num-batched-tokens 4096\n</code></pre> Benchmark result <pre><code># --max-num-batched-tokens 4096\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  136.64\nTotal input tokens:                      217393\nTotal generated tokens:                  195953\nRequest throughput (req/s):              7.32\nOutput token throughput (tok/s):         1434.07\nPeak output token throughput (tok/s):    2784.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3025.05\n---------------Time to First Token----------------\nMean TTFT (ms):                          45389.84\nMedian TTFT (ms):                        41651.71\nP99 TTFT (ms):                           98975.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          155.37\nMedian TPOT (ms):                        142.48\nP99 TPOT (ms):                           509.56\n---------------Inter-token Latency----------------\nMean ITL (ms):                           131.19\nMedian ITL (ms):                         114.03\nP99 ITL (ms):                            395.26\n==================================================\n\n# --max-num-batched-tokens 8192\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  137.59\nTotal input tokens:                      217393\nTotal generated tokens:                  195781\nRequest throughput (req/s):              7.27\nOutput token throughput (tok/s):         1422.97\nPeak output token throughput (tok/s):    2724.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3003.01\n---------------Time to First Token----------------\nMean TTFT (ms):                          45405.05\nMedian TTFT (ms):                        41603.31\nP99 TTFT (ms):                           99900.26\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          157.10\nMedian TPOT (ms):                        143.37\nP99 TPOT (ms):                           561.98\n---------------Inter-token Latency----------------\nMean ITL (ms):                           132.24\nMedian ITL (ms):                         120.38\nP99 ITL (ms):                            306.32\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +22.2% Quantization - Parallelism +59.2% Attention Backend - Max Batched Token Numbers -"},{"location":"performance-lab/qwen3-235b-a22b/a100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 -tp 8\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  481.89\nTotal input tokens:                      3200000\nTotal generated tokens:                  8191\nRequest throughput (req/s):              0.21\nOutput token throughput (tok/s):         17.00\nPeak output token throughput (tok/s):    140.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          6657.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          240229.05\nMedian TTFT (ms):                        240137.10\nP99 TTFT (ms):                           475251.39\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          298.86\nMedian TPOT (ms):                        307.94\nP99 TPOT (ms):                           311.82\n---------------Inter-token Latency----------------\nMean ITL (ms):                           295.13\nMedian ITL (ms):                         303.36\nP99 ITL (ms):                            370.10\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  255.70\nTotal input tokens:                      1997942\nTotal generated tokens:                  91352\nRequest throughput (req/s):              1.96\nOutput token throughput (tok/s):         357.26\nPeak output token throughput (tok/s):    1403.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          8170.72\n---------------Time to First Token----------------\nMean TTFT (ms):                          122190.64\nMedian TTFT (ms):                        121615.01\nP99 TTFT (ms):                           244882.42\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          205.64\nMedian TPOT (ms):                        220.33\nP99 TPOT (ms):                           237.80\n---------------Inter-token Latency----------------\nMean ITL (ms):                           204.69\nMedian ITL (ms):                         239.57\nP99 ITL (ms):                            246.21\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  112.15\nTotal input tokens:                      998175\nTotal generated tokens:                  49632\nRequest throughput (req/s):              4.46\nOutput token throughput (tok/s):         442.54\nPeak output token throughput (tok/s):    1691.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          9342.67\n---------------Time to First Token----------------\nMean TTFT (ms):                          54722.50\nMedian TTFT (ms):                        54532.60\nP99 TTFT (ms):                           106756.03\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          190.79\nMedian TPOT (ms):                        208.61\nP99 TPOT (ms):                           209.09\n---------------Inter-token Latency----------------\nMean ITL (ms):                           189.23\nMedian ITL (ms):                         208.52\nP99 ITL (ms):                            211.58\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  17.68\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              56.56\nOutput token throughput (tok/s):         226.26\nPeak output token throughput (tok/s):    948.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          7449.43\n---------------Time to First Token----------------\nMean TTFT (ms):                          12767.63\nMedian TTFT (ms):                        12409.68\nP99 TTFT (ms):                           17339.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          155.16\nMedian TPOT (ms):                        158.22\nP99 TPOT (ms):                           168.04\n---------------Inter-token Latency----------------\nMean ITL (ms):                           116.37\nMedian ITL (ms):                         157.64\nP99 ITL (ms):                            179.04\n==================================================\n</code></pre> Optimized serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  776.36\nTotal input tokens:                      3200000\nTotal generated tokens:                  8300\nRequest throughput (req/s):              0.13\nOutput token throughput (tok/s):         10.69\nPeak output token throughput (tok/s):    152.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          4132.51\n---------------Time to First Token----------------\nMean TTFT (ms):                          387908.55\nMedian TTFT (ms):                        387968.52\nP99 TTFT (ms):                           766523.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          481.71\nMedian TPOT (ms):                        496.29\nP99 TPOT (ms):                           513.27\n---------------Inter-token Latency----------------\nMean ITL (ms):                           476.10\nMedian ITL (ms):                         490.85\nP99 ITL (ms):                            612.03\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  450.71\nTotal input tokens:                      1997942\nTotal generated tokens:                  90389\nRequest throughput (req/s):              1.11\nOutput token throughput (tok/s):         200.55\nPeak output token throughput (tok/s):    1066.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          4633.41\n---------------Time to First Token----------------\nMean TTFT (ms):                          216485.01\nMedian TTFT (ms):                        215365.76\nP99 TTFT (ms):                           436587.02\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          347.13\nMedian TPOT (ms):                        371.93\nP99 TPOT (ms):                           412.29\n---------------Inter-token Latency----------------\nMean ITL (ms):                           344.46\nMedian ITL (ms):                         425.81\nP99 ITL (ms):                            436.76\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  196.42\nTotal input tokens:                      998175\nTotal generated tokens:                  49495\nRequest throughput (req/s):              2.55\nOutput token throughput (tok/s):         251.99\nPeak output token throughput (tok/s):    1116.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          5333.91\n---------------Time to First Token----------------\nMean TTFT (ms):                          95261.76\nMedian TTFT (ms):                        94982.78\nP99 TTFT (ms):                           189238.63\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          342.39\nMedian TPOT (ms):                        376.64\nP99 TPOT (ms):                           377.99\n---------------Inter-token Latency----------------\nMean ITL (ms):                           339.59\nMedian ITL (ms):                         376.62\nP99 ITL (ms):                            380.69\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  26.54\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              37.67\nOutput token throughput (tok/s):         150.70\nPeak output token throughput (tok/s):    311.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4961.66\n---------------Time to First Token----------------\nMean TTFT (ms):                          17260.17\nMedian TTFT (ms):                        17240.71\nP99 TTFT (ms):                           26154.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          265.87\nMedian TPOT (ms):                        272.95\nP99 TPOT (ms):                           278.77\n---------------Inter-token Latency----------------\nMean ITL (ms):                           199.40\nMedian ITL (ms):                         272.50\nP99 ITL (ms):                            288.23\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/h100/","title":"Optimizing Qwen3-235B-A22B Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/qwen3-235b-a22b/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-235B-A22B-Instruct-2507 on H100 GPUs:</p> Serving Command <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4 --max-num-batched-tokens 16384\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 8424.86Mean TPOT(ms): 147.56 Total TPS: 7787.17x2 (+84.9%)Mean TPOT(ms): 186.26 Short Prompt Total TPS: 14668.27Mean TPOT(ms): 212.98 Total TPS: 21211.83x2 (+189.2%)Mean TPOT(ms): 125.18 Medium Prompt Total TPS: 23858.69Mean TPOT(ms): 101.08 Total TPS: 18950.04x2 (+58.9%)Mean TPOT(ms): 125.93 Long Prompt Total TPS: 21113.53Mean TPOT(ms): 68.94 Total TPS: 16478.79x2 (+56.1%)Mean TPOT(ms): 89.52 Very Long Prompt Total TPS: 22657.51Mean TPOT(ms): 132.10 Total TPS: 15366.32x2 (+35.6%)Mean TPOT(ms): 194.60 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-235b-a22b/h100/#model","title":"Model","text":"<p>Qwen3-235B-A22B-Instruct-2507</p>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.5.post1</li> </ul>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-235B-A22B-Instruct-2507 --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-235B-A22B-Instruct-2507 --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-235b-a22b/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 -tp 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  48.28\nTotal input tokens:                      212894\nTotal generated tokens:                  193865\nRequest throughput (req/s):              20.38\nOutput token throughput (tok/s):         4015.36\nPeak output token throughput (tok/s):    9638.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          8424.86\n---------------Time to First Token----------------\nMean TTFT (ms):                          8730.69\nMedian TTFT (ms):                        8707.89\nP99 TTFT (ms):                           12940.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          147.56\nMedian TPOT (ms):                        114.67\nP99 TPOT (ms):                           342.35\n---------------Inter-token Latency----------------\nMean ITL (ms):                           81.31\nMedian ITL (ms):                         51.14\nP99 ITL (ms):                            677.61\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Instruct-2507 --tp-size 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  53.18\nTotal input tokens:                      214137\nTotal generated tokens:                  193829\nRequest throughput (req/s):              18.50\nOutput token throughput (tok/s):         3644.72\nPeak output token throughput (tok/s):    8791.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          7671.31\n---------------Time to First Token----------------\nMean TTFT (ms):                          11715.28\nMedian TTFT (ms):                        6274.12\nP99 TTFT (ms):                           32860.14\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          178.35\nMedian TPOT (ms):                        102.70\nP99 TPOT (ms):                           841.80\n---------------Inter-token Latency----------------\nMean ITL (ms):                           88.35\nMedian ITL (ms):                         44.39\nP99 ITL (ms):                            732.90\n==================================================\n</code></pre> <p>Result: vLLM (8424.86 tok/s) &gt; SGLang (7671.31 tok/s)</p>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#2-quantization-in-vllm","title":"2. Quantization in vLLM","text":"Serving script <pre><code># TP8 only is not compatible with the model weight. Use EP as well.\nvllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 8 --enable-expert-parallel\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  49.88\nTotal input tokens:                      211761\nTotal generated tokens:                  192350\nRequest throughput (req/s):              19.73\nOutput token throughput (tok/s):         3856.59\nPeak output token throughput (tok/s):    9788.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          8102.36\n---------------Time to First Token----------------\nMean TTFT (ms):                          9755.64\nMedian TTFT (ms):                        9742.53\nP99 TTFT (ms):                           14508.64\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          169.52\nMedian TPOT (ms):                        129.08\nP99 TPOT (ms):                           353.11\n---------------Inter-token Latency----------------\nMean ITL (ms):                           88.39\nMedian ITL (ms):                         49.03\nP99 ITL (ms):                            343.14\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#3-parallelism-in-vllm","title":"3. Parallelism in vLLM","text":"<p>TP4</p> Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  54.12\nTotal input tokens:                      212992\nTotal generated tokens:                  194446\nRequest throughput (req/s):              18.18\nOutput token throughput (tok/s):         3592.57\nPeak output token throughput (tok/s):    8605.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          7527.80\n---------------Time to First Token----------------\nMean TTFT (ms):                          9924.69\nMedian TTFT (ms):                        9896.14\nP99 TTFT (ms):                           15574.61\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          169.20\nMedian TPOT (ms):                        121.96\nP99 TPOT (ms):                           372.71\n---------------Inter-token Latency----------------\nMean ITL (ms):                           89.89\nMedian ITL (ms):                         57.15\nP99 ITL (ms):                            351.02\n==================================================  \n</code></pre> <p>TP4+EP</p> Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4 --enable-expert-parallel\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  57.08\nTotal input tokens:                      214311\nTotal generated tokens:                  192752\nRequest throughput (req/s):              17.24\nOutput token throughput (tok/s):         3376.99\nPeak output token throughput (tok/s):    8005.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          7131.69\n---------------Time to First Token----------------\nMean TTFT (ms):                          10706.10\nMedian TTFT (ms):                        10676.53\nP99 TTFT (ms):                           17226.88\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          190.24\nMedian TPOT (ms):                        131.62\nP99 TPOT (ms):                           432.31\n---------------Inter-token Latency----------------\nMean ITL (ms):                           96.98\nMedian ITL (ms):                         60.65\nP99 ITL (ms):                            408.09\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#4-attention-backend-in-vllm","title":"4. Attention Backend in vLLM","text":"<p>FlashAttention is the default.</p> <p>FlashInfer</p> Serving script <pre><code>VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4\n</code></pre> Benchmark result <pre><code># Crash\n</code></pre> <p>XFormers</p> Serving script <pre><code>VLLM_ATTENTION_BACKEND=XFORMERS vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4\n</code></pre> Benchmark result <pre><code># Crash on inference\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#5-max-number-of-batched-tokens-in-vllm","title":"5. Max Number of Batched Tokens in vLLM","text":"Serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4 --max-num-batched-tokens 16384\n</code></pre> Benchmark result <pre><code># --max-num-batched-tokens 16384\n============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  52.46\nTotal input tokens:                      214381\nTotal generated tokens:                  194148\nRequest throughput (req/s):              18.76\nOutput token throughput (tok/s):         3700.75\nPeak output token throughput (tok/s):    8574.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          7787.17\n---------------Time to First Token----------------\nMean TTFT (ms):                          9600.73\nMedian TTFT (ms):                        9525.78\nP99 TTFT (ms):                           13887.13\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          186.26\nMedian TPOT (ms):                        108.31\nP99 TPOT (ms):                           634.48\n---------------Inter-token Latency----------------\nMean ITL (ms):                           86.62\nMedian ITL (ms):                         56.97\nP99 ITL (ms):                            632.37\n==================================================\n\n# --max-num-batched-tokens 32768\n# Crash on inference\n</code></pre>"},{"location":"performance-lab/qwen3-235b-a22b/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection - Quantization - Parallelism +78.7% Attention Backend - Max Number of Batched Tokens +3.4%"},{"location":"performance-lab/qwen3-235b-a22b/h100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 -tp 8\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  141.60\nTotal input tokens:                      3200000\nTotal generated tokens:                  8191\nRequest throughput (req/s):              0.71\nOutput token throughput (tok/s):         57.85\nPeak output token throughput (tok/s):    346.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          22657.51\n---------------Time to First Token----------------\nMean TTFT (ms):                          70409.55\nMedian TTFT (ms):                        70737.29\nP99 TTFT (ms):                           138084.17\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          132.10\nMedian TPOT (ms):                        134.49\nP99 TPOT (ms):                           182.76\n---------------Inter-token Latency----------------\nMean ITL (ms):                           129.44\nMedian ITL (ms):                         23.29\nP99 ITL (ms):                            376.34\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  98.99\nTotal input tokens:                      1997942\nTotal generated tokens:                  92185\nRequest throughput (req/s):              5.05\nOutput token throughput (tok/s):         931.21\nPeak output token throughput (tok/s):    2450.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          21113.53\n---------------Time to First Token----------------\nMean TTFT (ms):                          47811.58\nMedian TTFT (ms):                        48562.54\nP99 TTFT (ms):                           93161.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          68.94\nMedian TPOT (ms):                        72.58\nP99 TPOT (ms):                           92.94\n---------------Inter-token Latency----------------\nMean ITL (ms):                           68.75\nMedian ITL (ms):                         30.18\nP99 ITL (ms):                            245.68\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  43.91\nTotal input tokens:                      998175\nTotal generated tokens:                  49500\nRequest throughput (req/s):              11.39\nOutput token throughput (tok/s):         1127.26\nPeak output token throughput (tok/s):    4021.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          23858.69\n---------------Time to First Token----------------\nMean TTFT (ms):                          21313.79\nMedian TTFT (ms):                        20242.32\nP99 TTFT (ms):                           40939.00\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          101.08\nMedian TPOT (ms):                        108.64\nP99 TPOT (ms):                           125.24\n---------------Inter-token Latency----------------\nMean ITL (ms):                           100.16\nMedian ITL (ms):                         35.94\nP99 ITL (ms):                            244.64\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  8.83\nTotal input tokens:                      125650\nTotal generated tokens:                  3936\nRequest throughput (req/s):              111.38\nOutput token throughput (tok/s):         445.53\nPeak output token throughput (tok/s):    1489.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          14668.27\n---------------Time to First Token----------------\nMean TTFT (ms):                          7042.17\nMedian TTFT (ms):                        6882.45\nP99 TTFT (ms):                           8721.24\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          212.98\nMedian TPOT (ms):                        238.84\nP99 TPOT (ms):                           242.28\n---------------Inter-token Latency----------------\nMean ITL (ms):                           159.73\nMedian ITL (ms):                         235.21\nP99 ITL (ms):                            250.35\n==================================================\n</code></pre> Optimized serving script <pre><code>vllm serve Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 -tp 4 --max-num-batched-tokens 16384\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  208.79\nTotal input tokens:                      3200000\nTotal generated tokens:                  8281\nRequest throughput (req/s):              0.48\nOutput token throughput (tok/s):         39.66\nPeak output token throughput (tok/s):    277.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          15366.32\n---------------Time to First Token----------------\nMean TTFT (ms):                          103754.50\nMedian TTFT (ms):                        103784.48\nP99 TTFT (ms):                           205204.25\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          194.60\nMedian TPOT (ms):                        194.50\nP99 TPOT (ms):                           277.94\n---------------Inter-token Latency----------------\nMean ITL (ms):                           193.15\nMedian ITL (ms):                         27.60\nP99 ITL (ms):                            1117.34\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  126.76\nTotal input tokens:                      1997942\nTotal generated tokens:                  90966\nRequest throughput (req/s):              3.94\nOutput token throughput (tok/s):         717.60\nPeak output token throughput (tok/s):    2139.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          16478.79\n---------------Time to First Token----------------\nMean TTFT (ms):                          60513.14\nMedian TTFT (ms):                        61048.35\nP99 TTFT (ms):                           120001.31\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          89.52\nMedian TPOT (ms):                        94.78\nP99 TPOT (ms):                           121.95\n---------------Inter-token Latency----------------\nMean ITL (ms):                           89.17\nMedian ITL (ms):                         34.82\nP99 ITL (ms):                            642.94\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  55.28\nTotal input tokens:                      998175\nTotal generated tokens:                  49471\nRequest throughput (req/s):              9.04\nOutput token throughput (tok/s):         894.84\nPeak output token throughput (tok/s):    3723.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          18950.04\n---------------Time to First Token----------------\nMean TTFT (ms):                          26860.95\nMedian TTFT (ms):                        26155.54\nP99 TTFT (ms):                           51773.01\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          125.93\nMedian TPOT (ms):                        140.65\nP99 TPOT (ms):                           152.49\n---------------Inter-token Latency----------------\nMean ITL (ms):                           124.89\nMedian ITL (ms):                         39.21\nP99 ITL (ms):                            632.84\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     984\nBenchmark duration (s):                  6.11\nTotal input tokens:                      125650\nTotal generated tokens:                  3936\nRequest throughput (req/s):              161.07\nOutput token throughput (tok/s):         644.28\nPeak output token throughput (tok/s):    3676.00\nPeak concurrent requests:                984.00\nTotal Token throughput (tok/s):          21211.83\n---------------Time to First Token----------------\nMean TTFT (ms):                          5621.51\nMedian TTFT (ms):                        5525.90\nP99 TTFT (ms):                           5803.57\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          125.18\nMedian TPOT (ms):                        152.34\nP99 TPOT (ms):                           182.84\n---------------Inter-token Latency----------------\nMean ITL (ms):                           93.89\nMedian ITL (ms):                         110.55\nP99 ITL (ms):                            243.41\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-30b-a3b/910b/","title":"Optimizing Qwen3-30B-A3B Throughput on ASCEND 910B NPUs","text":""},{"location":"performance-lab/qwen3-30b-a3b/910b/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-30B-A3B on ASCEND 910B NPUs:</p> Serving Command <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve Qwen/Qwen3-30B-A3B\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 1227.19Mean TPOT(ms): 318.03 Total TPS: 4712.47 (+284.0%)Mean TPOT(ms): 964.45 Short Prompt Total TPS: 5340.68Mean TPOT(ms): 312.49 Total TPS: 15040.15 (+181.6%)Mean TPOT(ms): 1249.72 Medium Prompt Total TPS: 7843.85Mean TPOT(ms): 344.86 Total TPS: 15591.71 (+98.8%)Mean TPOT(ms): 194.54 Long Prompt Total TPS: 5881.36Mean TPOT(ms): 277.5 Total TPS: 13362.49 (+127.2%)Mean TPOT(ms): 130.97 Very Long Prompt Total TPS: 3704.57Mean TPOT(ms): 503.84 Total TPS: 10830.52 (+192.4%)Mean TPOT(ms): 242.08 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-30b-a3b/910b/#model","title":"Model","text":"<p>Qwen/Qwen3-30B-A3B</p>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#hardware","title":"Hardware","text":"<p>Ascend 910B NPUs</p>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM-Ascend: v0.9.1</li> <li>MindIE: 2.1RC1</li> </ul>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-30B-A3B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-30B-A3B --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-30b-a3b/910b/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-30B-A3B -tp 2\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  341.49\nTotal input tokens:                      217393\nTotal generated tokens:                  201675\nRequest throughput (req/s):              2.93\nOutput token throughput (tok/s):         590.58\nPeak output token throughput (tok/s):    1283.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1227.19\n---------------Time to First Token----------------\nMean TTFT (ms):                          91791.47\nMedian TTFT (ms):                        85430.73\nP99 TTFT (ms):                           217017.19\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          318.03\nMedian TPOT (ms):                        326.12\nP99 TPOT (ms):                           376.59\n---------------Inter-token Latency----------------\nMean ITL (ms):                           302.02\nMedian ITL (ms):                         245.14\nP99 ITL (ms):                            557.35\n==================================================\n</code></pre> <p>MindIE</p> Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve Qwen3-30B-A3B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  88.95\nTotal input tokens:                      217393\nTotal generated tokens:                  201779\nRequest throughput (req/s):              11.24\nOutput token throughput (tok/s):         2268.46\nPeak output token throughput (tok/s):    3394.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4712.47\n---------------Time to First Token----------------\nMean TTFT (ms):                          6315.01\nMedian TTFT (ms):                        6421.94\nP99 TTFT (ms):                           11305.60\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          964.45\nMedian TPOT (ms):                        242.38\nP99 TPOT (ms):                           8546.71\n---------------Inter-token Latency----------------\nMean ITL (ms):                           183.79\nMedian ITL (ms):                         60.42\nP99 ITL (ms):                            74.07\n==================================================\n</code></pre> <p>Result: MineIE (4712.47 tok/s) &gt; vLLM (1227.19 tok/s)</p>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#2-enable-prefix-cache-in-mindie","title":"2. Enable Prefix Cache in MindIE","text":"Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve Qwen3-30B-A3B --enable-prefix-caching\n</code></pre> Benchmark result <pre><code># Crash, not supported for the model\n</code></pre>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#3-quantization-in-mindie","title":"3. Quantization in MindIE","text":"Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve vllm-ascend/Qwen3-8B-W8A8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  92.29\nTotal input tokens:                      217393\nTotal generated tokens:                  201586\nRequest throughput (req/s):              10.84\nOutput token throughput (tok/s):         2184.30\nPeak output token throughput (tok/s):    3392.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4539.88\n---------------Time to First Token----------------\nMean TTFT (ms):                          5588.80\nMedian TTFT (ms):                        4708.65\nP99 TTFT (ms):                           18446.14\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          988.22\nMedian TPOT (ms):                        240.48\nP99 TPOT (ms):                           11241.85\n---------------Inter-token Latency----------------\nMean ITL (ms):                           195.17\nMedian ITL (ms):                         62.16\nP99 ITL (ms):                            226.37\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-30b-a3b/910b/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +284.0% Prefix Caching - Quantization -"},{"location":"performance-lab/qwen3-30b-a3b/910b/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-30B-A3B -tp 2\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  866.50\nTotal input tokens:                      3200000\nTotal generated tokens:                  10000\nRequest throughput (req/s):              0.12\nOutput token throughput (tok/s):         11.54\nPeak output token throughput (tok/s):    117.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          3704.57\n---------------Time to First Token----------------\nMean TTFT (ms):                          442404.81\nMedian TTFT (ms):                        440136.40\nP99 TTFT (ms):                           853640.24\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          503.84\nMedian TPOT (ms):                        498.47\nP99 TPOT (ms):                           998.84\n---------------Inter-token Latency----------------\nMean ITL (ms):                           498.80\nMedian ITL (ms):                         125.10\nP99 ITL (ms):                            177.10\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  357.03\nTotal input tokens:                      2000000\nTotal generated tokens:                  99842\nRequest throughput (req/s):              1.40\nOutput token throughput (tok/s):         279.64\nPeak output token throughput (tok/s):    864.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          5881.36\n---------------Time to First Token----------------\nMean TTFT (ms):                          156755.15\nMedian TTFT (ms):                        163311.37\nP99 TTFT (ms):                           314736.64\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          277.51\nMedian TPOT (ms):                        273.18\nP99 TPOT (ms):                           452.09\n---------------Inter-token Latency----------------\nMean ITL (ms):                           276.20\nMedian ITL (ms):                         243.71\nP99 ITL (ms):                            499.20\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  133.86\nTotal input tokens:                      1000000\nTotal generated tokens:                  50000\nRequest throughput (req/s):              3.74\nOutput token throughput (tok/s):         373.52\nPeak output token throughput (tok/s):    1080.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          7843.85\n---------------Time to First Token----------------\nMean TTFT (ms):                          56434.48\nMedian TTFT (ms):                        62622.42\nP99 TTFT (ms):                           117368.72\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          344.86\nMedian TPOT (ms):                        350.15\nP99 TPOT (ms):                           667.15\n---------------Inter-token Latency----------------\nMean ITL (ms):                           341.55\nMedian ITL (ms):                         240.26\nP99 ITL (ms):                            1189.67\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  24.72\nTotal input tokens:                      128000\nTotal generated tokens:                  4000\nRequest throughput (req/s):              40.46\nOutput token throughput (tok/s):         161.84\nPeak output token throughput (tok/s):    768.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5340.68\n---------------Time to First Token----------------\nMean TTFT (ms):                          19130.06\nMedian TTFT (ms):                        17860.56\nP99 TTFT (ms):                           23751.62\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          312.49\nMedian TPOT (ms):                        308.02\nP99 TPOT (ms):                           338.42\n---------------Inter-token Latency----------------\nMean ITL (ms):                           234.36\nMedian ITL (ms):                         269.04\nP99 ITL (ms):                            465.28\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nMaximum request concurrency:             4\nBenchmark duration (s):                  6271.85\nTotal input tokens:                      217393\nTotal generated tokens:                  201847\nRequest throughput (req/s):              0.16\nOutput token throughput (tok/s):         32.18\nPeak output token throughput (tok/s):    40.00\nPeak concurrent requests:                7.00\nTotal Token throughput (tok/s):          66.84\n---------------Time to First Token----------------\nMean TTFT (ms):                          359.99\nMedian TTFT (ms):                        357.20\nP99 TTFT (ms):                           445.74\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          122.71\nMedian TPOT (ms):                        122.70\nP99 TPOT (ms):                           134.22\n---------------Inter-token Latency----------------\nMean ITL (ms):                           122.00\nMedian ITL (ms):                         120.56\nP99 ITL (ms):                            236.94\n==================================================\n</code></pre> Optimized serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve Qwen3-30B-A3B\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  296.36\nTotal input tokens:                      3200000\nTotal generated tokens:                  9693\nRequest throughput (req/s):              0.34\nOutput token throughput (tok/s):         32.71\nPeak output token throughput (tok/s):    330.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          10830.52\n---------------Time to First Token----------------\nMean TTFT (ms):                          147133.58\nMedian TTFT (ms):                        147851.57\nP99 TTFT (ms):                           289974.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          242.08\nMedian TPOT (ms):                        232.74\nP99 TPOT (ms):                           741.49\n---------------Inter-token Latency----------------\nMean ITL (ms):                           225.71\nMedian ITL (ms):                         46.46\nP99 ITL (ms):                            56.81\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  157.15\nTotal input tokens:                      2000000\nTotal generated tokens:                  99961\nRequest throughput (req/s):              3.18\nOutput token throughput (tok/s):         636.07\nPeak output token throughput (tok/s):    2142.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          13362.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          70716.71\nMedian TTFT (ms):                        74982.88\nP99 TTFT (ms):                           148427.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          130.97\nMedian TPOT (ms):                        134.98\nP99 TPOT (ms):                           208.16\n---------------Inter-token Latency----------------\nMean ITL (ms):                           130.95\nMedian ITL (ms):                         63.84\nP99 ITL (ms):                            489.52\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  67.34\nTotal input tokens:                      1000000\nTotal generated tokens:                  49975\nRequest throughput (req/s):              7.42\nOutput token throughput (tok/s):         742.11\nPeak output token throughput (tok/s):    3000.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          15591.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          29078.68\nMedian TTFT (ms):                        32203.66\nP99 TTFT (ms):                           62077.78\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          194.54\nMedian TPOT (ms):                        192.07\nP99 TPOT (ms):                           358.38\n---------------Inter-token Latency----------------\nMean ITL (ms):                           194.46\nMedian ITL (ms):                         68.54\nP99 ITL (ms):                            4496.71\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  8.78\nTotal input tokens:                      128000\nTotal generated tokens:                  4000\nRequest throughput (req/s):              113.94\nOutput token throughput (tok/s):         455.76\nPeak output token throughput (tok/s):    2511.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          15040.15\n---------------Time to First Token----------------\nMean TTFT (ms):                          4491.88\nMedian TTFT (ms):                        4467.47\nP99 TTFT (ms):                           7693.24\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1249.72\nMedian TPOT (ms):                        1258.92\nP99 TPOT (ms):                           2252.88\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1249.72\nMedian ITL (ms):                         54.67\nP99 ITL (ms):                            6378.56\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nMaximum request concurrency:             4\nBenchmark duration (s):                  1292.33\nTotal input tokens:                      217393\nTotal generated tokens:                  201773\nRequest throughput (req/s):              0.77\nOutput token throughput (tok/s):         156.13\nPeak output token throughput (tok/s):    200.00\nPeak concurrent requests:                9.00\nTotal Token throughput (tok/s):          324.35\n---------------Time to First Token----------------\nMean TTFT (ms):                          112.38\nMedian TTFT (ms):                        111.05\nP99 TTFT (ms):                           154.06\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          25.42\nMedian TPOT (ms):                        24.47\nP99 TPOT (ms):                           35.05\n---------------Inter-token Latency----------------\nMean ITL (ms):                           25.08\nMedian ITL (ms):                         23.05\nP99 ITL (ms):                            79.54\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-32b/a100/","title":"Optimizing Qwen3-32B Throughput on NVIDIA A100 GPUs","text":""},{"location":"performance-lab/qwen3-32b/a100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-14B on A100 GPUs:</p> Serving Command <pre><code>trtllm-serve Qwen/Qwen3-32B --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 1730.98Mean TPOT(ms): 149.67 Total TPS: 2054.21 (+18.7%)Mean TPOT(ms): 89.01 Short Prompt Total TPS: 2656.71Mean TPOT(ms): 528.57 Total TPS: 3535.54 (+33.1%)Mean TPOT(ms): 1885.18 Medium Prompt Total TPS: 2521.38Mean TPOT(ms): 143.23 Total TPS: 2715.90 (+7.7%)Mean TPOT(ms): 145.49 Long Prompt Total TPS: 1837.08Mean TPOT(ms): 96.19 Total TPS: 2037.67 (+10.9%)Mean TPOT(ms): 100.09 Very Long Prompt Total TPS: 1798.45Mean TPOT(ms): 67.87 Total TPS: 1580.36 (-13.8%)Mean TPOT(ms): 47.37 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-32b/a100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-32b/a100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-32b/a100/#model","title":"Model","text":"<p>Qwen3-32B</p>"},{"location":"performance-lab/qwen3-32b/a100/#hardware","title":"Hardware","text":"<p>NVIDIA A100 GPUs</p>"},{"location":"performance-lab/qwen3-32b/a100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.5.post1</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/qwen3-32b/a100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-32b/a100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-32B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-32B --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-32b/a100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-32b/a100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-32B --max-model-len 32768\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  241.94\nTotal input tokens:                      217393\nTotal generated tokens:                  201396\nRequest throughput (req/s):              4.13\nOutput token throughput (tok/s):         832.43\nPeak output token throughput (tok/s):    1950.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1730.98\n---------------Time to First Token----------------\nMean TTFT (ms):                          105557.78\nMedian TTFT (ms):                        105990.93\nP99 TTFT (ms):                           209863.50\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          149.67\nMedian TPOT (ms):                        123.31\nP99 TPOT (ms):                           563.16\n---------------Inter-token Latency----------------\nMean ITL (ms):                           117.98\nMedian ITL (ms):                         61.18\nP99 ITL (ms):                            572.34\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-32B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  299.51\nTotal input tokens:                      217393\nTotal generated tokens:                  201439\nRequest throughput (req/s):              3.34\nOutput token throughput (tok/s):         672.56\nPeak output token throughput (tok/s):    1295.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1398.38\n---------------Time to First Token----------------\nMean TTFT (ms):                          139199.11\nMedian TTFT (ms):                        141086.43\nP99 TTFT (ms):                           271684.04\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          75.11\nMedian TPOT (ms):                        66.68\nP99 TPOT (ms):                           149.79\n---------------Inter-token Latency----------------\nMean ITL (ms):                           69.65\nMedian ITL (ms):                         49.10\nP99 ITL (ms):                            298.90\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-32B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  203.88\nTotal input tokens:                      217393\nTotal generated tokens:                  201412\nRequest throughput (req/s):              4.90\nOutput token throughput (tok/s):         987.91\nPeak output token throughput (tok/s):    1741.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2054.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          84105.84\nMedian TTFT (ms):                        84356.20\nP99 TTFT (ms):                           171829.62\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          89.01\nMedian TPOT (ms):                        79.85\nP99 TPOT (ms):                           265.32\n---------------Inter-token Latency----------------\nMean ITL (ms):                           78.87\nMedian ITL (ms):                         54.75\nP99 ITL (ms):                            318.76\n==================================================\n</code></pre> <p>Result: TensorRT-LLM (2054.21 tok/s) &gt; vLLM (1730.98 tok/s) &gt; SGLang (1398.38 tok/s)</p>"},{"location":"performance-lab/qwen3-32b/a100/#2-quantization-in-tensorrt-llmvllm","title":"2. Quantization in TensorRT-LLM/vLLM","text":"<p>TensorRT-LLM FP8</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-32B-FP8\n</code></pre> Benchmark result <pre><code># RuntimeError: Unsupported SM version for FP8 block scaling GEMM\n</code></pre> <p>TensorRT-LLM AWQ</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-32B-AWQ\n</code></pre> Benchmark result <pre><code># KeyError: 'weight'\n</code></pre> <p>vLLM FP8</p> Serving script <pre><code>vllm serve Qwen/Qwen3-32B-FP8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  267.77\nTotal input tokens:                      217393\nTotal generated tokens:                  201218\nRequest throughput (req/s):              3.73\nOutput token throughput (tok/s):         751.46\nPeak output token throughput (tok/s):    1965.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1563.33\n---------------Time to First Token----------------\nMean TTFT (ms):                          103256.52\nMedian TTFT (ms):                        94937.77\nP99 TTFT (ms):                           225680.67\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          339.76\nMedian TPOT (ms):                        310.91\nP99 TPOT (ms):                           944.57\n---------------Inter-token Latency----------------\nMean ITL (ms):                           273.16\nMedian ITL (ms):                         218.10\nP99 ITL (ms):                            954.38\n==================================================\n</code></pre> <p>vLLM AWQ</p> Serving script <pre><code>vllm serve Qwen/Qwen3-32B-AWQ\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  237.22\nTotal input tokens:                      217393\nTotal generated tokens:                  201832\nRequest throughput (req/s):              4.22\nOutput token throughput (tok/s):         850.81\nPeak output token throughput (tok/s):    2072.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1767.21\n---------------Time to First Token----------------\nMean TTFT (ms):                          92528.26\nMedian TTFT (ms):                        85550.21\nP99 TTFT (ms):                           202791.46\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          302.45\nMedian TPOT (ms):                        277.31\nP99 TPOT (ms):                           812.48\n---------------Inter-token Latency----------------\nMean ITL (ms):                           243.88\nMedian ITL (ms):                         203.46\nP99 ITL (ms):                            817.18\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-32b/a100/#3-max-batched-token-numbers-in-tensorrt-llm","title":"3. Max Batched Token Numbers in TensorRT-LLM","text":"Serving script <pre><code>trtllm-serve Qwen/Qwen3-14B --max_num_tokens=16384\n</code></pre> Benchmark result <pre><code># --max_num_tokens=16384\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  220.69\nTotal input tokens:                      217393\nTotal generated tokens:                  201360\nRequest throughput (req/s):              4.53\nOutput token throughput (tok/s):         912.43\nPeak output token throughput (tok/s):    1560.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1897.50\n---------------Time to First Token----------------\nMean TTFT (ms):                          90546.36\nMedian TTFT (ms):                        89254.77\nP99 TTFT (ms):                           184658.38\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          79.75\nMedian TPOT (ms):                        74.69\nP99 TPOT (ms):                           191.75\n---------------Inter-token Latency----------------\nMean ITL (ms):                           74.37\nMedian ITL (ms):                         53.56\nP99 ITL (ms):                            325.39\n==================================================\n\n# --max_num_tokens=32768\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  257.31\nTotal input tokens:                      217393\nTotal generated tokens:                  201414\nRequest throughput (req/s):              3.89\nOutput token throughput (tok/s):         782.77\nPeak output token throughput (tok/s):    1266.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1627.65\n---------------Time to First Token----------------\nMean TTFT (ms):                          113890.79\nMedian TTFT (ms):                        114913.27\nP99 TTFT (ms):                           225978.59\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          66.22\nMedian TPOT (ms):                        64.31\nP99 TPOT (ms):                           96.69\n---------------Inter-token Latency----------------\nMean ITL (ms):                           64.16\nMedian ITL (ms):                         49.74\nP99 ITL (ms):                            258.34\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-32b/a100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +18.7% Quantization - Max Batched Token Numbers -"},{"location":"performance-lab/qwen3-32b/a100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-32B\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  1783.59\nTotal input tokens:                      3200000\nTotal generated tokens:                  7699\nRequest throughput (req/s):              0.06\nOutput token throughput (tok/s):         4.32\nPeak output token throughput (tok/s):    21.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          1798.45\n---------------Time to First Token----------------\nMean TTFT (ms):                          890711.38\nMedian TTFT (ms):                        892708.86\nP99 TTFT (ms):                           1760024.36\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          67.87\nMedian TPOT (ms):                        67.32\nP99 TPOT (ms):                           147.19\n---------------Inter-token Latency----------------\nMean ITL (ms):                           64.54\nMedian ITL (ms):                         48.55\nP99 ITL (ms):                            690.90\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  1141.94\nTotal input tokens:                      1997942\nTotal generated tokens:                  99893\nRequest throughput (req/s):              0.44\nOutput token throughput (tok/s):         87.48\nPeak output token throughput (tok/s):    180.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          1837.08\n---------------Time to First Token----------------\nMean TTFT (ms):                          566579.70\nMedian TTFT (ms):                        564705.97\nP99 TTFT (ms):                           1116945.95\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          96.19\nMedian TPOT (ms):                        96.40\nP99 TPOT (ms):                           99.35\n---------------Inter-token Latency----------------\nMean ITL (ms):                           95.82\nMedian ITL (ms):                         51.55\nP99 ITL (ms):                            628.61\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  415.71\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              1.20\nOutput token throughput (tok/s):         120.27\nPeak output token throughput (tok/s):    381.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          2521.38\n---------------Time to First Token----------------\nMean TTFT (ms):                          208397.03\nMedian TTFT (ms):                        209108.79\nP99 TTFT (ms):                           408103.76\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          143.23\nMedian TPOT (ms):                        143.96\nP99 TPOT (ms):                           166.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           142.07\nMedian ITL (ms):                         51.49\nP99 ITL (ms):                            590.04\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  49.57\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              20.17\nOutput token throughput (tok/s):         80.69\nPeak output token throughput (tok/s):    200.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2656.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          31177.64\nMedian TTFT (ms):                        31073.76\nP99 TTFT (ms):                           48960.57\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          528.57\nMedian TPOT (ms):                        544.55\nP99 TPOT (ms):                           547.31\n---------------Inter-token Latency----------------\nMean ITL (ms):                           396.42\nMedian ITL (ms):                         543.22\nP99 ITL (ms):                            554.84\n==================================================\n</code></pre> Optimized serving script <pre><code>trtllm-serve Qwen/Qwen3-32B --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  2029.84\nTotal input tokens:                      3200000\nTotal generated tokens:                  7880\nRequest throughput (req/s):              0.05\nOutput token throughput (tok/s):         3.88\nPeak output token throughput (tok/s):    23.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          1580.36\n---------------Time to First Token----------------\nMean TTFT (ms):                          1040855.88\nMedian TTFT (ms):                        1047384.87\nP99 TTFT (ms):                           2008642.88\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          47.37\nMedian TPOT (ms):                        47.37\nP99 TPOT (ms):                           47.49\n---------------Inter-token Latency----------------\nMean ITL (ms):                           46.77\nMedian ITL (ms):                         47.36\nP99 ITL (ms):                            49.08\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  1029.52\nTotal input tokens:                      1997942\nTotal generated tokens:                  99874\nRequest throughput (req/s):              0.49\nOutput token throughput (tok/s):         97.01\nPeak output token throughput (tok/s):    241.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          2037.67\n---------------Time to First Token----------------\nMean TTFT (ms):                          509693.25\nMedian TTFT (ms):                        505676.34\nP99 TTFT (ms):                           1014720.64\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          100.09\nMedian TPOT (ms):                        98.07\nP99 TPOT (ms):                           115.57\n---------------Inter-token Latency----------------\nMean ITL (ms):                           99.60\nMedian ITL (ms):                         50.97\nP99 ITL (ms):                            2379.87\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  385.94\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              1.30\nOutput token throughput (tok/s):         129.55\nPeak output token throughput (tok/s):    480.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          2715.90\n---------------Time to First Token----------------\nMean TTFT (ms):                          189776.20\nMedian TTFT (ms):                        189321.80\nP99 TTFT (ms):                           378730.86\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          145.49\nMedian TPOT (ms):                        144.79\nP99 TPOT (ms):                           178.50\n---------------Inter-token Latency----------------\nMean ITL (ms):                           144.04\nMedian ITL (ms):                         51.18\nP99 ITL (ms):                            2256.49\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  37.25\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              26.85\nOutput token throughput (tok/s):         107.38\nPeak output token throughput (tok/s):    400.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3535.54\n---------------Time to First Token----------------\nMean TTFT (ms):                          19761.77\nMedian TTFT (ms):                        19497.33\nP99 TTFT (ms):                           36592.74\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1885.18\nMedian TPOT (ms):                        2140.52\nP99 TPOT (ms):                           2149.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1413.87\nMedian ITL (ms):                         2136.43\nP99 ITL (ms):                            2170.23\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-32b/h100/","title":"Optimizing Qwen3-32B Throughput on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/qwen3-32b/h100/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-32B on H100 GPUs:</p> Serving Command <pre><code>trtllm-serve Qwen/Qwen3-32B-FP8 --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 2352.82Mean TPOT(ms): 88.09 Total TPS: 4284.68 (+82.1%)Mean TPOT(ms): 158.64 Short Prompt Total TPS: 4005.62Mean TPOT(ms): 1169.72 Total TPS: 6826.33 (+70.4%)Mean TPOT(ms): 890.11 Medium Prompt Total TPS: 3195.30Mean TPOT(ms): 86.62 Total TPS: 5901.95 (+84.7%)Mean TPOT(ms): 223.40 Long Prompt Total TPS: 2148.69Mean TPOT(ms): 66.61 Total TPS: 4929.16 (+129.4%)Mean TPOT(ms): 138.75 Very Long Prompt Total TPS: 2771.81Mean TPOT(ms): 43.42 Total TPS: 4068.16 (+10.7%)Mean TPOT(ms): 286.97 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for high-throughput serving scenarios.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-32b/h100/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-32b/h100/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-32b/h100/#model","title":"Model","text":"<p>Qwen3-32B</p>"},{"location":"performance-lab/qwen3-32b/h100/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/qwen3-32b/h100/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.5.post1</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/qwen3-32b/h100/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-32b/h100/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-32B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-32B --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-32b/h100/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-32b/h100/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-32B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  178.01\nTotal input tokens:                      217393\nTotal generated tokens:                  201422\nRequest throughput (req/s):              5.62\nOutput token throughput (tok/s):         1131.55\nPeak output token throughput (tok/s):    2360.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2352.82\n---------------Time to First Token----------------\nMean TTFT (ms):                          78118.74\nMedian TTFT (ms):                        77291.46\nP99 TTFT (ms):                           153161.51\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          88.09\nMedian TPOT (ms):                        70.72\nP99 TPOT (ms):                           378.18\n---------------Inter-token Latency----------------\nMean ITL (ms):                           69.76\nMedian ITL (ms):                         50.20\nP99 ITL (ms):                            277.93\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-32B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  247.94\nTotal input tokens:                      217393\nTotal generated tokens:                  201826\nRequest throughput (req/s):              4.03\nOutput token throughput (tok/s):         814.03\nPeak output token throughput (tok/s):    1329.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          1690.84\n---------------Time to First Token----------------\nMean TTFT (ms):                          115690.30\nMedian TTFT (ms):                        117738.49\nP99 TTFT (ms):                           230503.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          57.86\nMedian TPOT (ms):                        54.19\nP99 TPOT (ms):                           95.84\n---------------Inter-token Latency----------------\nMean ITL (ms):                           56.25\nMedian ITL (ms):                         43.94\nP99 ITL (ms):                            207.79\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-32B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  157.59\nTotal input tokens:                      217393\nTotal generated tokens:                  201723\nRequest throughput (req/s):              6.35\nOutput token throughput (tok/s):         1280.04\nPeak output token throughput (tok/s):    1958.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          2659.50\n---------------Time to First Token----------------\nMean TTFT (ms):                          65281.52\nMedian TTFT (ms):                        65325.76\nP99 TTFT (ms):                           133106.50\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          61.90\nMedian TPOT (ms):                        59.59\nP99 TPOT (ms):                           121.97\n---------------Inter-token Latency----------------\nMean ITL (ms):                           59.22\nMedian ITL (ms):                         47.89\nP99 ITL (ms):                            214.01\n==================================================\n</code></pre> <p>Result: TensorRT-LLM (2659.50 tok/s) &gt; vLLM (2352.82 tok/s) &gt; SGLang (1690.84 tok/s)</p>"},{"location":"performance-lab/qwen3-32b/h100/#2-quantization-in-tensorrt-llm","title":"2. Quantization in TensorRT-LLM","text":"<p>FP8</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-32B-FP8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  104.55\nTotal input tokens:                      217393\nTotal generated tokens:                  201376\nRequest throughput (req/s):              9.56\nOutput token throughput (tok/s):         1926.06\nPeak output token throughput (tok/s):    4391.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4005.32\n---------------Time to First Token----------------\nMean TTFT (ms):                          35510.55\nMedian TTFT (ms):                        31577.26\nP99 TTFT (ms):                           79653.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          205.13\nMedian TPOT (ms):                        119.96\nP99 TPOT (ms):                           1141.98\n---------------Inter-token Latency----------------\nMean ITL (ms):                           118.32\nMedian ITL (ms):                         86.53\nP99 ITL (ms):                            1195.28\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-32b/h100/#3-enable-chunked-prefill-in-tensorrt-llm","title":"3. Enable Chunked Prefill in TensorRT-LLM","text":"Serving script <pre><code>trtllm-serve Qwen/Qwen3-32B-FP8 --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  97.74\nTotal input tokens:                      217393\nTotal generated tokens:                  201376\nRequest throughput (req/s):              10.23\nOutput token throughput (tok/s):         2060.40\nPeak output token throughput (tok/s):    4371.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4284.68\n---------------Time to First Token----------------\nMean TTFT (ms):                          31208.75\nMedian TTFT (ms):                        25003.73\nP99 TTFT (ms):                           72827.99\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          158.64\nMedian TPOT (ms):                        115.67\nP99 TPOT (ms):                           993.03\n---------------Inter-token Latency----------------\nMean ITL (ms):                           107.04\nMedian ITL (ms):                         85.28\nP99 ITL (ms):                            569.95\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-32b/h100/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +13.0% Quantization +50.6% Enable Chunked Prefill +7.0%"},{"location":"performance-lab/qwen3-32b/h100/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-32B\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  1157.25\nTotal input tokens:                      3200000\nTotal generated tokens:                  7681\nRequest throughput (req/s):              0.09\nOutput token throughput (tok/s):         6.64\nPeak output token throughput (tok/s):    24.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          2771.81\n---------------Time to First Token----------------\nMean TTFT (ms):                          585981.22\nMedian TTFT (ms):                        582757.65\nP99 TTFT (ms):                           1144686.57\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          43.42\nMedian TPOT (ms):                        43.36\nP99 TPOT (ms):                           44.34\n---------------Inter-token Latency----------------\nMean ITL (ms):                           42.86\nMedian ITL (ms):                         43.30\nP99 ITL (ms):                            46.54\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  976.29\nTotal input tokens:                      1997942\nTotal generated tokens:                  99790\nRequest throughput (req/s):              0.51\nOutput token throughput (tok/s):         102.21\nPeak output token throughput (tok/s):    186.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          2148.69\n---------------Time to First Token----------------\nMean TTFT (ms):                          482284.21\nMedian TTFT (ms):                        483375.28\nP99 TTFT (ms):                           956141.26\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          66.61\nMedian TPOT (ms):                        62.08\nP99 TPOT (ms):                           100.86\n---------------Inter-token Latency----------------\nMean ITL (ms):                           66.29\nMedian ITL (ms):                         43.38\nP99 ITL (ms):                            1395.23\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  328.04\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              1.52\nOutput token throughput (tok/s):         152.42\nPeak output token throughput (tok/s):    359.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          3195.30\n---------------Time to First Token----------------\nMean TTFT (ms):                          163909.40\nMedian TTFT (ms):                        164148.38\nP99 TTFT (ms):                           322980.86\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          86.62\nMedian TPOT (ms):                        84.24\nP99 TPOT (ms):                           131.41\n---------------Inter-token Latency----------------\nMean ITL (ms):                           85.97\nMedian ITL (ms):                         44.80\nP99 ITL (ms):                            1462.05\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  32.88\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              30.42\nOutput token throughput (tok/s):         121.66\nPeak output token throughput (tok/s):    522.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          4005.62\n---------------Time to First Token----------------\nMean TTFT (ms):                          21648.67\nMedian TTFT (ms):                        21342.36\nP99 TTFT (ms):                           32678.31\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1169.72\nMedian TPOT (ms):                        1292.78\nP99 TPOT (ms):                           1417.05\n---------------Inter-token Latency----------------\nMean ITL (ms):                           877.29\nMedian ITL (ms):                         1141.11\nP99 ITL (ms):                            1435.80\n==================================================\n</code></pre> Optimized serving script <pre><code>trtllm-serve Qwen/Qwen3-32B-FP8 --max_seq_len=40960 --enable_chunked_prefill\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  788.52\nTotal input tokens:                      3200000\nTotal generated tokens:                  7832\nRequest throughput (req/s):              0.13\nOutput token throughput (tok/s):         9.93\nPeak output token throughput (tok/s):    104.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          4068.16\n---------------Time to First Token----------------\nMean TTFT (ms):                          396294.90\nMedian TTFT (ms):                        397717.15\nP99 TTFT (ms):                           778540.06\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          286.97\nMedian TPOT (ms):                        255.80\nP99 TPOT (ms):                           581.09\n---------------Inter-token Latency----------------\nMean ITL (ms):                           284.98\nMedian ITL (ms):                         39.76\nP99 ITL (ms):                            2105.62\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  425.60\nTotal input tokens:                      1997942\nTotal generated tokens:                  99888\nRequest throughput (req/s):              1.17\nOutput token throughput (tok/s):         234.70\nPeak output token throughput (tok/s):    828.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          4929.16\n---------------Time to First Token----------------\nMean TTFT (ms):                          208082.20\nMedian TTFT (ms):                        204136.20\nP99 TTFT (ms):                           415000.76\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          138.75\nMedian TPOT (ms):                        141.52\nP99 TPOT (ms):                           151.74\n---------------Inter-token Latency----------------\nMean ITL (ms):                           138.01\nMedian ITL (ms):                         45.37\nP99 ITL (ms):                            1254.20\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  177.60\nTotal input tokens:                      998175\nTotal generated tokens:                  50000\nRequest throughput (req/s):              2.82\nOutput token throughput (tok/s):         281.53\nPeak output token throughput (tok/s):    1529.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          5901.95\n---------------Time to First Token----------------\nMean TTFT (ms):                          86596.60\nMedian TTFT (ms):                        86629.72\nP99 TTFT (ms):                           171842.84\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          223.40\nMedian TPOT (ms):                        237.63\nP99 TPOT (ms):                           249.66\n---------------Inter-token Latency----------------\nMean ITL (ms):                           221.16\nMedian ITL (ms):                         49.40\nP99 ITL (ms):                            1216.32\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  19.29\nTotal input tokens:                      127698\nTotal generated tokens:                  4000\nRequest throughput (req/s):              51.83\nOutput token throughput (tok/s):         207.33\nPeak output token throughput (tok/s):    860.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6826.33\n---------------Time to First Token----------------\nMean TTFT (ms):                          10718.84\nMedian TTFT (ms):                        10511.60\nP99 TTFT (ms):                           19055.52\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          890.11\nMedian TPOT (ms):                        949.23\nP99 TPOT (ms):                           1351.38\n---------------Inter-token Latency----------------\nMean ITL (ms):                           667.57\nMedian ITL (ms):                         925.89\nP99 ITL (ms):                            1854.09\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/910b/","title":"Optimizing Qwen3-8B Throughput on ASCEND 910B NPUs","text":""},{"location":"performance-lab/qwen3-8b/910b/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing throughput of Qwen3-8B on ASCEND 910B NPUs:</p> Serving Command <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve vllm-ascend/Qwen3-8B-W8A8 --enable-prefix-caching\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case baseline (vLLM without any optimizations) Optimized ShareGPT Total TPS: 3143.94Mean TPOT(ms): 27.61 Total TPS: 6256.39 (+99.0%)Mean TPOT(ms): 796.13 Short Prompt Total TPS: 5834.79Mean TPOT(ms): 125.55 Total TPS: 16611.86 (+184.7%)Mean TPOT(ms): 1131.26 Medium Prompt Total TPS: 9589.00Mean TPOT(ms): 140.39 Total TPS: 14484.86 (+51.1%)Mean TPOT(ms): 122.59 Long Prompt Total TPS: 7677.41Mean TPOT(ms): 99.26 Total TPS: 11159.01 (+45.4%)Mean TPOT(ms): 97.74 Very Long Prompt Total TPS: 2890.64Mean TPOT(ms): 344.18 Total TPS: 7835.01 (+171.1%)Mean TPOT(ms): 169.70 <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, Torch Compile, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-8b/910b/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve high throughput under high-concurrency request scenarios.</p>"},{"location":"performance-lab/qwen3-8b/910b/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-8b/910b/#model","title":"Model","text":"<p>Qwen/Qwen3-30B-A3B</p>"},{"location":"performance-lab/qwen3-8b/910b/#hardware","title":"Hardware","text":"<p>Ascend 910B NPUs</p>"},{"location":"performance-lab/qwen3-8b/910b/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM-Ascend: v0.9.1</li> <li>MindIE: 2.1RC1</li> </ul>"},{"location":"performance-lab/qwen3-8b/910b/#benchmark-dataset","title":"Benchmark Dataset","text":"<ol> <li>ShareGPT</li> <li>Random dataset with varying sequence lengths:<ul> <li>Very long prompt: 32000 input tokens, 100 output tokens</li> <li>Long prompt: 4000 input tokens, 200 output tokens</li> <li>Medium prompt: 2000 input tokens, 100 output tokens</li> <li>Short prompt: 128 input tokens, 4 output tokens</li> </ul> </li> </ol>"},{"location":"performance-lab/qwen3-8b/910b/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-8B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n\n# Benchmark on random dataset (fixed seed for reproducibility)\nvllm bench serve --model Qwen/Qwen3-8B --backend openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 4000 --random-output-len 200 --num-prompts 500 --seed 42\n</code></pre>"},{"location":"performance-lab/qwen3-8b/910b/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-8b/910b/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-8B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  133.23\nTotal input tokens:                      217393\nTotal generated tokens:                  201469\nRequest throughput (req/s):              7.51\nOutput token throughput (tok/s):         1512.21\nPeak output token throughput (tok/s):    2821.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          3143.94\n---------------Time to First Token----------------\nMean TTFT (ms):                          43963.16\nMedian TTFT (ms):                        41177.19\nP99 TTFT (ms):                           91086.90\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          125.09\nMedian TPOT (ms):                        122.07\nP99 TPOT (ms):                           269.65\n---------------Inter-token Latency----------------\nMean ITL (ms):                           112.90\nMedian ITL (ms):                         78.96\nP99 ITL (ms):                            280.07\n==================================================\n</code></pre> <p>MindIE</p> Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve Qwen/Qwen3-8B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  74.36\nTotal input tokens:                      217393\nTotal generated tokens:                  201609\nRequest throughput (req/s):              13.45\nOutput token throughput (tok/s):         2711.10\nPeak output token throughput (tok/s):    4588.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5634.45\n---------------Time to First Token----------------\nMean TTFT (ms):                          8242.22\nMedian TTFT (ms):                        8298.00\nP99 TTFT (ms):                           18074.38\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          788.31\nMedian TPOT (ms):                        204.41\nP99 TPOT (ms):                           6547.97\n---------------Inter-token Latency----------------\nMean ITL (ms):                           155.96\nMedian ITL (ms):                         45.35\nP99 ITL (ms):                            218.57\n==================================================\n</code></pre> <p>Result: MineIE (5634.4 tok/s) &gt; vLLM (3143.94 tok/s)</p>"},{"location":"performance-lab/qwen3-8b/910b/#2-quantization-in-mindie","title":"2. Quantization in MindIE","text":"Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve vllm-ascend/Qwen3-8B-W8A8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  69.32\nTotal input tokens:                      217393\nTotal generated tokens:                  201315\nRequest throughput (req/s):              14.42\nOutput token throughput (tok/s):         2903.93\nPeak output token throughput (tok/s):    4799.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6039.79\n---------------Time to First Token----------------\nMean TTFT (ms):                          5834.61\nMedian TTFT (ms):                        5648.87\nP99 TTFT (ms):                           11181.32\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          822.87\nMedian TPOT (ms):                        188.93\nP99 TPOT (ms):                           7710.20\n---------------Inter-token Latency----------------\nMean ITL (ms):                           148.97\nMedian ITL (ms):                         45.31\nP99 ITL (ms):                            97.87\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/910b/#3-enable-prefix-cache-in-mindie","title":"3. Enable Prefix Cache in MindIE","text":"Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve vllm-ascend/Qwen3-8B-W8A8 --enable-prefix-caching\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  66.96\nTotal input tokens:                      217393\nTotal generated tokens:                  201517\nRequest throughput (req/s):              14.93\nOutput token throughput (tok/s):         3009.64\nPeak output token throughput (tok/s):    4985.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6256.39\n---------------Time to First Token----------------\nMean TTFT (ms):                          5320.32\nMedian TTFT (ms):                        5159.10\nP99 TTFT (ms):                           10384.83\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          796.13\nMedian TPOT (ms):                        183.35\nP99 TPOT (ms):                           7267.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           141.91\nMedian ITL (ms):                         43.55\nP99 ITL (ms):                            115.20\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/910b/#4-batch-size-tuning-in-mindie","title":"4. Batch Size Tuning in MindIE","text":"Serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve vllm-ascend/Qwen3-8B-W8A8 --enable-prefix-caching --max-batch-size=400 --max-prefill-batch-size=200\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  69.07\nTotal input tokens:                      217393\nTotal generated tokens:                  201424\nRequest throughput (req/s):              14.48\nOutput token throughput (tok/s):         2916.34\nPeak output token throughput (tok/s):    6017.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          6063.89\n---------------Time to First Token----------------\nMean TTFT (ms):                          6335.34\nMedian TTFT (ms):                        5812.45\nP99 TTFT (ms):                           11260.65\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          581.93\nMedian TPOT (ms):                        167.14\nP99 TPOT (ms):                           5501.66\n---------------Inter-token Latency----------------\nMean ITL (ms):                           143.92\nMedian ITL (ms):                         70.27\nP99 ITL (ms):                            281.16\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/910b/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +79.2% Quantization +7.2% Prefix Caching +3.6% Batch Size Tuning -"},{"location":"performance-lab/qwen3-8b/910b/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration to evaluate its generalization under various workloads.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-8B\n</code></pre> Baseline benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  1110.48\nTotal input tokens:                      3200000\nTotal generated tokens:                  10000\nRequest throughput (req/s):              0.09\nOutput token throughput (tok/s):         9.01\nPeak output token throughput (tok/s):    147.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          2890.64\n---------------Time to First Token----------------\nMean TTFT (ms):                          559550.01\nMedian TTFT (ms):                        561560.86\nP99 TTFT (ms):                           1096851.78\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          344.18\nMedian TPOT (ms):                        346.89\nP99 TPOT (ms):                           668.43\n---------------Inter-token Latency----------------\nMean ITL (ms):                           340.74\nMedian ITL (ms):                         48.54\nP99 ITL (ms):                            59.04\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  273.17\nTotal input tokens:                      1997463\nTotal generated tokens:                  99812\nRequest throughput (req/s):              1.83\nOutput token throughput (tok/s):         365.38\nPeak output token throughput (tok/s):    1062.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          7677.41\n---------------Time to First Token----------------\nMean TTFT (ms):                          132541.20\nMedian TTFT (ms):                        135284.67\nP99 TTFT (ms):                           262452.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          99.26\nMedian TPOT (ms):                        100.01\nP99 TPOT (ms):                           175.24\n---------------Inter-token Latency----------------\nMean ITL (ms):                           98.69\nMedian ITL (ms):                         55.84\nP99 ITL (ms):                            134.52\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  109.30\nTotal input tokens:                      998057\nTotal generated tokens:                  50000\nRequest throughput (req/s):              4.57\nOutput token throughput (tok/s):         457.47\nPeak output token throughput (tok/s):    1904.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          9589.00\n---------------Time to First Token----------------\nMean TTFT (ms):                          53978.52\nMedian TTFT (ms):                        57661.39\nP99 TTFT (ms):                           103428.31\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          140.39\nMedian TPOT (ms):                        142.80\nP99 TPOT (ms):                           275.05\n---------------Inter-token Latency----------------\nMean ITL (ms):                           139.05\nMedian ITL (ms):                         63.53\nP99 ITL (ms):                            627.10\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  22.59\nTotal input tokens:                      127782\nTotal generated tokens:                  4000\nRequest throughput (req/s):              44.28\nOutput token throughput (tok/s):         177.10\nPeak output token throughput (tok/s):    1280.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          5834.79\n---------------Time to First Token----------------\nMean TTFT (ms):                          18464.39\nMedian TTFT (ms):                        17489.55\nP99 TTFT (ms):                           22134.55\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          125.55\nMedian TPOT (ms):                        135.76\nP99 TPOT (ms):                           149.27\n---------------Inter-token Latency----------------\nMean ITL (ms):                           94.16\nMedian ITL (ms):                         83.83\nP99 ITL (ms):                            251.91\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  275.57\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              0.36\nOutput token throughput (tok/s):         80.06\nPeak output token throughput (tok/s):    92.00\nPeak concurrent requests:                8.00\nTotal Token throughput (tok/s):          164.46\n---------------Time to First Token----------------\nMean TTFT (ms):                          145.88\nMedian TTFT (ms):                        142.78\nP99 TTFT (ms):                           201.58\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          47.28\nMedian TPOT (ms):                        47.29\nP99 TPOT (ms):                           50.41\n---------------Inter-token Latency----------------\nMean ITL (ms):                           47.10\nMedian ITL (ms):                         46.72\nP99 ITL (ms):                            91.82\n==================================================\n</code></pre> Optimized serving script <pre><code># This is a simplified vLLM-like command. GPUStack maps arguments to the\n# corresponding MindIE JSON configuration and runs with mindieservice_daemon.\nmindie serve vllm-ascend/Qwen3-8B-W8A8 --enable-prefix-caching\n</code></pre> Optimized benchmark results <pre><code># random 32K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nBenchmark duration (s):                  296.36\nTotal input tokens:                      3200000\nTotal generated tokens:                  9693\nRequest throughput (req/s):              0.34\nOutput token throughput (tok/s):         32.71\nPeak output token throughput (tok/s):    330.00\nPeak concurrent requests:                100.00\nTotal Token throughput (tok/s):          10830.52\n---------------Time to First Token----------------\nMean TTFT (ms):                          147133.58\nMedian TTFT (ms):                        147851.57\nP99 TTFT (ms):                           289974.37\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          242.08\nMedian TPOT (ms):                        232.74\nP99 TPOT (ms):                           741.49\n---------------Inter-token Latency----------------\nMean ITL (ms):                           225.71\nMedian ITL (ms):                         46.46\nP99 ITL (ms):                            56.81\n==================================================\n\n# random 4K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  157.15\nTotal input tokens:                      2000000\nTotal generated tokens:                  99961\nRequest throughput (req/s):              3.18\nOutput token throughput (tok/s):         636.07\nPeak output token throughput (tok/s):    2142.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          13362.49\n---------------Time to First Token----------------\nMean TTFT (ms):                          70716.71\nMedian TTFT (ms):                        74982.88\nP99 TTFT (ms):                           148427.09\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          130.97\nMedian TPOT (ms):                        134.98\nP99 TPOT (ms):                           208.16\n---------------Inter-token Latency----------------\nMean ITL (ms):                           130.95\nMedian ITL (ms):                         63.84\nP99 ITL (ms):                            489.52\n==================================================\n\n# random 2K input\n============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  67.34\nTotal input tokens:                      1000000\nTotal generated tokens:                  49975\nRequest throughput (req/s):              7.42\nOutput token throughput (tok/s):         742.11\nPeak output token throughput (tok/s):    3000.00\nPeak concurrent requests:                500.00\nTotal Token throughput (tok/s):          15591.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          29078.68\nMedian TTFT (ms):                        32203.66\nP99 TTFT (ms):                           62077.78\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          194.54\nMedian TPOT (ms):                        192.07\nP99 TPOT (ms):                           358.38\n---------------Inter-token Latency----------------\nMean ITL (ms):                           194.46\nMedian ITL (ms):                         68.54\nP99 ITL (ms):                            4496.71\n==================================================\n\n# random 128 input\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  8.78\nTotal input tokens:                      128000\nTotal generated tokens:                  4000\nRequest throughput (req/s):              113.94\nOutput token throughput (tok/s):         455.76\nPeak output token throughput (tok/s):    2511.00\nPeak concurrent requests:                1000.00\nTotal Token throughput (tok/s):          15040.15\n---------------Time to First Token----------------\nMean TTFT (ms):                          4491.88\nMedian TTFT (ms):                        4467.47\nP99 TTFT (ms):                           7693.24\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1249.72\nMedian TPOT (ms):                        1258.92\nP99 TPOT (ms):                           2252.88\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1249.72\nMedian ITL (ms):                         54.67\nP99 ITL (ms):                            6378.56\n==================================================\n\n# ShareGPT batch size 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nMaximum request concurrency:             4\nBenchmark duration (s):                  1292.33\nTotal input tokens:                      217393\nTotal generated tokens:                  201773\nRequest throughput (req/s):              0.77\nOutput token throughput (tok/s):         156.13\nPeak output token throughput (tok/s):    200.00\nPeak concurrent requests:                9.00\nTotal Token throughput (tok/s):          324.35\n---------------Time to First Token----------------\nMean TTFT (ms):                          112.38\nMedian TTFT (ms):                        111.05\nP99 TTFT (ms):                           154.06\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          25.42\nMedian TPOT (ms):                        24.47\nP99 TPOT (ms):                           35.05\n---------------Inter-token Latency----------------\nMean ITL (ms):                           25.08\nMedian ITL (ms):                         23.05\nP99 ITL (ms):                            79.54\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/","title":"Optimizing Qwen3-8B Latency on NVIDIA H100 GPUs","text":""},{"location":"performance-lab/qwen3-8b/h100-latency/#conclusion","title":"Conclusion","text":"<p>Recommended configuration for optimizing latency of Qwen3-8B on H100 GPUs:</p> Serving Command <pre><code>python3 -m sglang.launch_server \\\n    --model Qwen/Qwen3-8B-FP8 \\\n    --speculative-algorithm EAGLE3 \\\n    --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 \\\n    --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n</code></pre> <p>Comparison of benchmark results before and after optimization:</p> Benchmark Case Baseline (vLLM without any optimizations) Optimized ShareGPT BS=1 Mean latency: 1.66s/req Mean latency: 0.62s/req (2.7x faster) ShareGPT BS=2 Mean latency: 1.68s/req Mean latency: 0.71s/req (2.4x faster) ShareGPT BS=4 Mean latency: 1.68s/req Mean latency: 0.77s/req (2.2x faster) ShareGPT BS=8 Mean latency: 1.78s/req Mean latency: 1.10s/req (1.6x faster) <p>Note</p> <ol> <li>Our benchmark tests do not cover all possible optimization combinations. For example, we select the inference engine that performs best under its default configuration as the starting point for further tuning. This pruning approach yields a local optimum, which may not be the global optimum.</li> <li>There are other optimization methods that depend on specific user scenarios, including max batch size, schedule configuration, extended KV cache, CUDA graph, etc. The conclusions in this document can serve as a starting point for more targeted optimizations.</li> <li>The tests are conducted on specific hardware and software setups. Advances in the inference engine may lead to new conclusions.</li> <li>Although using quantization may impact accuracy. FP8 quantization can achieves less than 1% accuracy drop for most models. See the evaluation results for more details. Therefore, it is highly recommended to use FP8 quantization for low-latency serving scenarios.</li> <li>Speculative decoding can significantly reduce latency for low-concurrency requests. However, the acceleration effect may vary depending on the data distribution of different benchmark datasets and the choice of draft models. For example, the chosen draft model here is trained on English data, which may lead to suboptimal performance on other languages.</li> </ol> <p>If there are any missing points or updates reflecting new changes, please let us know.</p>"},{"location":"performance-lab/qwen3-8b/h100-latency/#optimization-objective","title":"Optimization Objective","text":"<p>Achieve low latency under low-concurrency (batch size &lt; 8) request scenarios.</p>"},{"location":"performance-lab/qwen3-8b/h100-latency/#experimental-setup","title":"Experimental Setup","text":""},{"location":"performance-lab/qwen3-8b/h100-latency/#model","title":"Model","text":"<p>Qwen/Qwen3-8B</p>"},{"location":"performance-lab/qwen3-8b/h100-latency/#hardware","title":"Hardware","text":"<p>NVIDIA H100 GPUs</p>"},{"location":"performance-lab/qwen3-8b/h100-latency/#engine-version","title":"Engine Version","text":"<ul> <li>vLLM: v0.11.0</li> <li>SGLang: v0.5.3.post3</li> <li>TensorRT-LLM: v1.2.0rc1</li> </ul>"},{"location":"performance-lab/qwen3-8b/h100-latency/#benchmark-dataset","title":"Benchmark Dataset","text":"<p>ShareGPT</p>"},{"location":"performance-lab/qwen3-8b/h100-latency/#benchmark-script","title":"Benchmark Script","text":"<p>We use the vLLM bench CLI tool to benchmark the model performance. The following command is used to run the benchmark:</p> <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-8B --backend openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 100 --max-concurrency 4\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/#experiment-results","title":"Experiment Results","text":""},{"location":"performance-lab/qwen3-8b/h100-latency/#1-choosing-the-inference-engine","title":"1. Choosing the Inference Engine","text":"<p>vLLM</p> Serving script <pre><code>vllm serve Qwen/Qwen3-8B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  44.40\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.25\nOutput token throughput (tok/s):         496.88\nPeak output token throughput (tok/s):    540.00\nPeak concurrent requests:                13.00\nTotal Token throughput (tok/s):          1020.76\n---------------Time to First Token----------------\nMean TTFT (ms):                          64.00\nMedian TTFT (ms):                        60.26\nP99 TTFT (ms):                           98.59\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.38\nMedian TPOT (ms):                        7.40\nP99 TPOT (ms):                           7.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.78\nMedian ITL (ms):                         7.37\nP99 ITL (ms):                            40.98\n==================================================\n</code></pre> <p>SGLang</p> Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  41.97\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.38\nOutput token throughput (tok/s):         525.59\nPeak output token throughput (tok/s):    572.00\nPeak concurrent requests:                12.00\nTotal Token throughput (tok/s):          1079.74\n---------------Time to First Token----------------\nMean TTFT (ms):                          27.70\nMedian TTFT (ms):                        27.25\nP99 TTFT (ms):                           43.35\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.19\nMedian TPOT (ms):                        7.14\nP99 TPOT (ms):                           7.90\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.08\nMedian ITL (ms):                         7.04\nP99 ITL (ms):                            11.48\n==================================================\n</code></pre> <p>TensorRT-LLM</p> Serving script <pre><code>trtllm-serve Qwen/Qwen3-8B\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  45.26\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.21\nOutput token throughput (tok/s):         487.38\nPeak output token throughput (tok/s):    541.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          1001.25\n---------------Time to First Token----------------\nMean TTFT (ms):                          51.61\nMedian TTFT (ms):                        45.46\nP99 TTFT (ms):                           134.71\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.67\nMedian TPOT (ms):                        7.58\nP99 TPOT (ms):                           10.15\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.59\nMedian ITL (ms):                         7.41\nP99 ITL (ms):                            24.38\n==================================================\n</code></pre> <p>Mean latency per request: SGLang(1.61 s) &lt; vLLM (1.68 s) &gt; TensorRT-LLM (1.74 s)</p>"},{"location":"performance-lab/qwen3-8b/h100-latency/#2-maximum-batch-size-for-cuda-graph-in-sglang","title":"2. Maximum batch size for cuda graph in SGLang","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B --cuda-graph-max-bs 8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  42.04\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.38\nOutput token throughput (tok/s):         524.71\nPeak output token throughput (tok/s):    572.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          1077.94\n---------------Time to First Token----------------\nMean TTFT (ms):                          28.46\nMedian TTFT (ms):                        27.35\nP99 TTFT (ms):                           39.82\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.20\nMedian TPOT (ms):                        7.15\nP99 TPOT (ms):                           8.13\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.09\nMedian ITL (ms):                         7.04\nP99 ITL (ms):                            11.56\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/#3-quantization","title":"3. Quantization","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B-FP8\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  33.54\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.98\nOutput token throughput (tok/s):         657.83\nPeak output token throughput (tok/s):    741.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          1351.41\n---------------Time to First Token----------------\nMean TTFT (ms):                          39.66\nMedian TTFT (ms):                        33.38\nP99 TTFT (ms):                           134.98\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          5.65\nMedian TPOT (ms):                        5.58\nP99 TPOT (ms):                           7.35\n---------------Inter-token Latency----------------\nMean ITL (ms):                           5.58\nMedian ITL (ms):                         5.39\nP99 ITL (ms):                            22.20\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/#4-attention-backend","title":"4. Attention Backend","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B-FP8 --attention-backend triton\n</code></pre> Benchmark result <pre><code># triton\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  34.73\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.88\nOutput token throughput (tok/s):         635.15\nPeak output token throughput (tok/s):    744.00\nPeak concurrent requests:                10.00\nTotal Token throughput (tok/s):          1304.82\n---------------Time to First Token----------------\nMean TTFT (ms):                          60.18\nMedian TTFT (ms):                        43.84\nP99 TTFT (ms):                           493.14\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          5.77\nMedian TPOT (ms):                        5.63\nP99 TPOT (ms):                           7.78\n---------------Inter-token Latency----------------\nMean ITL (ms):                           5.71\nMedian ITL (ms):                         5.40\nP99 ITL (ms):                            28.58\n==================================================\n\n# flashinfer\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  39.63\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.52\nOutput token throughput (tok/s):         556.61\nPeak output token throughput (tok/s):    728.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          1143.47\n---------------Time to First Token----------------\nMean TTFT (ms):                          269.33\nMedian TTFT (ms):                        40.70\nP99 TTFT (ms):                           5700.99\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          5.71\nMedian TPOT (ms):                        5.63\nP99 TPOT (ms):                           7.44\n---------------Inter-token Latency----------------\nMean ITL (ms):                           5.65\nMedian ITL (ms):                         5.40\nP99 ITL (ms):                            28.71\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/#5-speculative-decoding","title":"5. Speculative Decoding","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B-FP8 \\\n    --speculative-algorithm EAGLE3 \\\n    --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 \\\n    --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n</code></pre> Benchmark result <pre><code># --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  22.53\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.44\nOutput token throughput (tok/s):         979.24\nPeak output token throughput (tok/s):    393.00\nPeak concurrent requests:                15.00\nTotal Token throughput (tok/s):          2011.70\n---------------Time to First Token----------------\nMean TTFT (ms):                          50.70\nMedian TTFT (ms):                        43.71\nP99 TTFT (ms):                           368.54\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.75\nMedian TPOT (ms):                        3.32\nP99 TPOT (ms):                           9.63\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.05\nMedian ITL (ms):                         9.86\nP99 ITL (ms):                            38.76\n==================================================\n\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path AngelSlim/Qwen3-8B_eagle3 --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  26.89\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              3.72\nOutput token throughput (tok/s):         820.28\nPeak output token throughput (tok/s):    404.00\nPeak concurrent requests:                13.00\nTotal Token throughput (tok/s):          1685.15\n---------------Time to First Token----------------\nMean TTFT (ms):                          44.60\nMedian TTFT (ms):                        44.68\nP99 TTFT (ms):                           78.96\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.45\nMedian TPOT (ms):                        4.40\nP99 TPOT (ms):                           7.91\n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.49\nMedian ITL (ms):                         9.88\nP99 ITL (ms):                            38.17\n==================================================\n\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 2 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  24.16\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.14\nOutput token throughput (tok/s):         913.20\nPeak output token throughput (tok/s):    433.00\nPeak concurrent requests:                12.00\nTotal Token throughput (tok/s):          1876.03\n---------------Time to First Token----------------\nMean TTFT (ms):                          49.89\nMedian TTFT (ms):                        44.38\nP99 TTFT (ms):                           277.15\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.98\nMedian TPOT (ms):                        3.74\nP99 TPOT (ms):                           7.81\n---------------Inter-token Latency----------------\nMean ITL (ms):                           9.82\nMedian ITL (ms):                         8.86\nP99 ITL (ms):                            38.40\n==================================================\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 8 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  27.17\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              3.68\nOutput token throughput (tok/s):         811.85\nPeak output token throughput (tok/s):    336.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          1667.83\n---------------Time to First Token----------------\nMean TTFT (ms):                          64.58\nMedian TTFT (ms):                        56.79\nP99 TTFT (ms):                           366.49\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.27\nMedian TPOT (ms):                        4.00\nP99 TPOT (ms):                           8.41\n---------------Inter-token Latency----------------\nMean ITL (ms):                           13.94\nMedian ITL (ms):                         12.20\nP99 ITL (ms):                            63.04\n==================================================\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 4 --speculative-eagle-topk 8 --speculative-num-draft-tokens 16\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  22.72\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.40\nOutput token throughput (tok/s):         971.03\nPeak output token throughput (tok/s):    382.00\nPeak concurrent requests:                14.00\nTotal Token throughput (tok/s):          1994.83\n---------------Time to First Token----------------\nMean TTFT (ms):                          49.08\nMedian TTFT (ms):                        36.31\nP99 TTFT (ms):                           335.43\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.62\nMedian TPOT (ms):                        3.36\nP99 TPOT (ms):                           9.61\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.24\nMedian ITL (ms):                         10.14\nP99 ITL (ms):                            41.05\n==================================================\n\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 32\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  22.68\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.41\nOutput token throughput (tok/s):         972.69\nPeak output token throughput (tok/s):    354.00\nPeak concurrent requests:                15.00\nTotal Token throughput (tok/s):          1998.25\n---------------Time to First Token----------------\nMean TTFT (ms):                          50.97\nMedian TTFT (ms):                        40.01\nP99 TTFT (ms):                           361.52\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.54\nMedian TPOT (ms):                        3.38\nP99 TPOT (ms):                           7.82\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.90\nMedian ITL (ms):                         10.67\nP99 ITL (ms):                            40.09\n==================================================\n\n\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 8\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  23.94\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.18\nOutput token throughput (tok/s):         921.47\nPeak output token throughput (tok/s):    399.00\nPeak concurrent requests:                14.00\nTotal Token throughput (tok/s):          1893.03\n---------------Time to First Token----------------\nMean TTFT (ms):                          47.83\nMedian TTFT (ms):                        45.01\nP99 TTFT (ms):                           255.59\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.71\nMedian TPOT (ms):                        3.59\nP99 TPOT (ms):                           6.20\n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.71\nMedian ITL (ms):                         9.78\nP99 ITL (ms):                            39.48\n==================================================\n\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 --speculative-num-steps 2 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  26.70\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              3.75\nOutput token throughput (tok/s):         826.28\nPeak output token throughput (tok/s):    521.00\nPeak concurrent requests:                12.00\nTotal Token throughput (tok/s):          1697.47\n---------------Time to First Token----------------\nMean TTFT (ms):                          56.09\nMedian TTFT (ms):                        44.66\nP99 TTFT (ms):                           327.69\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.37\nMedian TPOT (ms):                        4.10\nP99 TPOT (ms):                           7.78\n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.78\nMedian ITL (ms):                         7.62\nP99 ITL (ms):                            48.17\n==================================================\n\n# --speculative-algorithm EAGLE3 --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  25.80\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              3.88\nOutput token throughput (tok/s):         854.98\nPeak output token throughput (tok/s):    392.00\nPeak concurrent requests:                11.00\nTotal Token throughput (tok/s):          1756.43\n---------------Time to First Token----------------\nMean TTFT (ms):                          53.61\nMedian TTFT (ms):                        45.08\nP99 TTFT (ms):                           374.04\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.07\nMedian TPOT (ms):                        3.68\nP99 TPOT (ms):                           12.12\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.69\nMedian ITL (ms):                         10.35\nP99 ITL (ms):                            40.08\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/#6-torchcompile","title":"6. Torch.compile","text":"Serving script <pre><code>python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B-FP8 \\\n  --enable-torch-compile \\\n  --speculative-algorithm EAGLE3 \\\n  --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 \\\n  --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n</code></pre> Benchmark result <pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  23.14\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.32\nOutput token throughput (tok/s):         953.40\nPeak output token throughput (tok/s):    386.00\nPeak concurrent requests:                14.00\nTotal Token throughput (tok/s):          1958.61\n---------------Time to First Token----------------\nMean TTFT (ms):                          54.86\nMedian TTFT (ms):                        45.34\nP99 TTFT (ms):                           410.48\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.53\nMedian TPOT (ms):                        3.30\nP99 TPOT (ms):                           9.67\n---------------Inter-token Latency----------------\nMean ITL (ms):                           11.38\nMedian ITL (ms):                         9.96\nP99 ITL (ms):                            41.21\n==================================================\n</code></pre>"},{"location":"performance-lab/qwen3-8b/h100-latency/#summary-of-optimization-options","title":"Summary of Optimization Options","text":"Optimization Option Throughput Improvement Engine Selection +4.3% Quantization +25.8% Speculative Decoding +66.2% Max CUDA graph batch size - Attention Backend - Torch.compile -"},{"location":"performance-lab/qwen3-8b/h100-latency/#other-benchmark-cases","title":"Other Benchmark Cases","text":"<p>We further benchmarked the optimized configuration with different batch sizes.</p> Baseline serving script <pre><code>vllm serve Qwen/Qwen3-8B\n</code></pre> Baseline benchmark results <pre><code># BS=1\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             1\nBenchmark duration (s):                  166.04\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              0.60\nOutput token throughput (tok/s):         132.86\nPeak output token throughput (tok/s):    140.00\nPeak concurrent requests:                5.00\nTotal Token throughput (tok/s):          272.95\n---------------Time to First Token----------------\nMean TTFT (ms):                          59.01\nMedian TTFT (ms):                        60.38\nP99 TTFT (ms):                           73.49\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.28\nMedian TPOT (ms):                        7.22\nP99 TPOT (ms):                           7.52\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.26\nMedian ITL (ms):                         7.23\nP99 ITL (ms):                            8.17\n==================================================\n\n# BS=2\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             2\nBenchmark duration (s):                  85.78\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              1.17\nOutput token throughput (tok/s):         257.19\nPeak output token throughput (tok/s):    277.00\nPeak concurrent requests:                7.00\nTotal Token throughput (tok/s):          528.35\n---------------Time to First Token----------------\nMean TTFT (ms):                          63.78\nMedian TTFT (ms):                        64.78\nP99 TTFT (ms):                           93.70\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.37\nMedian TPOT (ms):                        7.30\nP99 TPOT (ms):                           8.57\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.44\nMedian ITL (ms):                         7.28\nP99 ITL (ms):                            8.75\n==================================================\n\n# BS=4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  44.40\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.25\nOutput token throughput (tok/s):         496.88\nPeak output token throughput (tok/s):    540.00\nPeak concurrent requests:                13.00\nTotal Token throughput (tok/s):          1020.76\n---------------Time to First Token----------------\nMean TTFT (ms):                          64.00\nMedian TTFT (ms):                        60.26\nP99 TTFT (ms):                           98.59\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.38\nMedian TPOT (ms):                        7.40\nP99 TPOT (ms):                           7.96\n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.78\nMedian ITL (ms):                         7.37\nP99 ITL (ms):                            40.98\n==================================================\n\n# BS=8\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             8\nBenchmark duration (s):                  24.05\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.16\nOutput token throughput (tok/s):         917.28\nPeak output token throughput (tok/s):    1039.00\nPeak concurrent requests:                15.00\nTotal Token throughput (tok/s):          1884.42\n---------------Time to First Token----------------\nMean TTFT (ms):                          74.57\nMedian TTFT (ms):                        60.77\nP99 TTFT (ms):                           204.11\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.71\nMedian TPOT (ms):                        7.67\nP99 TPOT (ms):                           8.19\n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.57\nMedian ITL (ms):                         7.63\nP99 ITL (ms):                            45.48\n==================================================\n</code></pre> Optimized serving script <pre><code>python3 -m sglang.launch_server \\\n  --model Qwen/Qwen3-8B-FP8 \\\n  --speculative-algorithm EAGLE3 \\\n  --speculative-draft-model-path Tengyunw/qwen3_8b_eagle3 \\\n  --speculative-num-steps 4 --speculative-eagle-topk 4 --speculative-num-draft-tokens 16\n</code></pre> Optimized benchmark results <pre><code># BS=1\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             1\nBenchmark duration (s):                  65.82\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              1.52\nOutput token throughput (tok/s):         335.15\nPeak output token throughput (tok/s):    112.00\nPeak concurrent requests:                6.00\nTotal Token throughput (tok/s):          688.51\n---------------Time to First Token----------------\nMean TTFT (ms):                          34.70\nMedian TTFT (ms):                        34.11\nP99 TTFT (ms):                           40.56\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          2.67\nMedian TPOT (ms):                        2.63\nP99 TPOT (ms):                           3.46\n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.74\nMedian ITL (ms):                         8.95\nP99 ITL (ms):                            10.46\n==================================================\n\n# BS=2\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             2\nBenchmark duration (s):                  37.62\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              2.66\nOutput token throughput (tok/s):         586.35\nPeak output token throughput (tok/s):    210.00\nPeak concurrent requests:                8.00\nTotal Token throughput (tok/s):          1204.57\n---------------Time to First Token----------------\nMean TTFT (ms):                          39.41\nMedian TTFT (ms):                        41.19\nP99 TTFT (ms):                           57.69\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.04\nMedian TPOT (ms):                        2.96\nP99 TPOT (ms):                           5.41\n---------------Inter-token Latency----------------\nMean ITL (ms):                           9.69\nMedian ITL (ms):                         9.57\nP99 ITL (ms):                            34.00\n==================================================\n\n# BS=4\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             4\nBenchmark duration (s):                  21.09\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              4.74\nOutput token throughput (tok/s):         1046.22\nPeak output token throughput (tok/s):    387.00\nPeak concurrent requests:                14.00\nTotal Token throughput (tok/s):          2149.30\n---------------Time to First Token----------------\nMean TTFT (ms):                          39.93\nMedian TTFT (ms):                        41.92\nP99 TTFT (ms):                           61.96\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          3.34\nMedian TPOT (ms):                        3.17\nP99 TPOT (ms):                           5.61\n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.54\nMedian ITL (ms):                         9.84\nP99 ITL (ms):                            35.23\n==================================================\n\n# BS=8\n============ Serving Benchmark Result ============\nSuccessful requests:                     100\nMaximum request concurrency:             8\nBenchmark duration (s):                  13.46\nTotal input tokens:                      23260\nTotal generated tokens:                  22061\nRequest throughput (req/s):              7.43\nOutput token throughput (tok/s):         1638.82\nPeak output token throughput (tok/s):    655.00\nPeak concurrent requests:                19.00\nTotal Token throughput (tok/s):          3366.71\n---------------Time to First Token----------------\nMean TTFT (ms):                          42.27\nMedian TTFT (ms):                        43.26\nP99 TTFT (ms):                           93.68\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.82\nMedian TPOT (ms):                        3.94\nP99 TPOT (ms):                           21.36\n---------------Inter-token Latency----------------\nMean ITL (ms):                           13.06\nMedian ITL (ms):                         10.94\nP99 ITL (ms):                            37.37\n==================================================\n</code></pre>"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/","title":"Evaluating LMCache Prefill Acceleration in vLLM","text":"<p>LMCache is an extensible KV Cache Layer for LLM inference designed to address key challenges in large-scale deployment scenarios. This documentation evaluates the performance impact of LMCache on vLLM inference, particularly focusing on prefill stage acceleration and its implications for various workload patterns.</p>"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#conclusions","title":"Conclusions","text":"<ol> <li> <p>LMCache provides significant prefill acceleration in scenarios with high cache hit rates, achieving up to +355.3% input TPS improvement and -58.8% reduction in TTFT for long-context (20K tokens) multi-turn conversations in the experiments.</p> </li> <li> <p>Performance benefits are highly workload-dependent:    - Optimal scenarios: Multi-turn conversations with shared prefixes and repeated patterns    - Suboptimal scenarios: Random inputs with no cache reuse patterns</p> </li> <li> <p>Chunk size optimization The default 256 chunk size shows the optimal results in tested configurations.</p> </li> <li> <p>Cache miss scenarios incur overhead, showing -3% to -15% performance degradation when no cache reuse occurs, making LMCache most suitable for workloads with predictable prefix patterns.</p> </li> </ol>"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#technical-background","title":"Technical Background","text":""},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#lmcache-overview","title":"LMCache Overview","text":"<p>LMCache extends vLLM's KV cache capabilities through:</p> Component Description CPU Offloading Extends cache capacity beyond GPU VRAM limits Chunk-based Management Efficient cache storage and retrieval with configurable chunk sizes Multiple Backends Support for local storage, Redis, and custom backends like Mooncake Distributed KV Cache Shared cache across multiple vLLM instances"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#key-use-cases","title":"Key Use Cases","text":"<ol> <li>Low Prefix Cache Hit Rates: Mitigates GPU VRAM limitations and cache eviction issues</li> <li>Distributed Cache Sharing: Enables cache sharing across multiple vLLM instances</li> <li>PD Disaggregation: Supports disaggregated deployment architectures</li> </ol>"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#experimental-setup","title":"Experimental Setup","text":"<ul> <li>Model: Qwen3-8B</li> <li>Hardware: NVIDIA RTX 4090 24GB</li> <li>vLLM Version: v0.10.1.1</li> <li>Benchmark Method: Multi-turn conversation benchmark</li> </ul> Serving Commands <pre><code># Standard vLLM serving\nvllm serve Qwen/Qwen3-8B\n\n# LMCache-enabled serving\n##### lmcache_config.yaml\nchunk_size: 256\nlocal_cpu: true\nmax_local_cpu_size: 50\n#####\nLMCACHE_CONFIG_FILE=/root/lmcache_config.yaml vllm serve /root/Qwen3-8B \\\n--kv-transfer-config '{\"kv_connector\":\"LMCacheConnectorV1\", \"kv_role\":\"kv_both\"}'\n</code></pre> Benchmark Scripts <pre><code># Multi-turn bench scripts\n# Ref: https://github.com/vllm-project/vllm/tree/main/benchmarks/multi_turn\n\n##### generate_multi_turn.json\n{\n    \"filetype\": \"generate_conversations\",\n    \"num_conversations\": 24,\n    \"text_files\": [\"pg1184.txt\"],\n    \"print_stats\": false,\n    \"prompt_input\": {\n        \"num_turns\": {\n            \"distribution\": \"uniform\",\n            \"min\": 12,\n            \"max\": 18\n        },\n        \"common_prefix_num_tokens\": {\n            \"distribution\": \"constant\",\n            \"value\": 500\n        },\n        \"prefix_num_tokens\": {\n            \"distribution\": \"lognormal\",\n            \"average\": 4000,\n            \"max\": 20000\n        },\n        \"num_tokens\": {\n            \"distribution\": \"uniform\",\n            \"min\": 120,\n            \"max\": 160\n        }\n    },\n    \"prompt_output\": {\n        \"num_tokens\": {\n            \"distribution\": \"uniform\",\n            \"min\": 80,\n            \"max\": 120\n        }\n    }\n}\n#####\n\npython benchmark_serving_multi_turn.py --model $MODEL_PATH --input-file generate_multi_turn.json --num-clients 10 --max-active-conversations 10\n</code></pre>"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#experimental-results","title":"Experimental Results","text":""},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#multi-turn-conversation-performance","title":"Multi-turn Conversation Performance","text":""},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#5k-input-tokens","title":"5K Input Tokens","text":"Configuration Input TPS Total TPS Mean TTFT (ms) Mean TPOT (ms) Without LMCache 5849 5957 4350.48 48.47 With LMCache 9426 (+61.2%) 9592 2646.09 (-39.2%) 30.60 (-36.9%)"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#20k-input-tokens","title":"20K Input Tokens","text":"Configuration Input TPS Total TPS Mean TTFT (ms) Mean TPOT (ms) Without LMCache 4312.17 4335.71 5070.52 33.91 With LMCache 7750.60 (+79.7%) 7792.92 2091.00 (-58.8%) 25.83 (-23.8%)"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#20k-input-tokens-1-output-token","title":"20K Input Tokens + 1 Output Token","text":"Configuration Input TPS Total TPS Mean TTFT (ms) Without LMCache 7443.2 7443.6 4658.66 With LMCache 33887.9 (+355.3%) 33889.8 980.87"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#tuning-chunk-size","title":"Tuning Chunk Size","text":"Chunk Size Input TPS Performance Gain Mean TTFT (ms) 64 33820.3 +354.4% 985.28 256 33887.9 +355.3% 980.87 1024 31634.0 +325.0% 1055.69"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#cache-miss-scenarios-random-dataset","title":"Cache Miss Scenarios (Random Dataset)","text":"Benchmark Scripts <pre><code>vllm bench serve --model Qwen/Qwen3-8B --endpoint-type openai-chat --endpoint /v1/chat/completions --dataset-name random --random-input-len 1024 --random-output-len 128 --num-prompts 100 --seed 40\n</code></pre>"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#1k-input-tokens","title":"1K Input Tokens","text":"Metric Without LMCache With LMCache Change Output TPS 579.86 561.44 -3.2% Total TPS 5212.32 5046.72 -3.2% Mean TTFT (ms) 8886.36 9242.72 +4.0% Mean TPOT (ms) 42.08 43.47 +3.3%"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#8k-input-tokens","title":"8K Input Tokens","text":"Metric Without LMCache With LMCache Change Output TPS 77.87 66.77 -14.3% Total TPS 5060.79 4338.96 -14.3% Mean TTFT (ms) 80610.70 92682.22 +15.0% Mean TPOT (ms) 43.33 42.27 -2.4%"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#20k-input-tokens_1","title":"20K Input Tokens","text":"Metric Without LMCache With LMCache Change Output TPS 22.97 21.77 -5.2% Total TPS 3698.09 3504.41 -5.2% Mean TTFT (ms) 277456.13 292811.62 +5.5% Mean TPOT (ms) 31.68 32.80 +3.5%"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#all-vram-kv-cache-hit-scenarios","title":"All VRAM KV Cache Hit Scenarios","text":""},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#1k-input-tokens_1","title":"1K Input Tokens","text":"Metric Without LMCache With LMCache Change Output TPS 5954.33 5752.71 -3.3% Total TPS 53589.01 51802.45 -3.3% Mean TTFT (ms) 3052.08 3247.10 +6.4% Mean TPOT (ms) 38.40 39.04 +1.7%"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#8k-input-tokens_1","title":"8K Input Tokens","text":"Metric Without LMCache With LMCache Change Output TPS 3676.71 3656.30 -0.6% Total TPS 238986.41 237659.44 -0.6% Mean TTFT (ms) 5060.41 5326.37 +5.3% Mean TPOT (ms) 54.37 53.86 -1.0%"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#20k-input-tokens_2","title":"20K Input Tokens","text":"Metric Without LMCache With LMCache Change Output TPS 2213.12 1972.32 -10.9% Total TPS 356312.70 317543.74 -10.9% Mean TTFT (ms) 9649.76 10109.51 +4.8% Mean TPOT (ms) 87.10 94.26 +8.2%"},{"location":"performance-lab/references/evaluating-lmcache-prefill-acceleration-in-vllm/#remote-storage-backend-performance-20k-tokens-ttft","title":"Remote Storage Backend Performance (20K Tokens TTFT)","text":"Backend Cache Miss (s) Cache Hit (s) Performance Boost lmcache_server 0.739 0.324 2.28x Redis 0.746 0.388 1.92x Mooncake (TCP) 0.759 0.362 2.10x"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/","title":"The Impact of Quantization on vLLM Inference Performance","text":"<p>Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).</p> <p>The purpose of this documentation is to evaluate commonly used quantization techniques in vLLM and how they affect inference performance, especially throughput in high concurrency scenarios.</p>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#conclusions","title":"Conclusions","text":"<ol> <li> <p>Weight-activation quantization (e.g., FP8) provides significant performance improvements with minimal quality loss and is recommended for most scenarios. Static quantization delivers higher throughput than dynamic, with accuracy dependent on calibration data:</p> <ul> <li>Dynamic FP8: +14.8% TPS, -14.1% TTFT, -21.8% TPOT</li> <li>Static FP8: +26.7% TPS, -20.4% TTFT, -32.4% TPOT</li> </ul> </li> <li> <p>Most quantization approaches can substantially improve throughput in VRAM-constrained scenarios, with up to +46% improvement observed in experiments.</p> </li> <li> <p>Weight-only quantization causes performance degradation in TPS, TTFT, and TPOT due to dequantization overhead when VRAM is not constrained.</p> </li> <li> <p>Among weight-only methods, AWQ and GPTQ deliver the best inference performance, while bitsandbytes and GGUF show poor performance or compatibility issues and are not recommended.</p> </li> <li> <p>Default vLLM kernels for AWQ and GPTQ outperform custom kernel implementations in experimental testing.</p> </li> <li> <p>KV-Cache quantization provides relatively modest throughput improvements compared to other optimization techniques.</p> </li> </ol>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#technical-background","title":"Technical Background","text":""},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#quantization-type","title":"Quantization Type","text":"Quantization Type Description Weight-only Only the weights are quantized after training; activations remain full-precision Dynamic Weights are pre-quantized; activations are quantized on-the-fly during inference Static Weights and activations are quantized ahead of time after calibration with a representative dataset Quantization-aware Training Simulates quantization during training so the model adapts to reduced precision"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#calibration","title":"Calibration","text":"<p>Calibration is the step during quantization where the float32 ranges are computed. For weights it is quite easy since the actual range is known at quantization-time. But it is less clear for activations, and different approaches exist:</p> <ol> <li> <p>Post-training dynamic quantization: The range for each activation is computed on the fly at runtime. While this gives great results without too much work, it can be slower than static quantization because of the overhead introduced by computing the range each time. It is also not an option on certain hardware.</p> </li> <li> <p>Post-training static quantization: The range for each activation is computed in advance at quantization-time, typically by passing representative data through the model and recording the activation values. In practice, the steps are:    - Observers are put on activations to record their values    - A certain number of forward passes on a calibration dataset is done (around 200 examples is enough)    - The ranges for each computation are computed according to some calibration technique</p> </li> <li> <p>Quantization-aware training: The range for each activation is computed at training-time, following the same idea as post-training static quantization. But \"fake quantize\" operators are used instead of observers: they record values just as observers do, but they also simulate the error induced by quantization to let the model adapt to it.</p> </li> </ol>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#kv-cache-quantization","title":"KV Cache Quantization","text":"<p>KV cache quantization reduces memory footprint during inference by storing key-value cache in lower precision, allowing more tokens to be cached and improving throughput. vLLM supports FP8 datatypes (E4M3 and E5M2) but not INT8 KV cache. While research has explored 4-bit or 2-bit KV cache quantization, these approaches typically cause noticeable accuracy degradation such as reduced MMLU scores.</p>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#quantization-kernels","title":"Quantization Kernels","text":"<p>vLLM offers multiple quantization kernel implementations for quantization methods like AWQ and GPTQ. For AWQ quantization, the official AWQ kernel serves as the default, while GPTQ models utilize the ExLlamaV2 kernel by default. Additional optimized kernels such as Marlin and Machete are also available, offering enhanced performance particularly for larger batch sizes.</p>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#inference-quality-degradation","title":"Inference Quality Degradation","text":"<p>Quantization can lead to degradation in inference quality. While this documentation primarily focuses on performance metrics, we provide the following references for assessing quality impact:</p> <p>Qwen3-8B model benchmarks on various quantization methods:</p> Quantization CEVAL MMLU GSM8K HUMANEVAL BF16 79.27 74.78 87.79 63.41 FP8-Static 78.23 74.79 86.96 62.20 FP8-Dynamic 78.45 74.75 87.64 62.80 INT8-Dynamic 78.01 74.84 86.96 67.07 INT4-GPTQ 77.19 73.26 86.43 62.20 INT4-AWQ 76.15 73.59 86.96 63.41 <p>For more details, please refer to the AngelSlim benchmarks.</p>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#experimental-setup","title":"Experimental Setup","text":"<ul> <li>Model: Qwen3-8B</li> <li>Hardware: NVIDIA RTX 4090 24GB / A100 80GB / H100 80GB</li> <li>vLLM Version: v0.9.2</li> <li>Dataset: ShareGPT</li> <li>Benchmark Script: <pre><code># Prepare the ShareGPT dataset\nwget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n\n# Benchmark on ShareGPT dataset\nvllm bench serve --model Qwen/Qwen3-8B --endpoint-type openai-chat --endpoint /v1/chat/completions --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000\n</code></pre></li> </ul>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#experimental-results","title":"Experimental Results","text":""},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#rtx-4090","title":"RTX 4090","text":"Quantization Method Throughput (tok/s) Mean TTFT (ms) Mean TPOT (ms) BF16 3869.3 15385.7 63.45 AWQ 5653.4 (+46.1%) 7913.4 87.7 AWQ Marlin 5536.7 (+43.1%) 8133.8 90.42 GPTQ (Int8) 4918.98 (+27.1%) 8415.70 96.07 GPTQ Marlin 5025.82 (+29.9%) 8143.93 93.05"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#a100-80gb","title":"A100 80GB","text":"Quantization Method Throughput (tok/s) Mean TTFT (ms) Mean TPOT (ms) BF16 10338.25 3412.85 200.02 GPTQ-Marlin (Int4) 8146.73 10336.24 261.81 GPTQ (Int4) 8129.27 10414.74 261.64 AWQ 9611.61 3950.64 249.06 AWQ Marlin 8066.03 10506.70 264.33 GPTQ Marlin (Int8) 7119.60 12359.22 309.37 GPTQ (Int8) 7100.46 12380.82 309.34 bitsandbytes 5916.34 9115.43 252.91 GGUF (Q4_K_M) N/A N/A N/A <p>Note</p> <p>GGUF model with architecture qwen3 is not supported.</p>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#h100-80gb","title":"H100 80GB","text":"Quantization Method Throughput (tok/s) Mean TTFT (ms) Mean TPOT (ms) FP8 Static 16452.52 (+26.7%) 4116.87 (-20.4%) 85.57 (-32.4%) FP8 Dynamic 15275.64 (+14.8%) 4445.10 (-14.1%) 98.94 (-21.8%) Int4-W4A16 13605.46 5302.14 130.54 BF16 13305.38 5172.78 126.55 AWQ 9756.06 8794.29 209.13 GPTQ N/A N/A N/A <p>Note</p> <p>GPTQ implementation is broken, see vLLM issue.</p>"},{"location":"performance-lab/references/the-impact-of-quantization-on-vllm-inference-performance/#kv-cache-quantization_1","title":"KV Cache Quantization","text":"Configuration Throughput (tok/s) Mean TTFT (ms) Mean TPOT (ms) BF16 (Baseline) 13305.38 5172.78 126.55 BF16 + KV Cache FP8 (E4M3) 13513.19 (+1.6%) 5688.39 103.21"},{"location":"tutorials/adding-gpucluster-using-digitalocean/","title":"Adding a GPU Cluster Using DigitalOcean","text":"<p>When creating a cluster, GPUStack can leverage DigitalOcean to create workers and add them to the GPUStack cluster.</p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#preparation","title":"Preparation","text":"<p>You need to sign up for a DigitalOcean account and create a Personal Access Token on the API page.</p> <p></p> <p>Note: The token scope must be set to Full Access. If you select permissions using Custom Scopes, you may encounter issues deleting droplets.</p> <p>When starting the GPUStack Server, you need to specify the <code>--server-external-url</code> parameter. This parameter is used to configure the worker's <code>--server-url</code> after the droplet is created and the worker is started. If your server is running behind a proxy, please set the proxy address to ensure that droplets running on the public network can access the GPUStack Server API using this address after startup.</p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#create-digitalocean-cluster","title":"Create DigitalOcean Cluster","text":"<p>Please refer to the steps in Creating DigitalOcean Cluster</p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#create-cloud-credential","title":"Create Cloud Credential","text":"<p>Create a DigitalOcean cloud credential on the <code>Cloud Credentials</code> page.</p> <p></p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#create-cluster-with-cloud-credential","title":"Create Cluster with Cloud Credential","text":"<p>On the cluster creation page, select <code>DigitalOcean</code> as the <code>Cloud Provider</code>:</p> <p></p> <p>Enter a name, select the cloud credential you just created, and choose a region that supports GPU droplets.</p> <p></p> <p>Click <code>Next</code> to create a worker pool for the cluster.</p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#create-worker-pool","title":"Create Worker Pool","text":"<p>Fill in the <code>Name</code>, <code>Replicas</code>, and <code>Batch Size</code> as needed. Then select the <code>Instance Type</code> (droplet size in DigitalOcean terms) for the worker pool.</p> <p></p> <p>Next, select the <code>OS Image</code>.</p> <p></p> <p>Currently, only Nvidia series GPU Droplets are supported, as AMD GPU Droplets are often unavailable. Only Debian-based operating systems are supported.</p> <ul> <li>For Debian distributions, the droplet is bootstrapped with the <code>nvidia-open</code> driver, CUDA 12.8, and nvidia-container-toolkit 1.17.8-1 via cloud-init.</li> <li>For Ubuntu distributions, the droplet is bootstrapped with the <code>nvidia-driver-570</code> driver, CUDA 12.8, and nvidia-container-toolkit 1.17.8-1 via cloud-init.</li> <li>For Nvidia AI/ML Ready (Recommended), which is based on Ubuntu 22.04 and includes drivers, CUDA, and container-toolkit pre-installed, no extra packages are needed.</li> </ul> <p>Labels and volumes are supported to set in worker pool to provision worker. The volumes created and attached for droplet won't be mount into worker container automatically. You can modify the run worker script in <code>/opt/gpustack-run-worker.sh</code> to mount the volumes as needed.</p> <p>Click <code>Save</code> if all set.</p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#waiting-for-workers-to-be-provisioned","title":"Waiting for Workers to be Provisioned","text":"<p>After saving the cluster, navigate to the <code>Workers</code> page to view the provisioning progress of DigitalOcean workers.</p> <p></p> <p>The provisioning process includes several steps:</p> <ol> <li>Create an SSH key.</li> <li>Create a droplet with the SSH key.</li> <li>Wait for the droplet to start.</li> <li>Wait for the public IP to be assigned.</li> <li>Create volumes and attach them to the droplet.</li> <li>The worker enters the <code>Initialized</code> status and waits for the worker container to start and connect to the server.</li> </ol> <p>Once the worker reaches the <code>Ready</code> status, you can deploy models on it.</p> <p></p>"},{"location":"tutorials/adding-gpucluster-using-digitalocean/#safely-scale-down-digitalocean-workers","title":"Safely Scale Down DigitalOcean Workers","text":"<p>When a DigitalOcean worker is no longer needed, follow these steps to safely destroy the droplet:</p> <ul> <li>Adjust the replica count of the worker pool to match the number of workers you want to delete. Note that workers are not deleted automatically in this step.</li> <li>Ensure that no model instances are deployed on the workers you intend to delete.</li> <li>Delete the workers as needed. The corresponding droplets will be deleted accordingly.</li> </ul>"},{"location":"tutorials/adding-gpucluster-using-kubernetes/","title":"Adding a GPU Cluster Using Kubernetes","text":"<p>GPUStack supports adding a Kubernetes cluster as a GPU cluster.</p>"},{"location":"tutorials/adding-gpucluster-using-kubernetes/#preparation","title":"Preparation","text":"<p>A Kubernetes cluster should be built with runtime class installed and configured on all nodes. In this tutorial, we will use a k3s cluster as example.</p>"},{"location":"tutorials/adding-gpucluster-using-kubernetes/#create-kubernetes-cluster","title":"Create Kubernetes Cluster","text":"<p>Please refer to the steps in Register Kubernetes Cluster</p> <p>After created the <code>Kubernetes</code> provider cluster, select Nvidia as the <code>GPU Vendor</code> and click <code>Next</code>.</p> <p></p> <p>Use the script to check the environment, run the bash script in the k3s installed host.</p> <p></p> <p>Please keep in mind that, the <code>runtimeclass</code> resource in k8s is just a data record and doesn't represent the container runtime is well configured in every node. Make sure the container runtime for specified GPU vendor is configured or the worker won't be able to start.</p> <p>Copy the script in <code>Run Command</code> step and run it in the k3s installed host.</p> <p></p> <p></p>"},{"location":"tutorials/adding-gpucluster-using-kubernetes/#waiting-for-workers-to-be-provisioned","title":"Waiting for Workers to be Provisioned","text":"<p>After applied the manifests into the Kubernetes cluster, we can wait for the worker nodes populated in <code>Workers</code> Page.</p> <p></p> <p>Once the worker reaches the <code>Ready</code> status, you can deploy models on it.</p>"},{"location":"tutorials/inference-on-cpus/","title":"Running Inference on CPUs","text":"<p>GPUStack supports inference on CPUs, offering flexibility when GPU resources are limited or when model sizes exceed allocatable GPU memory. The following CPU inference modes are available:</p> <ul> <li>Hybrid CPU+GPU Inference: Enables partial acceleration by offloading portions of large models to the CPU when VRAM capacity is insufficient.</li> <li>Full CPU Inference: Runs entirely on CPU when no GPU resources are available.</li> </ul> <p>Note</p> <p>Available for custom backends only.</p> <p>When CPU offloading is enabled, GPUStack will allocate CPU memory if GPU resources are insufficient. You must correctly configure the inference backend to use hybrid CPU+GPU or full CPU inference.</p> <p>It is strongly recommended to use CPU inference only on CPU workers.</p> <p>For example, to deploy a model with CPU inference by Text Embeddings Inference, follow the configuration below:</p> <p>Source: <code>HuggingFace</code></p> <p>Repo ID: <code>BAAI/bge-large-en-v1.5</code></p> <p>Backend: <code>Custom</code></p> <p>Image Name: <code>ghcr.io/huggingface/text-embeddings-inference:cpu-1.8</code></p> <p>Execution Command: <code>--model-id BAAI/bge-large-en-v1.5 --huggingface-hub-cache /var/lib/gpustack/cache/huggingface --port {{port}}</code></p> <p>Note</p> <p><code>TEI (Text Embeddings Inference)</code> only supports deploying models from <code>HuggingFace</code>.</p> <p><code>ghcr.io/huggingface/text-embeddings-inference:cpu-1.8</code> is the CPU inference image for TEI. See: TEI Supported Hardware.</p> <p><code>--huggingface-hub-cache /var/lib/gpustack/cache/huggingface</code> sets the location of the HuggingFace Hub cache for TEI to the path where GPUStack stores downloaded HuggingFace models. The default path is <code>/var/lib/gpustack/cache/huggingface</code>. See: TEI CLI Arguments.</p> <p><code>{{port}}</code> is a placeholder that represents the port automatically assigned by GPUStack.</p> <p></p> <p>And in Advanced Settings, check <code>Allow CPU Offloading</code>:</p> <p></p> <p>If you need to access non-OpenAI-compatible APIs, you can also check <code>Enable Generic Proxy</code>.</p> <p>For more details, see Enable Generic Proxy.</p>"},{"location":"tutorials/inference-with-tool-calling/","title":"Inference with Tool Calling","text":"<p>Tool calling allows you to connect models to external tools and systems. This is useful for many things such as empowering AI assistants with capabilities, or building deep integrations between your applications and the models.</p> <p>In this tutorial, you\u2019ll learn how to set up and use tool calling within GPUStack to extend your AI\u2019s capabilities.</p> <p>Note</p> <ol> <li>Tool calling is supported in vLLM and SGLang inference backends.</li> <li>Tool calling is essentially achieved through prompt engineering, requiring models to be trained with internalized templates to enable this capability. Therefore, not all LLMs support tool calling.</li> </ol>"},{"location":"tutorials/inference-with-tool-calling/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following:</p> <ul> <li>GPUStack is installed and running.</li> <li>A Linux worker node with a GPU is available. We'll use Qwen2.5-7B-Instruct as the model for this tutorial. The model requires a GPU with at least 18GB VRAM.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul>"},{"location":"tutorials/inference-with-tool-calling/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":""},{"location":"tutorials/inference-with-tool-calling/#deploy-from-catalog","title":"Deploy from Catalog","text":"<p>LLMs that support tool calling are marked with the <code>tools</code> capability in the catalog. When you select such a model from the catalog, tool calling is enabled by default.</p>"},{"location":"tutorials/inference-with-tool-calling/#example-of-custom-deployment-using-llama-box","title":"Example of Custom Deployment Using llama-box","text":"<p>When you deploy GGUF models using llama-box, tool calling is enabled by default for models that support it.</p> <ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI and click the <code>Deploy Model</code> button. In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Enable the <code>GGUF</code> checkbox to filter models by GGUF format.</li> <li>Use the search bar to find the <code>Qwen/Qwen2.5-7B-Instruct-GGUF</code> model.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p></p>"},{"location":"tutorials/inference-with-tool-calling/#example-of-custom-deployment-using-vllm","title":"Example of Custom Deployment Using vLLM","text":"<p>When you deploy models using vLLM, you need to enable tool calling with additional parameters.</p> <ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI and click the <code>Deploy Model</code> button. In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar to find the <code>Qwen/Qwen2.5-7B-Instruct</code> model.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button and add the following parameters:</li> </ol> <ul> <li><code>--enable-auto-tool-choice</code></li> <li><code>--tool-call-parser=hermes</code></li> </ul> <ol> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Deployments</code> page.</p>"},{"location":"tutorials/inference-with-tool-calling/#step-2-generate-an-api-key","title":"Step 2: Generate an API Key","text":"<p>We will use the GPUStack API to interact with the model. To do this, you need to generate an API key:</p> <ol> <li>Hover over the user avatar and navigate to the <code>API Keys</code> page.</li> <li>Click the <code>New API Key</code> button.</li> <li>Enter a name for the API key and click the <code>Save</code> button.</li> <li>Copy the generated API key for later use.</li> </ol>"},{"location":"tutorials/inference-with-tool-calling/#step-3-do-inference","title":"Step 3: Do Inference","text":"<p>With the model deployed and an API key, you can call the model via the GPUStack API. Here is an example script using <code>curl</code> (replace <code>&lt;your-server-url&gt;</code> with your GPUStack server URL and <code>&lt;your-api-key&gt;</code> with the API key generated in the previous step):</p> <pre><code>export GPUSTACK_SERVER_URL=&lt;your-server-url&gt;\nexport GPUSTACK_API_KEY=&lt;your-api-key&gt;\ncurl $GPUSTACK_SERVER_URL/v1-openai/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n-d '{\n  \"model\": \"qwen2.5-7b-instruct\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What'\\''s the weather like in Boston today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"\n}'\n</code></pre> <p>Example response:</p> <pre><code>{\n  \"model\": \"qwen2.5-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"tool_calls\": [\n          {\n            \"id\": \"chatcmpl-tool-b99d32848b324eaea4bac5a5830d00b8\",\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"get_current_weather\",\n              \"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\"\n            }\n          }\n        ]\n      },\n      \"finish_reason\": \"tool_calls\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 212,\n    \"total_tokens\": 242,\n    \"completion_tokens\": 30\n  }\n}\n</code></pre>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/","title":"Running DeepSeek R1 671B with Distributed Ascend MindIE","text":"<p>This tutorial guides you through the process of configuring and running the original DeepSeek R1 671B using Distributed Ascend MindIE on a GPUStack cluster.</p> <p>Due to the extremely large size of the model, distributed inference across multiple workers is usually required.</p> <p>GPUStack enables easy setup and orchestration of distributed inference using Ascend MindIE, making it possible to run massive models like DeepSeek R1 with minimal manual configuration.</p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure the following requirements are met:</p> <ul> <li>You have access to a sufficient number of Linux nodes, each equipped with the required Ascend Atlas AI processors. For example:</li> </ul> Server(NPU) Number of Nodes Atlas 800T A2 (910B3 x 8) 4 <ul> <li>All nodes need to support Huawei Collective Communication adaptive Protocol (HCCP), installed Huawei Collective Communication Library (HCCL), and enabled Huawei Cache Coherence System (HCCS).</li> <li>Model files should be downloaded to the same path on each node. While GPUStack supports on-the-fly model downloading, pre-downloading is recommended as it can be time consuming depending on the network speed.</li> </ul> <p>Note</p> <ul> <li>In this tutorial, we assume a setup of 4 nodes, each equipped with 8 910B3 NPUs and connected via 200G Huawei Cache Conherence Network (HCCN).</li> <li>Altas NPUs do not support the FP8 precision originally used by DeepSeek R1. Hence, we use the BF16 version from Unsloth.</li> </ul>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#step-1-install-gpustack-server","title":"Step 1: Install GPUStack Server","text":"<p>According to the Ascend Installation, you can use the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --env \"ASCEND_VISIBLE_DEVICES=$(sudo ls /dev/davinci* | head -1 | grep -o '[0-9]\\+' || echo \"0\")\" \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --volume /path/to/your/model:/path/to/your/model \\\n    --runtime ascend \\\n    gpustack/gpustack\n</code></pre> <p>Note</p> <ul> <li>Replace <code>/path/to/your/model</code> with the actual path on your system where the DeepSeek R1 model files are stored.</li> <li>Ensure the <code>hccn_tool</code> tool is installed and configured correctly on your system. This is required for discvoring the HCCN network communication.</li> </ul> <p>After GPUStack server is up and running, run the following commands to get the initial admin password:</p> <pre><code>sudo docker exec gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#step-2-access-gpustack-ui","title":"Step 2: Access GPUStack UI","text":"<p>Login to the GPUStack UI using the <code>admin</code> user and the obtained password.</p> <pre><code>http://your_gpustack_server_ip_or_hostname\n</code></pre>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#step-3-install-gpustack-workers","title":"Step 3: Install GPUStack Workers","text":"<p>Navigate to the <code>Workers</code> page in the GPUStack UI, click <code>Add Worker</code> button to get the command for adding workers.</p> <p>And then on each worker node, run the worker adding command to start a GPUStack worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --env \"ASCEND_VISIBLE_DEVICES=$(sudo ls /dev/davinci* | head -1 | grep -o '[0-9]\\+' || echo \"0\")\" \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --volume /path/to/your/model:/path/to/your/model \\\n    --runtime ascend \\\n    gpustack/gpustack \\\n    --server-url http://your_gpustack_server_ip_or_hostname \\\n    --token your_gpustack_cluster_token\n</code></pre> <p>Note</p> <ul> <li>Replace the placeholder paths, IP address/hostname, and cluster token accordingly.</li> <li>Replace <code>/path/to/your/model</code> with the actual path on your system where the DeepSeek R1 model files are stored.</li> <li>Ensure the <code>hccn_tool</code> tool is installed and configured correctly on your system. This is required for discvoring the HCCN network communication.</li> </ul> <p>After all workers are added, return to the GPUStack UI.</p> <p>Navigate to the <code>Workers</code> page to verify that all workers are in the Ready state and their GPUs are listed.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#step-4-deploy-the-deepseek-r1-model","title":"Step 4: Deploy the DeepSeek R1 Model","text":"<ol> <li>Go to the <code>Deployments</code> page.</li> <li>Click <code>Deploy Model</code>.</li> <li>Select <code>Local Path</code> as your source.</li> <li>Enter a name (e.g., <code>DeepSeek-R1</code>) in the <code>Name</code> field.</li> <li>Specify the <code>Model Path</code> as the directory that contains the DeepSeek R1 model files on each worker node.</li> <li>Ensure the <code>Backend</code> is set to <code>Ascend MindIE</code>.</li> <li>Expand <code>Advanced</code>, append following parameters to <code>Backend Parameters</code>:    - <code>--data-parallel-size=4</code>    - <code>--tensor-parallel-size=8</code>    - <code>--moe-tensor-parallel-size=1</code>    - <code>--moe-expert-parallel-size=32</code>    - <code>--npu-memory-fraction=0.95</code>, since we are using Data Parallelism, the memory fraction should be set to 0.95 to ensure efficient memory usage across all NPUs.</li> <li>After passing the compatibility check, click <code>Save</code> to deploy.</li> </ol>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#step-5-monitor-deployment","title":"Step 5: Monitor Deployment","text":"<p>You can monitor the deployment status on the <code>Deployments</code> page. Hover over <code>distributed across workers</code> to view GPU and worker usage. Click <code>View Logs</code> to see real-time logs showing model loading progress. It may take a few minutes to load the model.</p> <p></p> <p>After the model is running, navigate to the <code>Workers</code> page to check GPU utilization. Ascend MindIE uses 95% of NPU memory with above settings.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-ascend-mindie/#step-6-run-inference-via-playground","title":"Step 6: Run Inference via Playground","text":"<p>Once the model is deployed and running, you can test it using the GPUStack Playground.</p> <ol> <li>Navigate to the <code>Playground</code> -&gt; <code>Chat</code>.</li> <li>If only one model is deployed, it will be selected by default. Otherwise, use the dropdown menu to choose <code>DeepSeek-R1</code>.</li> <li>Enter prompts and interact with the model.</li> </ol> <p></p> <p>You can also use the <code>Compare</code> tab to test concurrent inference scenarios.</p> <p></p> <p>You have now successfully deployed and run DeepSeek R1 671B using Distributed Ascend MindIE on a GPUStack cluster. Explore the model's performance and capabilities in your own applications.</p> <p>For further assistance, feel free to reach out to the GPUStack community or support team.</p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/","title":"Running DeepSeek R1 671B with Distributed vLLM","text":"<p>This tutorial guides you through the process of configuring and running the original DeepSeek R1 671B using Distributed vLLM on a GPUStack cluster. Due to the extremely large size of the model, distributed inference across multiple workers is usually required.</p> <p>GPUStack enables easy setup and orchestration of distributed inference using vLLM, making it possible to run massive models like DeepSeek R1 with minimal manual configuration.</p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure the following requirements are met:</p> <ul> <li>You have access to a sufficient number of Linux nodes, each equipped with the required GPUs. For example:</li> </ul> GPU Number of Nodes H100/H800:8 2 A100/A800-80GB:8 4 A100/A800:8 8 <ul> <li>High-speed interconnects such as NVLink or InfiniBand are recommended for optimal performance.</li> <li>Model files should be downloaded to the same path on each node. While GPUStack supports on-the-fly model downloading, pre-downloading is recommended as it can be time consuming depending on the network speed.</li> </ul> <p>Note</p> <ul> <li>In this tutorial, we assume a setup of 4 nodes, each equipped with 8 A800-80GB GPUs and connected via 200G InfiniBand.</li> <li>A100/A800 GPUs do not support the FP8 precision originally used by DeepSeek R1. Hence, we use the BF16 version from Unsloth.</li> </ul>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-1-install-gpustack-server","title":"Step 1: Install GPUStack Server","text":"<p>According to the NVIDIA Installation, you can use the following command to start the GPUStack server with the built-in worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --volume /path/to/your/model:/path/to/your/model \\\n    --runtime nvidia \\\n    gpustack/gpustack\n</code></pre> <p>Note</p> <ul> <li>Replace <code>/path/to/your/model</code> with the actual path.</li> </ul> <p>After GPUStack server is up and running, run the following commands to get the initial admin password:</p> <pre><code>sudo docker exec gpustack \\\n    cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-2-access-gpustack-ui","title":"Step 2: Access GPUStack UI","text":"<p>Login to the GPUStack UI using the <code>admin</code> user and the obtained password.</p> <pre><code>http://your_gpustack_server_ip_or_hostname\n</code></pre>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-3-install-gpustack-workers","title":"Step 3: Install GPUStack Workers","text":"<p>Navigate to the <code>Workers</code> page in the GPUStack UI, click <code>Add Worker</code> button to get the command for adding workers.</p> <p>And then on each worker node, run the worker adding command to start a GPUStack worker:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart unless-stopped \\\n    --privileged \\\n    --network host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --volume /path/to/your/model:/path/to/your/model \\\n    --runtime nvidia \\\n    gpustack/gpustack \\\n    --server-url http://your_gpustack_server_ip_or_hostname \\\n    --token your_gpustack_cluster_token\n</code></pre> <p>Note</p> <ul> <li>Replace the placeholder paths, IP address/hostname, and cluster token accordingly.</li> <li>Replace <code>/path/to/your/model</code> with the actual path on your system where the DeepSeek R1 model files are stored.</li> </ul> <p>After all workers are added, return to the GPUStack UI.</p> <p>Navigate to the <code>Workers</code> page to verify that all workers are in the Ready state and their GPUs are listed.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-4-deploy-the-deepseek-r1-model","title":"Step 4: Deploy the DeepSeek R1 Model","text":"<ol> <li>Go to the <code>Deployments</code> page.</li> <li>Click <code>Deploy Model</code>.</li> <li>Select <code>Local Path</code> as your source.</li> <li>Enter a name (e.g., <code>DeepSeek-R1</code>) in the <code>Name</code> field.</li> <li>Specify the <code>Model Path</code> as the directory that contains the DeepSeek R1 model files on each worker node.</li> <li>Ensure the <code>Backend</code> is set to <code>vLLM</code>.</li> <li>After passing the compatibility check, click <code>Save</code> to deploy.</li> </ol>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-5-monitor-deployment","title":"Step 5: Monitor Deployment","text":"<p>You can monitor the deployment status on the <code>Deployments</code> page. Hover over <code>distributed across workers</code> to view GPU and worker usage. Click <code>View Logs</code> to see real-time logs showing model loading progress. It may take a few minutes to load the model.</p> <p></p> <p>After the model is running, navigate to the <code>Workers</code> page to check GPU utilization. By default, vLLM uses 90% of GPU memory. You may adjust this in the model configuration settings.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-6-run-inference-via-playground","title":"Step 6: Run Inference via Playground","text":"<p>Once the model is deployed and running, you can test it using the GPUStack Playground.</p> <ol> <li>Navigate to the <code>Playground</code> -&gt; <code>Chat</code>.</li> <li>If only one model is deployed, it will be selected by default. Otherwise, use the dropdown menu to choose <code>DeepSeek-R1</code>.</li> <li>Enter prompts and interact with the model.</li> </ol> <p></p> <p>You can also use the <code>Compare</code> tab to test concurrent inference scenarios.</p> <p></p> <p>You have now successfully deployed and run DeepSeek R1 671B using Distributed vLLM on a GPUStack cluster. Explore the model\u2019s performance and capabilities in your own applications.</p> <p>For further assistance, feel free to reach out to the GPUStack community or support team.</p>"},{"location":"tutorials/using-custom-backends/","title":"Using Custom Inference Backends","text":"<p>This guide shows how to add a custom inference backend that is not built into GPUStack, using TensorRT-LLM as an example. Configuration examples for common inference backends are provided at the end of the article. For a description of each parameter, see the User Guide.</p>"},{"location":"tutorials/using-custom-backends/#core-steps","title":"Core Steps","text":"<ol> <li>Prepare the Docker image for the required inference backend.</li> <li>Understand the image's ENTRYPOINT or CMD to determine the inference backend startup command.</li> <li>Add configuration on the Inference Backend page.</li> <li>Deploy models on the Deployments page and select the newly added backend.</li> </ol>"},{"location":"tutorials/using-custom-backends/#example","title":"Example","text":"<p>The following uses TensorRT-LLM as an example to illustrate how to add and use an inference backend.</p> <p>These examples are functional demonstrations, not performance-optimized configurations. For better performance, consult each backend\u2019s official documentation for tuning.</p> <ol> <li>Find the required image from the release page linked from the TensorRT-LLM documentation.</li> <li>TensorRT-LLM images must launch the inference service using <code>trtllm-serve</code>; otherwise, they start an interactive shell session. The <code>run_command</code> supports placeholders such as <code>{{model_path}}</code> and <code>{{port}}</code> (and optionally <code>{{model_name}}</code>, <code>{{worker_ip}}</code>), which are automatically replaced with the actual values when the deployment is scheduled to a worker.</li> <li>Add configuration on the Inference Backend page; YAML import is supported. Example: <pre><code>backend_name: TensorRT-LLM-custom\ndefault_version: 1.2.0rc0\nversion_configs:\n  1.2.0rc0:\n    image_name: nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc0\n    run_command: 'trtllm-serve {{model_path}} --host 0.0.0.0 --port {{port}}'\n    custom_framework: cuda\n</code></pre></li> </ol> <p>Note</p> <p>Some inference backends are labeled as Built-in (e.g., vLLM, MindIE) on the Inference Backend page. These are GPUStack's built-in inference backends. When using built-in backends, appropriate container images matching the worker environment are automatically obtained based on the runtime. You can add custom versions to these built-in inference backends and specify the image names you need.</p> <ol> <li>On the Deployments page, select the newly added backend and deploy the model. </li> </ol> <p>Result</p> <p>After the inference backend service starts, you can see the model_instance status becomes RUNNING.  You can engage in conversations in the Playground. </p>"},{"location":"tutorials/using-custom-backends/#typical-examples","title":"Typical Examples","text":""},{"location":"tutorials/using-custom-backends/#deploy-gguf-models-with-llamacpp","title":"Deploy GGUF Models with llama.cpp","text":"<ol> <li>Find the image name in the documentation: <code>ghcr.io/ggml-org/llama.cpp:server</code> (ensure you select the variant that matches your worker platform).</li> <li>Add the following backend configuration on the Inference Backend page:     <pre><code>backend_name: llama.cpp-custom\ndefault_run_command: '-m {{model_path}} --host 0.0.0.0 --port {{port}}'\nversion_configs:\n  v1-cuda:\n    image_name: ghcr.io/ggml-org/llama.cpp:server-cuda\n    custom_framework: cuda\n  v1-cpu:\n    image_name: ghcr.io/ggml-org/llama.cpp:server\n    custom_framework: cpu\ndefault_version: v1-custom\n</code></pre></li> <li>On the Deployment page, locate a GGUF-format model, select <code>llama.cpp</code>, and deploy.</li> </ol> <p>For more information, refer to the llama.cpp GitHub repository.</p> <p>Screenshots:  </p>"},{"location":"tutorials/using-custom-backends/#use-kokoro-fastapi","title":"Use Kokoro-FastAPI","text":"<ol> <li>Find the image name in the documentation, and choose the variant that matches your worker platform:    - <code>ghcr.io/remsky/kokoro-fastapi-cpu:latest</code>    - <code>ghcr.io/remsky/kokoro-fastapi-gpu:latest</code></li> </ol> <p>Warning</p> <p>This image includes a built-in model, so the model you select on the Deployments page may be ignored. To avoid unexpected errors, choose a model consistent with the one bundled in the image. The kokoro-fastapi image uses the Kokoro-82M model.</p> <ol> <li>Add the following backend configuration on the Inference Backend page:    <pre><code>backend_name: kokoro-custom\nversion_configs:\n  v1:\n    image_name: ghcr.io/remsky/kokoro-fastapi-gpu:latest\n    custom_framework: cuda\ndefault_run_command: python -m uvicorn api.src.main:app --host 0.0.0.0 --port {{port}} --log-level debug\n</code></pre></li> <li>On the Deployments page, select the Kokoro-82M model, choose <code>kokoro</code> as the backend, and set <code>Name</code> to one of the supported keys (e.g., <code>kokoro</code>).</li> </ol> <p>Known Limitations for Name</p> <p>In kokoro-fastapi, the <code>model_name</code> is restricted to the keys below; other values will result in an \"unsupported\" error.</p> <pre><code>\"models\": {\n    \"tts-1\": \"kokoro-v1_0\",\n    \"tts-1-hd\": \"kokoro-v1_0\",\n    \"kokoro\": \"kokoro-v1_0\"\n}\n</code></pre> <p>Therefore, restrict the <code>Name</code> during deployment to one of these supported keys.</p> <p>Screenshots:</p> <p> </p>"},{"location":"user-guide/api-key-management/","title":"API Key Management","text":"<p>GPUStack supports authentication using API keys. Each GPUStack user can generate and manage their own API keys.</p>"},{"location":"user-guide/api-key-management/#create-api-key","title":"Create API Key","text":"<ol> <li>Hover over the user avatar and navigate to the <code>API Keys</code> page.</li> <li>Click the <code>Add API Key</code> button.</li> <li>Fill in the <code>Name</code>, <code>Description</code>, and select the <code>Expiration</code> of the API key.</li> <li>In the Model Access section, select either All models or Allowed models, and if choosing Allowed models, select which models this API key can access from the list.</li> <li>Click the <code>Save</code> button.</li> <li>Copy and store the key somewhere safe, then click the <code>Done</code> button.</li> </ol> <p>Note</p> <p>Please note that you can only see the generated API key once upon creation.</p>"},{"location":"user-guide/api-key-management/#edit-model-access","title":"Edit Model Access","text":"<ol> <li>Hover over the user avatar and navigate to the <code>API Keys</code> page.</li> <li>Find the API key you want to edit.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>In the Model Access section, select either All models or Allowed models, and if choosing Allowed models, select which models this API key can access from the list.</li> <li>Click the <code>Save</code> button.</li> </ol> <p>Note</p> <p>Changes will take effect within one minute.</p>"},{"location":"user-guide/api-key-management/#delete-api-key","title":"Delete API Key","text":"<ol> <li>Hover over the user avatar and navigate to the <code>API Keys</code> page.</li> <li>Find the API key you want to delete.</li> <li>Click the <code>Delete</code> button in the <code>Operations</code> column.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/api-key-management/#use-api-key","title":"Use API Key","text":"<p>GPUStack supports using the API key as a bearer token. The following is an example using curl:</p> <pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"qwen3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"user-guide/built-in-inference-backends/","title":"Built-in Inference Backends","text":"<p>GPUStack supports the following inference backends:</p> <ul> <li>vLLM</li> <li>SGLang</li> <li>MindIE</li> <li>VoxBox</li> </ul> <p>When initiating a model deployment, the UI will automatically pre-select a backend based on the following criteria:</p> <ul> <li>If the model is a known <code>Text-to-Speech</code> or <code>Speech-to-Text</code> model, <code>VoxBox</code> is used.</li> <li>If the model is a known <code>Image</code> model, <code>SGLang</code> is used.</li> <li>Otherwise, <code>vLLM</code> is used.</li> </ul> <p>This pre-selection only populates the default value in the deployment form. The actual backend used for deployment will be the one explicitly selected by the user when submitting the deployment request. Users can select from the list of supported backends provided above.</p> <p>Note</p> <p>For all supported inference backends, we have pre-built Docker images available at DockerHub. When users deploy models, the system will automatically pull and run the corresponding images.</p>"},{"location":"user-guide/built-in-inference-backends/#vllm","title":"vLLM","text":"<p>vLLM is a high-throughput and memory-efficient LLMs inference backend. It is a popular choice for running LLMs in production.</p> <p>vLLM seamlessly supports most state-of-the-art open-source models, including:</p> <ul> <li>Transformer-like LLMs (e.g., <code>Qwen3</code>)</li> <li>Mixture-of-Expert LLMs (e.g., <code>DeepSeek V3/R1</code>)</li> <li>Multi-modal LLMs (e.g., <code>Qwen3-VL</code>)</li> <li>Embedding Models (e.g. <code>Qwen3-Embedding</code>)</li> <li>Reranker Models (e.g. <code>Qwen3-Reranker</code>)</li> </ul> <p>By default, GPUStack estimates the VRAM requirement for the model instance based on the model's metadata.</p> <p>You can customize the parameters to fit your needs. The following vLLM parameters might be useful:</p> <ul> <li><code>--gpu-memory-utilization</code> (default: 0.9): The fraction of GPU memory to use for the model instance.</li> <li><code>--max-model-len</code>: Model context length. For large-context models, GPUStack automatically sets this parameter to <code>8192</code> to simplify model deployment, especially in resource constrained environments. You can customize this parameter to fit your needs.</li> <li><code>--tensor-parallel-size</code>: Number of tensor parallel replicas. By default, GPUStack sets this parameter given the GPU resources available and the estimation of the model's memory requirement. You can customize this parameter to fit your needs.</li> </ul> <p>For more details, please refer to vLLM CLI Reference.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-models","title":"Supported Models","text":"<p>Please refer to the vLLM documentation for supported models.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/built-in-inference-backends/#multimodal-language-models","title":"Multimodal Language Models","text":"<p>vLLM supports multimodal language models listed here.</p> <p>When users deploy a vision language model using the vLLM backend, image inputs are supported in the chat completion API.</p>"},{"location":"user-guide/built-in-inference-backends/#distributed-inference-across-workers-experimental","title":"Distributed Inference Across Workers (Experimental)","text":"<p>vLLM supports distributed inference across multiple workers using Ray. You can enable a Ray cluster in GPUStack by checking the <code>Allow Distributed Inference Across Workers</code> option when deploying a model. This allowing vLLM to run distributed inference across multiple workers.</p> <p>Known Limitations</p> <ol> <li>Model files must be accessible at the same path on all participating workers. You must either use a shared file system or download the model files to the same path on all participating workers.</li> <li>Each worker can only be assigned to one distributed vLLM model instance at a time.</li> </ol> <p>Auto-scheduling is supported with the following conditions:</p> <ul> <li>Participating workers have the same number of GPUs.</li> <li>All GPUs in the worker satisfy the gpu_memory_utilization(defaults to 0.9) requirement.</li> <li>With tensor parallelism, GPUs are divided based on the number of attention heads.</li> <li>The total VRAM claim is greater than the estimated VRAM claim.</li> </ul> <p>If the above conditions are not met, the model instance will not be scheduled automatically. However, you can manually schedule it by selecting the desired workers/GPUs in the model configuration.</p>"},{"location":"user-guide/built-in-inference-backends/#parameters-reference","title":"Parameters Reference","text":"<p>See the full list of supported parameters for vLLM here.</p>"},{"location":"user-guide/built-in-inference-backends/#sglang","title":"SGLang","text":"<p>SGLang is a high-performance serving framework for large language models and vision-language models.</p> <p>It is designed to deliver low-latency and high-throughput inference across a wide range of setups, from a single GPU to large distributed clusters.</p> <p>By default, GPUStack estimates the VRAM requirement for the model instance based on model metadata.</p> <p>When needed, GPUStack also sets several parameters automatically for large-context models. Common SGLang parameters include:</p> <ul> <li><code>--mem-fraction-static</code> (default: <code>0.9</code>): The per-GPU allocatable VRAM fraction. The scheduler uses this value for resource matching and candidate selection. You can override it via the model's <code>backend_parameters</code>.</li> <li><code>--context-length</code>: Model context length. For large-context models, if the automatically estimated context length exceeds device capability, GPUStack sets this parameter to <code>8192</code> to simplify deployment in resource-constrained environments. You can customize this parameter as needed.</li> <li><code>--tp-size</code>: Tensor parallel size. When not explicitly provided, GPUStack infers and sets this parameter based on the selected GPUs.</li> <li><code>--pp-size</code>: Pipeline parallel size. In multi-node deployments, GPUStack determines a combination of <code>--tp-size</code> and <code>--pp-size</code> according to the model and cluster configuration.</li> <li>Multi-node arguments: <code>--nnodes</code>, <code>--node-rank</code>, <code>--dist-init-addr</code>. When distributed inference is enabled, GPUStack injects these arguments to initialize multi-node communication.</li> </ul> <p>For more details, please refer to SGLang documentation.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-models_1","title":"Supported Models","text":"<p>Please refer to the SGLang documentation for supported models.</p> <p>SGLang also supports image models. The ones we have verified include: Qwen-Image, Flux 1 Dev. For more information, please refer to the SGLang Diffusion documentation.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-features_1","title":"Supported Features","text":""},{"location":"user-guide/built-in-inference-backends/#distributed-inference-across-workers-experimental_1","title":"Distributed Inference Across Workers (Experimental)","text":"<p>You can enable distributed SGLang inference across multiple workers in GPUStack.</p> <p>Known Limitations</p> <ol> <li>All participating nodes must run Linux, and images/environments should be compatible in Python version and communication libraries (e.g., NCCL).</li> <li>Model files must be accessible at the same path on all nodes. Use a shared file system or download the model to the same path on each node.</li> </ol> <p>Auto-scheduling candidate selection considers the following conditions:</p> <ul> <li>Participating GPUs satisfy the <code>--mem-fraction-static</code> requirement (default 0.9).</li> <li>In single-node multi-GPU or multi-node multi-GPU scenarios, the total effective allocatable VRAM must meet the model's requirement.</li> <li>Model parallelism requirements must be met (e.g., total number of attention heads divisible by the tensor parallel size), otherwise the candidate is rejected.</li> </ul> <p>If the above conditions are not met, you can still manually schedule the model instance by selecting workers/GPUs in the configuration, though overcommit risk may be indicated.</p>"},{"location":"user-guide/built-in-inference-backends/#diffusion-models-experimental","title":"Diffusion Models (Experimental)","text":"<p>SGLang Diffusion is a high-performance inference engine designed specifically for diffusion models, aiming to accelerate the generation process of images and videos. Through SGLang's parallel processing techniques and optimized kernels, it achieves up to 1.2x higher generation speed compared to mainstream baselines (e.g., Hugging Face Diffusers).</p> <p>For more details, please refer to SGLang Diffusion.</p>"},{"location":"user-guide/built-in-inference-backends/#other-advanced-features","title":"Other Advanced Features","text":"<p>Additional advanced features are available, such as:</p> <ul> <li>Speculative Decoding</li> <li>Hierarchical KV Caching</li> </ul> <p>Please refer to the official documentation for usage instructions.</p>"},{"location":"user-guide/built-in-inference-backends/#parameters-reference_1","title":"Parameters Reference","text":"<p>See the full list of supported parameters for SGLang here.</p>"},{"location":"user-guide/built-in-inference-backends/#mindie","title":"MindIE","text":"<p>MindIE is a high-performance inference service on Ascend hardware.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-models_2","title":"Supported Models","text":"<p>MindIE supports various models listed here.</p> <p>Within GPUStack, support large language models (LLMs) and multimodal language models (VLMs).</p> <p>However, embedding models and multimodal generation models are not supported yet.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-features_2","title":"Supported Features","text":"<p>MindIE owns a variety of features outlined here.</p> <p>At present, GPUStack supports a subset of these capabilities, including Quantization, Extending Context Size, Distributed Inference, Mixture of Experts(MoE), Split Fuse, Speculative Decoding, Multi-Token Prediction, Prefix Caching, Function Calling, Multimodal Understanding, Multi-head Latent Attention(MLA), Tensor Parallelism, Context Parallelism, Sequence Parallelism, Expert Parallelism, Data Parallelism, Buffer Response(Since Ascend MindIE 2.0.RC1). KV Pool(Since Ascend MindIE 2.2.RC1).</p> <p>Note</p> <ol> <li> <p>Quantization needs specific weight, and must adjust the model's <code>config.json</code>, please follow the reference(guide) to prepare the correct weight.</p> </li> <li> <p>Some features are mutually exclusive, so be careful when using them. For example, with Prefix Caching enabled, the Extending Context Size feature cannot be used.</p> </li> </ol>"},{"location":"user-guide/built-in-inference-backends/#parameters-reference_2","title":"Parameters Reference","text":"<p>MindIE has configurable parameters and environment variables.</p> <p>To avoid directly configuring JSON, GPUStack provides a set of command line parameters as below.</p> Parameter Default Range Scope Description <code>--log-level</code> Info Log Config Log level for MindIE. Options: <code>Verbose</code>, <code>Info</code>, <code>Warning</code>, <code>Warn</code>, <code>Error</code>, <code>Debug</code>. <code>--max-link-num</code> 1000 [1, 1000] Server Config Maximum number of parallel requests. <code>--token-timeout</code> 60 [1, 3600] Server Config Timeout for a token generation in seconds. <code>--e2e-timeout</code> 60 [1, 3600] Server Config E2E (from request accepted to inference stopped) timeout in seconds. <code>--kv-pool-config</code> Backend Config KV pool configuration in JSON format. For example: <code>{\"backend\":\"&lt;KV pool backend name&gt;\", \"configPath\":\"/path/to/your/config/file\"}</code>. <code>--max-seq-len</code> 8192 (0, *) Model Deploy Config Model context length. If unspecified, it will be derived from the model config. <code>--max-input-token-len</code> (0, <code>--max-seq-len</code>] Model Deploy Config Maximum input token length. If unspecified, it will be derived from <code>min(--max-seq-len, --max-prefill-tokens)</code>. <code>--truncation</code> Model Deploy Config Truncate the input token length when it exceeds the minimum of <code>--max-input-token-len</code> and <code>--max-seq-len</code> - 1. <code>--cpu-mem-size</code> 0 [0, *) Model Config CPU swap space size in GiB. Works when specified <code>--max-preempt-count</code>. <code>--npu-memory-fraction</code> 0.9 (0, 1] Model Config Fraction of NPU memory to be used for the model executor (0 to 1). For example: <code>0.5</code> means 50% memory utilization. <code>--trust-remote-code</code> Model Config Trust remote code (for model loading). <code>--models</code> Model Config Models configuration in JSON format, for certain specific configurations, like Expert Parallelism Implementation Method, Tensor Parallelism LM Header/Output Attention Split. <code>--async-scheduler-wait-time</code> 120 [1, 3600] Model Config The wait time (in seconds) for the asynchronous scheduler to start. <code>--cache-block-size</code> 128 Schedule Config KV cache block size. Must be a power of 2. <code>--max-prefill-batch-size</code> 50 [1, <code>--max-batch-size</code>] Schedule Config Maximum number of requests batched during prefill stage. Must be less than <code>--max-batch-size</code>. <code>--max-prefill-tokens</code> [1, <code>--max-seq-len</code>] Schedule Config During each prefill, the total number of all input tokens in the current batch cannot exceed <code>--max-prefill-tokens</code>. Default is same as <code>--max-seq-len</code>. <code>--prefill-time-ms-per-req</code> 150 [0, 1000] Schedule Config Estimated prefill time per request (ms). Used to decide between prefill and decode stage. <code>--prefill-policy-type</code> 0 Schedule Config Prefill stage strategy:  <code>0</code>: FCFS (First Come First Serve).  <code>1</code>: STATE (same as FCFS).  <code>2</code>: PRIORITY (priority queue).  <code>3</code>: MLFQ (Multi-Level Feedback Queue). <code>--max-batch-size</code> 200 [1, 5000] Schedule Config Maximum number of requests batched during decode stage. <code>--max-iter-times</code> [1, <code>--max-seq-len</code>] Schedule Config Maximum iterations for decoding stage. Default is same as <code>--max-seq-len</code>. <code>--decode-time-ms-per-req</code> 50 [0, 1000] Schedule Config Estimated decode time per request (ms). Used with <code>--prefill-time-ms-per-req</code> for batch selection. <code>--decode-policy-type</code> 0 Schedule Config Decode stage strategy:  <code>0</code>: FCFS  <code>1</code>: STATE (prioritize preempted or swapped requests)  <code>2</code>: PRIORITY  <code>3</code>: MLFQ <code>--max-preempt-count</code> 0 [0, <code>--max-batch-size</code>] Schedule Config Maximum number of preempted requests allowed during decoding. Must be less than <code>--max-batch-size</code>. <code>--support-select-batch</code> Schedule Config Enable batch selection. Determines execution priority based on <code>--prefill-time-ms-per-req</code> and <code>--decode-time-ms-per-req</code>. Use <code>--no-support-select-batch</code> to disable explicitly. <code>--max-queue-delay-microseconds</code> 5000 [500, 1000000] Schedule Config Maximum queue wait time in microseconds. <code>--max-first-token-wait-time</code> 2500 [0, 3600000] Schedule Config Maximum milliseconds to wait for the first token generation. <code>--override-generation-config</code> Overrides or sets generation config in JSON format. For example: <code>{\"temperature\": 0.5}</code>. This will merge into the <code>generation_config.json</code> of the model structure. <code>--enforce-eager</code> Emit operators in eager mode. <code>--no-metrics</code> Disable exposing metrics at <code>/metrics</code> endpoint. <code>--dtype</code> auto Data type for model weights and activations.  <code>auto</code>: use the default data type of the model config.  <code>half</code>/<code>float16</code>: for FP16.  <code>bfloat16</code>: for BF16.  <code>float</code>/<code>float32</code>: for FP32. <code>--rope-scaling</code> Extending Context Size RoPE scaling configuration in JSON format. For example: <code>{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}</code>. This will merge into the <code>config.json</code> of the model structure. <code>--rope-theta</code> Extending Context Size RoPE theta configuration. This will merge into the <code>config.json</code> of the model structure. <code>--enable-split</code> Split Fuse Enable split fuse, something like chunked prefill. Use <code>--no-enable-split</code> to disable explicitly. <code>--policy-type</code> 0 Split Fuse Strategy of split fuse.  <code>0</code>: FCFS, first come first serving.  <code>4</code>: SJF, shortest job first.  <code>5</code>: LJF, longest job first.  <code>6</code>: Skip-Join MLFQ, skip-Join multi-levels feedback queue.  <code>7</code>: SJF-MLFQ, shortest job first and multi-levels feedback queue. <code>--split-chunk-tokens</code> 512 [1, 8192] Split Fuse Tokens size to batch for split fuse. Multiples of 16. <code>--split-start-batch-size</code> 16 [0, <code>--max-batch-size</code>] Split Fuse Batch size to start splitting for split fuse. <code>--enable-memory-decoding</code> Speculative Decoding / Memory Decoding Enable memory decoding speculation. Use <code>--no-enable-memory-decoding</code> to disable explicitly. <code>--memory-decoding-length</code> 16 [1, 16] Speculative Decoding / Memory Decoding Length for memory decoding speculation. <code>--memory-decoding-dynamic-algo</code> Speculative Decoding / Memory Decoding Enable dynamic algorithm for memory decoding speculation. <code>--enable-lookahead</code> Speculative Decoding / Lookahead Enable lookahead speculation. Use <code>--no-enable-lookahead</code> to disable explicitly. <code>--lookahead-level</code> 4 [3, 16] Speculative Decoding / Lookahead Level for lookahead speculation. <code>--lookahead-window</code> 5 [1, 16] Speculative Decoding / Lookahead Window size for lookahead speculation. <code>--lookahead-guess-set-size</code> 5 [1, 16] Speculative Decoding / Lookahead Guess set size for lookahead speculation. <code>--enable-multi-token-prediction</code> Multi-Token Prediction Enable multi-token prediction. Use <code>--no-enable-multi-token-prediction</code> to disable explicitly. <code>--multi-token-prediction-tokens</code> 1 (0, *) Multi-Token Prediction Number of multi-token prediction tokens. This is only effective when <code>--enable-multi-token-prediction</code> is enabled. <code>--enable-prefix-caching</code> Prefix Caching Enable prefix caching. Use <code>--no-enable-prefix-caching</code> to disable explicitly. <code>--pipeline-parallel-size</code>, <code>-pp</code> 1 (0, *) Parallelism Number of pipeline parallel groups. <code>--data-parallel-size</code>, <code>-dp</code> -1 Parallelism Number of data parallel groups for Attention layers. <code>-1</code> means disabling data parallelism, otherwise, must be a power of 2. <code>--context-parallel-size</code>, <code>-cp</code> -1 Parallelism Number of context parallel groups for Attention layers. <code>-1</code> means disabling context parallelism, otherwise, must be a power of 2. <code>--tensor-parallel-size</code>, <code>-tp</code> -1 Parallelism Number of tensor parallel groups for Attention layers. <code>-1</code> means using world size as tensor parallel size, otherwise, must be a power of 2. <code>--sequence-parallel-size</code>, <code>-sp</code> -1 <code>--tensor-parallel-size</code> Parallelism Number of sequence parallel groups for MLP layers. <code>-1</code> means disabling sequence parallelism, otherwise, must be power of 2. <code>--moe-expert-parallel-size</code>, <code>-moe-ep</code> -1 Parallelism Number of expert parallel groups. <code>-1</code> means disabling MoE expert parallelism, otherwise, must be power of 2. <code>--moe-tensor-parallel-size</code>, <code>-moe-tp</code> -1 Parallelism Number of tensor parallel groups for MoE MLP layers. <code>-1</code> means using world size as MoE tensor parallel size, otherwise, must be power of 2. <code>--enable-buffer-response</code> Buffer Response Enable buffer response. Use <code>--no-enable-buffer-response</code> to disable explicitly. <code>--prefill-expected-time-ms</code> Buffer Response Expected latency (SLO) for Time to First Token (TTFT) in milliseconds. <code>--decode-expected-time-ms</code> Buffer Response Expected latency (SLO) for Time Per Output Token (TPOT) in milliseconds. <p>Note</p> <p>GPUStack allows users to inject custom environment variables during model deployment, however, some variables may be conflicted with GPUStack managment.</p> <p>Hence, GPUStack will override/prevent those variables. Please compare the model instance logs' output with your expectations.</p>"},{"location":"user-guide/built-in-inference-backends/#voxbox","title":"Voxbox","text":"<p>VoxBox is an inference engine designed for deploying Text-to-Speech and Speech-to-Text models. It also provides an API that is fully compatible with the OpenAI audio API.</p>"},{"location":"user-guide/built-in-inference-backends/#supported-models_3","title":"Supported Models","text":"Model Type Link Supported Platforms Faster-whisper-large-v3 Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-large-v2 Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-large-v1 Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-medium Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-medium.en Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-small Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-small.en Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-distil-whisper-large-v3 Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-distil-whisper-large-v2 Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-distil-whisper-medium.en Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-tiny Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 Faster-whisper-tiny.en Speech-to-Text Hugging Face, ModelScope AMD64,ARM64 CosyVoice-300M-Instruct Text-to-Speech Hugging Face, ModelScope AMD64 CosyVoice-300M-SFT Text-to-Speech Hugging Face, ModelScope AMD64 CosyVoice-300M Text-to-Speech Hugging Face, ModelScope AMD64 CosyVoice-300M-25Hz Text-to-Speech ModelScope AMD64 CosyVoice2-0.5B Text-to-Speech Hugging Face, ModelScope AMD64 Dia-1.6B Text-to-Speech Hugging Face, ModelScope AMD64"},{"location":"user-guide/built-in-inference-backends/#supported-features_3","title":"Supported Features","text":""},{"location":"user-guide/built-in-inference-backends/#allow-gpucpu-offloading","title":"Allow GPU/CPU Offloading","text":"<p>VoxBox supports deploying models to NVIDIA GPUs. If GPU resources are insufficient, it will automatically deploy the models to the CPU.</p>"},{"location":"user-guide/cloud-credential-management/","title":"Cloud Credential Management","text":"<p>GPUStack supports cloud credential management, allowing secure connections to external cloud providers. Cloud credentials contain provider information, keys, and options required for API access.</p>"},{"location":"user-guide/cloud-credential-management/#supported-providers","title":"Supported Providers","text":"<p>Only the <code>DigitalOcean</code> provider is supported for now.</p>"},{"location":"user-guide/cloud-credential-management/#create-cloud-credential","title":"Create Cloud Credential","text":"<ol> <li>Go to the <code>Cloud Credentials</code> page.</li> <li>Click the <code>Add Cloud Credential</code> dropdown and select <code>DigitalOcean</code>.</li> <li>Fill in the following information:</li> </ol> <ul> <li><code>Name</code>: Unique credential name.</li> <li><code>Access Token</code>: The API token generated on the DigitalOcean <code>Applications &amp; API</code> page.</li> <li><code>Description</code>: Additional information for the cloud credential.</li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/cloud-credential-management/#update-cloud-credential","title":"Update Cloud Credential","text":"<ol> <li>Go to the <code>Cloud Credentials</code> page.</li> <li>Find the credential you want to edit.</li> <li>Click the <code>Edit</code> button.</li> <li>Update the <code>Name</code>, <code>Access Token</code>, and <code>Description</code> as needed.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/cloud-credential-management/#delete-cloud-credential","title":"Delete Cloud Credential","text":"<ol> <li>Go to the <code>Cloud Credentials</code> page.</li> <li>Find the credential you want to delete.</li> <li>Click the ellipsis button in the operations column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/cluster-management/","title":"Cluster Management","text":"<p>GPUStack supports cluster-based worker management and provides multiple cluster types. You can provision a cluster through a <code>Cloud Provider</code> such as <code>DigitalOcean</code>, or create a self-hosted cluster and add workers using <code>Docker</code> run commands. Alternatively, you can register all nodes in a self-hosted <code>Kubernetes</code> cluster as GPUStack workers.</p>"},{"location":"user-guide/cluster-management/#create-cluster","title":"Create Cluster","text":"<ol> <li>Go to the <code>Clusters</code> page.</li> <li>Click the <code>Add Cluster</code> button.</li> <li>Select a cluster provider. There are <code>Docker</code> and <code>Kubernetes</code> for the <code>Self-Host</code> provider and <code>DigitalOcean</code> for the <code>Cloud Provider</code>.</li> <li>Depending on the provider, different options need to be set in the <code>Base Configuration</code> and <code>Add Worker</code> steps.</li> </ol>"},{"location":"user-guide/cluster-management/#create-docker-cluster","title":"Create Docker Cluster","text":"<ol> <li>In the <code>Basic Configuration</code> step, the <code>Name</code> field is required and <code>Description</code> is optional.</li> <li>Click <code>Save</code>.</li> <li>In the <code>Add Worker</code> step, some options and validations are needed before adding a worker via the <code>docker run</code> command.</li> <li><code>Select the GPU vendor</code>. Tested vendors include <code>Nvidia</code>, <code>AMD</code>, and <code>Ascend</code>. Experimental vendors include <code>Hygon</code>, <code>Moore Threads</code>, <code>Iluvatar</code>, <code>Cambricon</code>, and <code>Metax</code>. Click <code>Next</code> after selecting a vendor.</li> <li><code>Check Environment</code>. A shell command is provided to verify your environment is ready to add a worker. Copy the script and run it in your environment. Click <code>Next</code> after the script returns OK.</li> <li><code>Specify arguments</code> for the worker to be added. Provide the following arguments and click <code>Next</code>:</li> </ol> <ul> <li><code>Specify the worker IP</code>, or let the worker <code>Auto-detect the Worker IP</code>. Make sure the worker IP is accessible from the server.</li> <li>Specify a <code>Additional Volume Mount</code> for the worker container. The mount path can be used to reuse existing model files.</li> </ul> <ol> <li><code>Run command</code> to create and start the worker container. Copy the bash script and run it in your environment.</li> </ol> <p>The worker also can be added after the cluster is created.</p> <ol> <li>Go to <code>Clusters</code> page.</li> <li>Find the cluster which you want to add workers.</li> <li>Click the ellipsis button in the operations column, then select <code>Add Worker</code>.</li> <li>Select the options to add worker. Following the same steps as above, from <code>Select the GPU vendor</code> to <code>Run command</code>.</li> </ol>"},{"location":"user-guide/cluster-management/#register-kubernetes-cluster","title":"Register Kubernetes Cluster","text":"<ol> <li>In the <code>Basic Configuration</code> step, the <code>Name</code> field is required and <code>Description</code> is optional.</li> <li>Click <code>Save</code>.</li> <li><code>Select the GPU vendor</code>. Tested vendors include <code>Nvidia</code>, <code>AMD</code>, and <code>Ascend</code>. Experimental vendors include <code>Hygon</code>, <code>Moore Threads</code>, <code>Iluvatar</code>, <code>Cambricon</code>, and <code>Metax</code>. Click <code>Next</code> after selecting a vendor.</li> <li><code>Check environment</code>. A shell command is provided to verify that your environment is ready to add a worker. Copy the script and run it in your environment. Click <code>Next</code> after the script returns OK.</li> <li><code>Run command</code> to apply the worker manifests. Copy the bash script and run it in an environment where <code>kubectl</code> is installed and <code>kubeconfig</code> is configured.</li> </ol> <p>The kubernetes can be registerred after the cluster is created.</p> <ol> <li>Go to <code>Clusters</code> page.</li> <li>Find the cluster which you want to register the Kubernetes cluster.</li> <li>Click the ellipsis button in the operations column, then select <code>Register Cluster</code>.</li> <li>Select the options to register cluster. Following the same steps as abovve, from <code>Select the GPU vendor</code> to <code>Run command</code>.</li> </ol>"},{"location":"user-guide/cluster-management/#creating-digitalocean-cluster","title":"Creating DigitalOcean Cluster","text":"<ol> <li>In the <code>Basic Configuration</code> step, the <code>Name</code> field is required and <code>Description</code> is optional. Create or select a <code>Cloud Credential</code> to use to communicate to DigitalOcean API. Select the <code>Region</code> from the regions have GPU Droplet to create.</li> <li>Click <code>Next</code>.</li> <li>Adding one or more <code>Worker Pools</code>. For each pool, <code>Name</code>, <code>Instance Type</code>, <code>OS Image</code>, <code>Replicas</code>, <code>Batch Size</code>, <code>Labels</code> and <code>Volumes</code> can be specified.</li> <li>Click <code>Save</code> after the worker pools are configured.</li> </ol> <p>The worker poll can be added after the cluster is created.</p> <ol> <li>Go to <code>Clusters</code> page.</li> <li>Find the <code>DigitalOcean</code> cluster which you want to add worker pool.</li> <li>Click the ellipsis button in the operations column, then select <code>Add Worker Pool</code></li> <li>Adding new worker pool with options from Step 3 above.</li> </ol>"},{"location":"user-guide/cluster-management/#operating-worker-pools","title":"Operating Worker Pools","text":"<p>You can manage worker pools for DigitalOcean clusters on the <code>Clusters</code> page:</p> <ol> <li>Go to the <code>Clusters</code> page.</li> <li>Find the DigitalOcean cluster you want to manage and expand it to view its worker pools.</li> <li>To edit the replica count for a worker, modify it directly in the worker column.</li> <li>To edit a worker pool, click the <code>Edit</code> button and update the <code>Name</code>, <code>Replica</code>, <code>Batch Size</code>, and <code>Labels</code> as needed.</li> <li>To delete a worker pool, click the ellipsis button in the operations column for the worker pool, then select <code>Delete</code>.</li> </ol>"},{"location":"user-guide/cluster-management/#update-cluster","title":"Update Cluster","text":"<ol> <li>Go to the <code>Clusters</code> page.</li> <li>Find the cluster which you want to edit.</li> <li>Click the <code>Edit</code> button.</li> <li>Update the <code>Name</code>, and <code>Description</code> as needed.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/cluster-management/#delete-cluster","title":"Delete Cluster","text":"<ol> <li>Go to the <code>Clusters</code> page.</li> <li>Find the cluster which you want to delete.</li> <li>Click the ellipsis button in the operations column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> <li>You cannot delete a cluster if there are any models or workers still present in it.</li> </ol>"},{"location":"user-guide/compatibility-check/","title":"Compatibility Check","text":"<p>GPUStack performs a compatibility check prior to model deployment. This check provides detailed information about the model\u2019s compatibility with the current GPUStack environment. The following compatibility checks are performed:</p>"},{"location":"user-guide/compatibility-check/#inference-backend-compatibility","title":"Inference Backend Compatibility","text":"<p>Checks whether the selected inference backend is compatible with the current environment, including operating system, GPU, and architecture.</p>"},{"location":"user-guide/compatibility-check/#model-compatibility","title":"Model Compatibility","text":"<p>Determines whether the model is supported by the selected inference backend. This includes checking for supported model formats and architectures (e.g., <code>LlamaForCausalLM</code>, <code>Qwen3ForCausalLM</code>, etc.). This check is based on built-in inference backends and their supported models. If a custom backend or backend version is used, this check will be skipped.</p>"},{"location":"user-guide/compatibility-check/#schedulability-check","title":"Schedulability Check","text":"<p>Evaluates whether the model can be scheduled in the current environment. This includes verifying available resources such as RAM and VRAM, as well as configured scheduling rules.</p>"},{"location":"user-guide/compatibility-check/#scheduling-rules","title":"Scheduling Rules","text":"<p>Scheduling rules (including worker selectors, GPU selectors, and scheduling policies) are used to determine whether a model can be scheduled in the current environment.</p>"},{"location":"user-guide/compatibility-check/#resource-check","title":"Resource Check","text":"<p>The resource check ensures that sufficient system resources are available to deploy the model. GPUStack estimates the required resources and compares them with available resources in the environment. Estimations are performed using the following methods:</p> <ol> <li>For GGUF models: GPUStack uses the GGUF parser to estimate the model's resource requirements.</li> <li>For other models: GPUStack estimates VRAM usage using the following formula:</li> </ol> \\[ \\text{VRAM} = \\text{WEIGHT\\_SIZE} \\times 1.2 + \\text{FRAMEWORK\\_FOOTPRINT} \\] <ul> <li><code>WEIGHT_SIZE</code> refers to the size of the model weights in bytes.</li> <li><code>FRAMEWORK_FOOTPRINT</code> is a constant representing the framework\u2019s memory overhead. For example, vLLM may use several gigabytes of VRAM for CUDA graphs.</li> <li>The 1.2x multiplier is an empirical estimate. For more details, refer to this explanation.</li> </ul> <p>This formula provides a rough estimate and may not be accurate for all models. Typically, it reflects a lower-bound estimate of the required VRAM. If the estimation is insufficient, users can perform fine-grained scheduling by manually selecting workers and GPUs, or by adjusting advanced backend parameters. For instance, with vLLM, users can specify <code>--tensor-parallel-size</code> and <code>--pipeline-parallel-size</code> to control GPU allocation for the model.</p>"},{"location":"user-guide/image-generation-apis/","title":"Image Generation APIs","text":"<p>GPUStack provides APIs for generating images given a prompt and/or an input image when running diffusion models.</p> <p>Note</p> <p>For other APIs, GPUStack allows you to enable the Generic Proxy when deploying a model.</p> <p>With the Generic Proxy enabled, you can send API requests to a unified gateway and add the <code>X-GPUStack-Model</code> header.</p> <p>This header tells GPUStack which model should handle the request.</p> <p>For more details, see Enable Generic Proxy.</p>"},{"location":"user-guide/image-generation-apis/#supported-models","title":"Supported Models","text":"<p>The following models are available for image generation:</p> <ul> <li>black-forest-labs/FLUX.1-dev [Hugging Face], [ModelScope]</li> <li>Qwen/Qwen-Image [Hugging Face], [ModelScope]</li> </ul>"},{"location":"user-guide/image-generation-apis/#api-details","title":"API Details","text":"<p>The image generation APIs adhere to OpenAI API specification. While OpenAI APIs for image generation are simple and opinionated, GPUStack extends these capabilities with additional features.</p>"},{"location":"user-guide/image-generation-apis/#create-image","title":"Create Image","text":""},{"location":"user-guide/image-generation-apis/#streaming","title":"Streaming","text":"<p>This image generation API supports streaming responses to return the progressing of the generation. To enable streaming, set the <code>stream</code> parameter to <code>true</code> in the request body. Example:</p> <pre><code>REQUEST : (application/json)\n{\n  \"n\": 1,\n  \"response_format\": \"b64_json\",\n  \"size\": \"512x512\",\n  \"prompt\": \"A lovely cat\",\n  \"quality\": \"standard\",\n  \"stream\": true,\n  \"stream_options\": {\n    \"include_usage\": true, // return usage information\n  }\n}\n\nRESPONSE : (text/event-stream)\ndata: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\ndata: [DONE]\n</code></pre>"},{"location":"user-guide/image-generation-apis/#advanced-options","title":"Advanced Options","text":"<p>This image generation API supports additional options to control the generation process. The following options are available:</p> <pre><code>REQUEST : (application/json)\n{\n  \"n\": 1,\n  \"response_format\": \"b64_json\",\n  \"size\": \"512x512\",\n  \"prompt\": \"A lovely cat\",\n  \"sampler\": \"euler\",      // required, select from euler_a;euler;heun;dpm2;dpm++2s_a;dpm++2m;dpm++2mv2;ipndm;ipndm_v;lcm\n  \"schedule\": \"default\",   // optional, select from default;discrete;karras;exponential;ays;gits\n  \"seed\": null,            // optional, random seed\n  \"cfg_scale\": 4.5,        // optional, for sampler, the scale of classifier-free guidance in the output phase\n  \"sample_steps\": 20,      // optional, number of sample steps\n  \"negative_prompt\": \"\",   // optional, negative prompt\n  \"stream\": true,\n  \"stream_options\": {\n    \"include_usage\": true, // return usage information\n  }\n}\n\nRESPONSE : (text/event-stream)\ndata: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\ndata: [DONE]\n</code></pre>"},{"location":"user-guide/image-generation-apis/#create-image-edit","title":"Create Image Edit","text":""},{"location":"user-guide/image-generation-apis/#streaming_1","title":"Streaming","text":"<p>This image generation API supports streaming responses to return the progressing of the generation. To enable streaming, set the <code>stream</code> parameter to <code>true</code> in the request body. Example:</p> <pre><code>REQUEST: (multipart/form-data)\nn=1\nresponse_format=b64_json\nsize=512x512\nprompt=\"A lovely cat\"\nquality=standard\nimage=...                         // required\nmask=...                          // optional\nstream=true\nstream_options_include_usage=true // return usage information\n\nRESPONSE : (text/event-stream)\nCASE 1: correct input image\n  data: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\n  data: [DONE]\nCASE 2: illegal input image\n  error: {\"code\": 400, \"message\": \"Invalid image\", \"type\": \"invalid_request_error\"}\n</code></pre>"},{"location":"user-guide/image-generation-apis/#advanced-options_1","title":"Advanced Options","text":"<p>This image generation API supports additional options to control the generation process. The following options are available:</p> <pre><code>REQUEST: (multipart/form-data)\nn=1\nresponse_format=b64_json\nsize=512x512\nprompt=\"A lovely cat\"\nimage=...                         // required\nmask=...                          // optional\nsampler=euler                     // required, select from euler_a;euler;heun;dpm2;dpm++2s_a;dpm++2m;dpm++2mv2;ipndm;ipndm_v;lcm\nschedule=default                  // optional, select from default;discrete;karras;exponential;ays;gits\nseed=null                         // optional, random seed\ncfg_scale=4.5                     // optional, for sampler, the scale of classifier-free guidance in the output phase\nsample_steps=20                   // optional, number of sample steps\nnegative_prompt=\"\"                // optional, negative prompt\nstream=true\nstream_options_include_usage=true // return usage information\n\nRESPONSE : (text/event-stream)\nCASE 1: correct input image\n  data: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\n  data: [DONE]\nCASE 2: illegal input image\n  error: {\"code\": 400, \"message\": \"Invalid image\", \"type\": \"invalid_request_error\"}\n</code></pre>"},{"location":"user-guide/image-generation-apis/#usage","title":"Usage","text":"<p>The followings are examples using the image generation APIs:</p>"},{"location":"user-guide/image-generation-apis/#curl-create-image","title":"curl (Create Image)","text":"<pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/image/generate \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"n\": 1,\n        \"response_format\": \"b64_json\",\n        \"size\": \"512x512\",\n        \"prompt\": \"A lovely cat\",\n        \"quality\": \"standard\",\n        \"stream\": true,\n        \"stream_options\": {\n          \"include_usage\": true\n        }\n      }'\n</code></pre>"},{"location":"user-guide/image-generation-apis/#curl-create-image-edit","title":"curl (Create Image Edit)","text":"<pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/image/edit \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -F image=\"@otter.png\" \\\n    -F mask=\"@mask.png\" \\\n    -F prompt=\"A lovely cat\" \\\n    -F n=1 \\\n    -F size=\"512x512\"\n</code></pre>"},{"location":"user-guide/inference-backend-management/","title":"Inference Backend Management","text":"<p>GPUStack allows admins to configure inference backends and backend versions.</p> <p>This article serves as an operational guide for the Inference Backend page. For supported built-in backends and their capabilities, see Built-in Inference Backends.</p> <p>For guidelines for configuring custom backends and examples of custom backends that have been verified to work, see Custom Inference Backends.</p>"},{"location":"user-guide/inference-backend-management/#parameter-description","title":"Parameter Description","text":"Parameter Name Description Required Name Inference backend name Yes Health Check Path Health check path used to verify the backend is up and responding. Default: /v1/models (OpenAI-compatible). No Default Execution Command Container startup command/args. For example (vLLM): <code>vllm serve {{model_path}} --port {{port}} --served-model-name {{model_name}} --host {{worker_ip}}</code>. The placeholders <code>{{model_path}}</code>, <code>{{model_name}}</code>, <code>{{port}}</code>, and <code>{{worker_ip}}</code> are automatically substituted when the deployment is scheduled to a worker. No Default Backend Parameters Pre-populate the Advanced Backend Parameters section during deployment; you can adjust them before launching No Description Description No Version Configs Configure available versions of this backend Yes Default Version Preselected during deployment. If you don\u2019t choose a version, its image is used No <p>Version Configs parameter description:</p> Parameter Name Description Required Version Version name shown in the Backend Version dropdown during deployment Yes Image Name Container image name for the backend (e.g., <code>ghcr.io/org/image:tag</code>) Yes Framework(custom_framework) Backend framework (internal identifier: <code>custom_framework</code>). Deployment and scheduling are filtered by supported frameworks Yes Execution Command Version-specific startup command. If omitted, the Default Execution Command is used No"},{"location":"user-guide/inference-backend-management/#add-custom-inference-backend","title":"Add Custom Inference Backend","text":"<ol> <li>Click the \"Add Backend\" button in the top-right corner.</li> <li>You can add a custom inference backend by completing the form or by pasting a YAML definition. Refer to the parameter descriptions above for field meanings.</li> <li>The backend name cannot be modified after creation. Custom backend names must end with \"-custom\" (pre-filled in the form).</li> <li>Click \"Save\" to submit.</li> </ol>"},{"location":"user-guide/inference-backend-management/#edit-inference-backend-or-add-custom-version","title":"Edit Inference Backend or Add Custom Version","text":"<ol> <li>On the Inference Backend page, locate the target backend. From the card's top-right dropdown menu, choose \"Edit\".</li> <li>Modify backend properties (the name cannot be changed), or add a new version.</li> <li>For built-in backends, custom versions must end with \"-custom\" (pre-filled in the form).</li> <li>Click \"Save\" to submit.</li> </ol>"},{"location":"user-guide/inference-backend-management/#delete-custom-inference-backend","title":"Delete Custom Inference Backend","text":"<ol> <li>On the Inference Backend page, locate the target backend and select \"Delete\" from the card's top-right dropdown menu.</li> <li>Built-in backends cannot be deleted.</li> <li>Click \"Delete\" in the confirmation dialog.</li> </ol>"},{"location":"user-guide/inference-backend-management/#list-versions-of-inference-backend","title":"List Versions of Inference Backend","text":"<p>On the Inference Backend page, click anywhere on the backend card (except the action buttons) to open a modal where you can browse all built-in and custom-added versions.</p>"},{"location":"user-guide/inference-backend-management/#flexible-testing-deployment","title":"Flexible Testing Deployment","text":"<p>Use this mode to quickly verify or tweak the image and startup command without editing the backend definition.</p> <ol> <li>Navigate to the Deployments page, click the \"Deploy Model\" button, and choose any model source.</li> <li>In the Basic tab, open the \"Backend\" dropdown and select \"Custom\" under the \"Built-in\" section.</li> <li>Two fields appear: <code>image_name</code> and <code>run_command</code>. These override the backend configuration for this deployment only.</li> <li>Review the remaining required settings and submit the deployment.</li> </ol>"},{"location":"user-guide/model-catalog/","title":"Model Catalog","text":"<p>The Model Catalog is an index of GPUStack-tuned models.</p>"},{"location":"user-guide/model-catalog/#browse-models","title":"Browse Models","text":"<p>You can browse the Model Catalog by navigating to the <code>Catalog</code> page. You can filter models by name and category. The following screenshot shows the Model Catalog page:</p> <p></p>"},{"location":"user-guide/model-catalog/#deploy-a-model-from-the-catalog","title":"Deploy a Model from the Catalog","text":"<p>You can deploy a model from the Model Catalog by clicking the model card. A model deployment configuration page will appear. You can review and customize the deployment configuration and click the <code>Save</code> button to deploy the model.</p>"},{"location":"user-guide/model-catalog/#customize-model-catalog","title":"Customize Model Catalog","text":"<p>You can customize the Model Catalog by providing a YAML file via GPUStack server configuration using the <code>--model-catalog-file</code> flag. It accepts either a local file path or a URL. You can refer to the built-in model catalog file here for the schema.</p> <p>The following is an example of a custom model catalog YAML file:</p> <pre><code>draft_models:\n- name: Qwen3-8B-EAGLE3\n  algorithm: eagle3\n  source: huggingface\n  huggingface_repo_id: Tengyunw/qwen3_8b_eagle3\nmodel_sets:\n- name: Deepseek R1 0528\n  description: DeepSeek-R1-0528 is a minor version of the DeepSeek R1 model that features enhanced reasoning depth and inference capabilities. These improvements are achieved through increased computational resources and algorithmic optimizations applied during post-training. The model delivers strong performance across a range of benchmark evaluations, including mathematics, programming, and general logic, with overall capabilities approaching those of leading models such as O3 and Gemini 2.5 Pro.\n  home: https://www.deepseek.com\n  icon: /static/catalog_icons/deepseek.png\n  categories:\n    - llm\n  capabilities:\n    - context/128K\n  size: 671\n  licenses:\n    - mit\n  release_date: \"2025-05-28\"\n  specs:\n    - mode: throughput\n      quantization: FP8\n      gpu_filters:\n        vendor: nvidia\n        compute_capability: \"&gt;=9.0\" # Hopper or later\n      source: huggingface\n      huggingface_repo_id: deepseek-ai/DeepSeek-R1-0528\n      backend: SGLang\n      backend_parameters:\n        - --enable-dp-attention\n        - --context-length=32768\n    - mode: standard\n      quantization: FP8\n      source: huggingface\n      huggingface_repo_id: deepseek-ai/DeepSeek-R1-0528\n      backend: vLLM\n      backend_parameters:\n        - --max-model-len=32768\n</code></pre>"},{"location":"user-guide/model-catalog/#using-model-catalog-in-air-gapped-environments","title":"Using Model Catalog in Air-Gapped Environments","text":"<p>The built-in model catalog sources models from either Hugging Face or ModelScope. If you are using GPUStack in an air-gapped environment without internet access, you can customize the model catalog to use a local-path model source. Here is an example:</p> <pre><code>model_sets:\n- name: Deepseek R1 0528\n  description: DeepSeek-R1-0528 is a minor version of the DeepSeek R1 model that features enhanced reasoning depth and inference capabilities. These improvements are achieved through increased computational resources and algorithmic optimizations applied during post-training. The model delivers strong performance across a range of benchmark evaluations, including mathematics, programming, and general logic, with overall capabilities approaching those of leading models such as O3 and Gemini 2.5 Pro.\n  home: https://www.deepseek.com\n  icon: /static/catalog_icons/deepseek.png\n  categories:\n    - llm\n  capabilities:\n    - context/128K\n  size: 671\n  licenses:\n    - mit\n  release_date: \"2025-05-28\"\n  specs:\n    - mode: throughput\n      quantization: FP8\n      gpu_filters:\n        vendor: nvidia\n        compute_capability: \"&gt;=9.0\" # Hopper or later\n      source: local_path\n      local_path: /path/to/DeepSeek-R1-0528\n      backend: SGLang\n      backend_parameters:\n        - --enable-dp-attention\n        - --context-length=32768\n    - mode: standard\n      quantization: FP8\n      source: local_path\n      # assuming you have /path/to/DeepSeek-R1-0528 directory\n      local_path: /path/to/DeepSeek-R1-0528\n      backend: vLLM\n      backend_parameters:\n        - --max-model-len=32768\n</code></pre>"},{"location":"user-guide/model-catalog/#model-catalog-schema","title":"Model Catalog Schema","text":"<p>The Model Catalog YAML file contains two main sections: <code>draft_models</code> and <code>model_sets</code>.</p> <ul> <li><code>draft_models</code>: A list of draft models for speculative decoding.</li> <li><code>model_sets</code>: A list of model sets that are tested and optimized.</li> </ul> <p>Each draft model has the following fields:</p> Field Type Description name string The name of the draft model. algorithm string The speculative decoding algorithm of the model. Currently, only eagle3 is supported. source string The source of the model (e.g., huggingface, model_scope). huggingface_repo_id string The Hugging Face repository ID of the model (if source is huggingface). model_scope_model_id string The ModelScope repository ID of the model (if source is model_scope). <p>Each model set has the following fields:</p> Field Type Description name string The name of the model. description string A brief description of the model. home string The homepage URL of the model. icon string The icon URL of the model. categories list of str A list of categories that the model belongs to. capabilities list of str A list of capabilities of the model. size int The size of the model in billions of parameters. licenses list of str A list of licenses of the model. release_date string The release date of the model in YYYY-MM-DD format. specs list of spec A list of deployment specifications for the model. <p>Each deployment spec has the following fields:</p> Field Type Description mode string GPUStack provides both conventional and optimized modes for different use cases, including throughput, latency, and standard scenarios. Users can also define custom modes as needed. quantization string The quantization type (e.g., FP16, FP8, INT8). gpu_filters dict GPU filters to specify compatible GPUs. <p>Other fields in a deployment spec are similar to the models API fields. For more details, see the API Reference documentation.</p>"},{"location":"user-guide/model-deployment-management/","title":"Model Deployment Management","text":"<p>You can manage model deployments in GPUStack by navigating to the <code>Models - Deployments</code> page. A model deployment in GPUStack contains one or multiple replicas of model instances. On deployment, GPUStack automatically computes resource requirements for the model instances from model metadata and schedules them to available workers accordingly.</p>"},{"location":"user-guide/model-deployment-management/#deploy-model","title":"Deploy Model","text":"<p>Currently, models from Hugging Face, ModelScope, and local paths are supported.</p>"},{"location":"user-guide/model-deployment-management/#deploying-a-hugging-face-model","title":"Deploying a Hugging Face Model","text":"<ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</p> </li> <li> <p>Search the model by name from <code>Hugging Face</code> using the search bar in the top left. For example, <code>Qwen/Qwen3-0.6B</code>.</p> </li> <li> <p>Adjust the <code>Name</code>, <code>Cluster</code>, <code>Backend</code>, <code>Backend Version</code>, and <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Performance</code> section for performance configurations if needed. Please refer to the Performance-Related Configuration section for more details.</p> </li> <li> <p>Expand the <code>Scheduling</code> section for scheduling configurations if needed. Please refer to the Scheduling Configuration section for more details.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-deployment-management/#deploying-a-modelscope-model","title":"Deploying a ModelScope Model","text":"<ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>ModelScope</code> in the dropdown.</p> </li> <li> <p>Search the model by name from <code>ModelScope</code> using the search bar in the top left. For example, <code>Qwen/Qwen3-0.6B</code>.</p> </li> <li> <p>Adjust the <code>Name</code>, <code>Cluster</code>, <code>Backend</code>, <code>Backend Version</code>, and <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Performance</code> section for performance configurations if needed. Please refer to the Performance-Related Configuration section for more details.</p> </li> <li> <p>Expand the <code>Scheduling</code> section for scheduling configurations if needed. Please refer to the Scheduling Configuration section for more details.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-deployment-management/#deploying-a-local-path-model","title":"Deploying a Local Path Model","text":"<p>You can deploy a model from a local path. The model path can be a directory (e.g., a downloaded Hugging Face model directory) or a file (e.g., a GGUF model file) located on workers. This is useful when running in an air-gapped environment.</p> <p>Note</p> <ol> <li>GPUStack does not check the validity of the model path for scheduling, which may lead to deployment failure if the model path is inaccessible. It is recommended to ensure the model path is accessible on all workers(e.g., using NFS, rsync, etc.). You can also use the worker selector configuration to deploy the model to specific workers.</li> <li>GPUStack cannot evaluate the model's resource requirements unless the server has access to the same model path. Consequently, you may observe empty VRAM/RAM allocations for a deployed model. To mitigate this, it is recommended to make the model files available on the same path on the server. Alternatively, you can customize backend parameters, such as <code>tensor-split</code>, to configure how the model is distributed across the GPUs.</li> </ol> <p>To deploy a local path model:</p> <ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>Local Path</code> in the dropdown.</p> </li> <li> <p>Fill in the <code>Name</code> of the deployment.</p> </li> <li> <p>Fill in the <code>Model Path</code>.</p> </li> <li> <p>Adjust the <code>Cluster</code>, <code>Backend</code>, <code>Backend Version</code>, and <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Performance</code> section for performance configurations if needed. Please refer to the Performance-Related Configuration section for more details.</p> </li> <li> <p>Expand the <code>Scheduling</code> section for scheduling configurations if needed. Please refer to the Scheduling Configuration section for more details.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-deployment-management/#backend","title":"Backend","text":"<p>Currently, GPUStack supports some built-in backends: vLLM, SGLang, MindIE and VoxBox.</p> <p>For more details, please refer to the Inference Backends section.</p>"},{"location":"user-guide/model-deployment-management/#backend-version","title":"Backend Version","text":"<p>Select a backend version. The version availability depend on the selected backend. This option is useful for ensuring compatibility or taking advantage of features introduced in specific backend versions.</p>"},{"location":"user-guide/model-deployment-management/#edit-model-deployment","title":"Edit Model Deployment","text":"<ol> <li>Find the model deployment you want to edit on the deployment list page.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>Update the attributes as needed. For example, change the <code>Replicas</code> to scale up or down.</li> <li>Click the <code>Save</code> button.</li> </ol> <p>Note</p> <p>After editing the model deployment, the configuration will not be applied to existing model instances. You need to delete the existing model instances. GPUStack will recreate new instances based on the updated model configuration.</p>"},{"location":"user-guide/model-deployment-management/#stop-model-deployment","title":"Stop Model Deployment","text":"<p>Stopping a model deployment will delete all model instances and release the resources. It is equivalent to scaling down the model to zero replicas.</p> <ol> <li>Find the model deployment you want to stop on the deployment list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Stop</code>.</li> <li>Confirm the operation.</li> </ol>"},{"location":"user-guide/model-deployment-management/#start-model-deployment","title":"Start Model Deployment","text":"<p>Starting a model deployment is equivalent to scaling up the model to one replica.</p> <ol> <li>Find the model deployment you want to start on the deployment list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Start</code>.</li> </ol>"},{"location":"user-guide/model-deployment-management/#delete-model-deployment","title":"Delete Model Deployment","text":"<ol> <li>Find the model deployment you want to delete on the deployment list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/model-deployment-management/#view-model-instance","title":"View Model Instance","text":"<ol> <li>Find the model deployment you want to check on the deployment list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the deployment.</li> </ol>"},{"location":"user-guide/model-deployment-management/#delete-model-instance","title":"Delete Model Instance","text":"<ol> <li>Find the model deployment you want to check on the deployment list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the deployment.</li> <li>Find the model instance you want to delete.</li> <li>Click the ellipsis button for the model instance in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol> <p>Note</p> <p>After a model instance is deleted, GPUStack will recreate a new instance to satisfy the expected replicas of the deployment if necessary.</p>"},{"location":"user-guide/model-deployment-management/#view-model-instance-logs","title":"View Model Instance Logs","text":"<ol> <li>Find the model deployment you want to check on the deployment list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the deployment.</li> <li>Find the model instance you want to check.</li> <li>Click the <code>View Logs</code> button for the model instance in the <code>Operations</code> column.</li> </ol>"},{"location":"user-guide/model-deployment-management/#performance-related-configuration","title":"Performance-Related Configuration","text":"<p>GPUStack provides the following configuration options to optimize model inference performance.</p>"},{"location":"user-guide/model-deployment-management/#extended-kv-cache","title":"Extended KV Cache","text":"<p>You can enable extended KV cache to offload the KV cache to CPU memory or remote storage. This feature is particularly useful for setups with limited GPU memory requiring long context lengths. Under the hood, GPUStack leverages LMCache to provide this functionality.</p> <p>Available options:</p> <ul> <li>RAM-to-VRAM Ratio: The ratio of system RAM to GPU VRAM used for KV cache. For example, 2.0 means the cache in RAM can be twice as large as the GPU VRAM.</li> <li>Maximum RAM Size: The maximum size of the KV cache stored in system memory (GiB). If set, this value overrides <code>RAM-to-VRAM Ratio</code>.</li> <li>Size of Cache Chunks: Number of tokens per KV cache chunk.</li> </ul> <p>This feature works for certain backends and frameworks only.</p>"},{"location":"user-guide/model-deployment-management/#compatibility-matrix","title":"Compatibility Matrix","text":"Backend Framework vLLM CUDA, ROCm SGLang CUDA, ROCm"},{"location":"user-guide/model-deployment-management/#scheduling-configuration","title":"Scheduling Configuration","text":""},{"location":"user-guide/model-deployment-management/#schedule-mode","title":"Schedule Mode","text":""},{"location":"user-guide/model-deployment-management/#auto","title":"Auto","text":"<p>GPUStack automatically schedules model instances to appropriate GPUs/Workers based on current resource availability.</p> <ul> <li>Placement Strategy</li> </ul> <p>Spread: Make the resources of the entire cluster relatively evenly distributed among all workers. It may produce more resource fragmentation on a single worker.</p> <p>Binpack: Prioritize the overall utilization of cluster resources, reducing resource fragmentation on Workers/GPUs.</p> <ul> <li>Worker Selector</li> </ul> <p>When configured, the scheduler will deploy the model instance to the worker containing specified labels.</p> <ol> <li> <p>Navigate to the <code>Workers</code> page and edit the desired worker. Assign custom labels to the worker by adding them in the labels section.</p> </li> <li> <p>Go to the <code>Deployments</code> page and click on the <code>Deploy Model</code> button. Expand the <code>Scheduling</code> section and input the previously assigned worker labels in the <code>Worker Selector</code> configuration. During deployment, the Model Instance will be allocated to the corresponding worker based on these labels.</p> </li> </ol>"},{"location":"user-guide/model-deployment-management/#manual","title":"Manual","text":"<p>This schedule type allows users to specify which GPU to deploy the model instance on.</p> <ul> <li>GPU Selector</li> </ul> <p>Select one or more GPUs from the list. The model instance will attempt to deploy to the selected GPU if resources permit.</p> <ul> <li>GPUs per Replica</li> </ul> <p>Auto: The system automatically calculates the GPU count per replica, using powers of two by default and capped by the selected GPUs.</p> <p>Manual: Select the number of GPUs each replica should use from the dropdown.</p>"},{"location":"user-guide/model-deployment-management/#advanced-configuration","title":"Advanced Configuration","text":"<p>GPUStack supports tailored configurations for model deployment.</p>"},{"location":"user-guide/model-deployment-management/#model-category","title":"Model Category","text":"<p>The model category helps you organize and filter models. By default, GPUStack automatically detects the model category based on the model's metadata. You can also customize the category by selecting it from the dropdown list.</p>"},{"location":"user-guide/model-deployment-management/#backend-parameters","title":"Backend Parameters","text":"<p>Input the parameters for the backend you want to customize when running the model. The parameter should be in the format <code>--parameter=value</code>, <code>--bool-parameter</code> or as separate fields for <code>--parameter</code> and <code>value</code>. For example, use <code>--max-model-length=8192</code> for vLLM.</p> <p>For full list of supported parameters, please refer to the Inference Backends section.</p>"},{"location":"user-guide/model-deployment-management/#environment-variables","title":"Environment Variables","text":"<p>Environment variables used when running the model. These variables are passed to the backend process at startup.</p>"},{"location":"user-guide/model-deployment-management/#allow-cpu-offloading","title":"Allow CPU Offloading","text":"<p>Note</p> <p>Available for custom backends only.</p> <p>When CPU offloading is enabled, GPUStack will allocate CPU memory if GPU resources are insufficient. You must correctly configure the inference backend to use hybrid CPU+GPU or full CPU inference.</p>"},{"location":"user-guide/model-deployment-management/#allow-distributed-inference-across-workers","title":"Allow Distributed Inference Across Workers","text":"<p>Note</p> <p>Available for vLLM, SGLang, and MindIE backends.</p> <p>Enable distributed inference across multiple workers. The primary Model Instance will communicate with backend instances on one or more other workers, offloading computation tasks to them.</p>"},{"location":"user-guide/model-deployment-management/#auto-restrat-on-error","title":"Auto-Restrat on Error","text":"<p>Enable automatic restart of the model instance if it encounters an error. This feature ensures high availability and reliability of the model instance. If an error occurs, GPUStack will automatically attempt to restart the model instance using an exponential backoff strategy. The delay between restart attempts increases exponentially, up to a maximum interval of 5 minutes. This approach prevents the system from being overwhelmed by frequent restarts in the case of persistent errors.</p>"},{"location":"user-guide/model-deployment-management/#enable-generic-proxy","title":"Enable Generic Proxy","text":"<p>While it is common practice to integrate with the OpenAI compatible APIs, users may have different requirements for their use cases. GPUStack supports any inference APIs other than the OpenAI-compatible ones and make it more flexible for AI application development.</p> <p>Below is an example of how to use the generic proxy with curl to access model instances:</p> <pre><code>curl http://&lt;server-url&gt;/model/proxy/embed \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;GPUSTACK_API_KEY&gt;\" \\\n  -H \"X-GPUStack-Model: bge-m3\" \\\n  -d '{\"inputs\":[\"What is Deep Learning?\", \"Deep Learning is not...\"]}'\n</code></pre> <p>When using the generic proxy endpoint, the path prefix <code>/model/proxy</code> will be removed before forwarding the request. You must provide either the <code>X-GPUStack-Model</code> header or the <code>model</code> attribute in the JSON body. On the model inference server, the request will look like:</p> <pre><code>curl http://&lt;inference-server-url&gt;/embed \\\n  -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-GPUStack-Model: bge-m3\" \\\n  -d '{\"inputs\":[\"What is Deep Learning?\", \"Deep Learning is not...\"]}'\n</code></pre>"},{"location":"user-guide/model-file-management/","title":"Model File Management","text":"<p>GPUStack allows admins to download and manage model files.</p>"},{"location":"user-guide/model-file-management/#add-model-file","title":"Add Model File","text":"<p>GPUStack currently supports models from Hugging Face, ModelScope, and local paths. To add model files, navigate to the <code>Model Files</code> page.</p>"},{"location":"user-guide/model-file-management/#add-a-hugging-face-model","title":"Add a Hugging Face Model","text":"<ol> <li>Click the <code>Add Model File</code> button and select <code>Hugging Face</code> from the dropdown.</li> <li>Use the search bar in the top left to find a model by name, e.g., <code>Qwen/Qwen3-0.6B</code>.</li> <li>(Optional) For GGUF models, select the desired quantization format from <code>Available Files</code>.</li> <li>Select the target worker to download the model file.</li> <li>(Optional) Specify a <code>Local Directory</code> to download the model to a custom path instead of the GPUStack cache directory.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#add-a-modelscope-model","title":"Add a ModelScope Model","text":"<ol> <li>Click the <code>Add Model File</code> button and select <code>ModelScope</code> from the dropdown.</li> <li>Use the search bar in the top left to find a model by name, e.g., <code>Qwen/Qwen3-0.6B</code>.</li> <li>(Optional) For GGUF models, select the desired quantization format from <code>Available Files</code>.</li> <li>Select the target worker to download the model file.</li> <li>(Optional) Specify a <code>Local Directory</code> to download the model to a custom path instead of the GPUStack cache directory.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#add-a-local-path-model","title":"Add a Local Path Model","text":"<p>You can add models from a local path. The path can be a directory (e.g., a Hugging Face model folder) or a file (e.g., a GGUF model) located on the worker.</p> <ol> <li>Click the <code>Add Model File</code> button and select <code>Local Path</code> from the dropdown.</li> <li>Enter the <code>Model Path</code>.</li> <li>Select the target worker.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#retry-download","title":"Retry Download","text":"<p>If a model file download fails \u2014 or gets stuck at a very low download speed \u2014 you can retry it:</p> <ol> <li>Navigate to the <code>Model Files</code> page.</li> <li>Locate the model file.</li> <li>Click the ellipsis button in the <code>Operations</code> column and select <code>Retry Download</code>.</li> <li>GPUStack will attempt to download the model file again from the specified source.</li> </ol>"},{"location":"user-guide/model-file-management/#deploy-model","title":"Deploy Model","text":"<p>Models can be deployed from model files. Since the model is stored on a specific worker, GPUStack will add a worker selector using the <code>worker-name</code> key to ensure proper scheduling.</p> <p>Tip</p> <p>If you want a model to fail over across nodes, make sure all nodes in the cluster can access the model files from the same path, and manually remove the <code>worker-name</code> label from the worker selector.</p> <ol> <li>Navigate to the <code>Model Files</code> page.</li> <li>Find the model file you want to deploy.</li> <li>Click the <code>Deploy</code> button in the <code>Operations</code> column.</li> <li>Review or adjust the <code>Name</code>, <code>Backend</code>, <code>Backend Version</code>, <code>Replicas</code>, and other deployment parameters.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#delete-model-file","title":"Delete Model File","text":"<ol> <li>Navigate to the <code>Model Files</code> page.</li> <li>Find the model file you want to delete.</li> <li>Click the ellipsis button in the <code>Operations</code> column and select <code>Delete</code>.</li> <li>(Optional) Check the <code>Also delete the file from disk</code> option.</li> <li>Click the <code>Delete</code> button to confirm.</li> </ol>"},{"location":"user-guide/observability/","title":"Observability","text":"<p>This document describes how to monitor GPUStack Server/Worker/LLM serving runtime metrics using Prometheus and Grafana.</p>"},{"location":"user-guide/observability/#overview","title":"Overview","text":"<p>GPUStack provides a comprehensive set of metrics for model serving and GPU resource management. By integrating Prometheus and Grafana, users can collect, store, and visualize these metrics in real time, enabling efficient monitoring and troubleshooting.</p>"},{"location":"user-guide/observability/#metrics-exposed-by-gpustack","title":"Metrics Exposed by GPUStack","text":"<p>The following metrics are exposed by GPUStack and can be scraped by Prometheus. Each metric includes hierarchical labels for cluster, worker, model, and instance identification.</p>"},{"location":"user-guide/observability/#llm-serving-runtime-metrics","title":"LLM Serving Runtime Metrics","text":"Metric Name Type Description gpustack:num_requests_running Gauge Number of requests currently being processed. gpustack:num_requests_waiting Gauge Number of requests waiting in the queue. gpustack:num_requests_swapped Gauge Number of requests swapped out to CPU. gpustack:prefix_cache_hit_rate Gauge Prefix cache hit rate. gpustack:kv_cache_usage_ratio Gauge KV-cache usage ratio. 1.0 means fully used. gpustack:prefix_cache_queries Counter Number of prefix cache queries (total tokens). gpustack:prefix_cache_hits Counter Number of prefix cache hits (total tokens). gpustack:prompt_tokens Counter Total number of prefill tokens processed. gpustack:generation_tokens Counter Total number of generated tokens. gpustack:request_prompt_tokens Histogram Number of prefill tokens processed per request. gpustack:request_generation_tokens Histogram Number of generation tokens processed per request. gpustack:time_to_first_token_seconds Histogram Time to generate first token. gpustack:inter_token_latency_seconds Histogram Time to generate the next token after the previous token has been produced. gpustack:time_per_output_token_seconds Histogram Time per generated token. gpustack:e2e_request_latency_seconds Histogram End-to-end request latency. gpustack:request_success Counter Total number of successful requests. <p>These metrics are mapped from various runtime engines (vLLM, SGLang, MindIE) as defined in metrics_config.yaml.</p>"},{"location":"user-guide/observability/#worker-metrics","title":"Worker Metrics","text":"Metric Name Type Description gpustack:worker_status Gauge Worker status (with state label). gpustack:worker_node_os Info Operating system information of the worker node. gpustack:worker_node_kernel Info Kernel information of the worker node. gpustack:worker_node_uptime_seconds Gauge Uptime in seconds of the worker node. gpustack:worker_node_cpu_cores Gauge Total CPU cores of the worker node. gpustack:worker_node_cpu_utilization_rate Gauge CPU utilization rate of the worker node. gpustack:worker_node_memory_total_bytes Gauge Total memory in bytes of the worker node. gpustack:worker_node_memory_used_bytes Gauge Memory used in bytes of the worker node. gpustack:worker_node_memory_utilization_rate Gauge Memory utilization rate of the worker node. gpustack:worker_node_gpu Info GPU information of the worker node. gpustack:worker_node_gpu_cores Gauge Total GPU cores of the worker node. gpustack:worker_node_gpu_utilization_rate Gauge GPU utilization rate of the worker node. gpustack:worker_node_gpu_temperature_celsius Gauge GPU temperature in Celsius. gpustack:worker_node_gram_total_bytes Gauge Total GPU RAM in bytes. gpustack:worker_node_gram_allocated_bytes Gauge Allocated GPU RAM in bytes. gpustack:worker_node_gram_used_bytes Gauge Used GPU RAM in bytes. gpustack:worker_node_gram_utilization_rate Gauge GPU RAM utilization rate. gpustack:worker_node_filesystem_total_bytes Gauge Total filesystem size in bytes. gpustack:worker_node_filesystem_used_bytes Gauge Used filesystem size in bytes. gpustack:worker_node_filesystem_utilization_rate Gauge Filesystem utilization rate."},{"location":"user-guide/observability/#server-metrics","title":"Server Metrics","text":"Metric Name Type Description gpustack:cluster Info Cluster information (ID, name, provider). gpustack:cluster_status Gauge Cluster status (with state label). gpustack:model Info Model information (ID, name, runtime, source). gpustack:model_desired_instances Gauge Desired number of model instances. gpustack:model_running_instances Gauge Number of running model instances. gpustack:model_instance_status Gauge Status of each model instance (with state label). <p>Note: All metrics are labeled with relevant identifiers (cluster, worker, model, instance, user) for fine-grained monitoring and filtering.</p>"},{"location":"user-guide/observability/#deploy-observability-stack","title":"Deploy Observability Stack","text":"<p>The observability stack consists of two components:</p> <ul> <li>Prometheus: Scrapes metrics from GPUStack and stores them.</li> <li>Grafana: Visualizes metrics from Prometheus.</li> </ul> <p>All components can be deployed together via Docker Compose for easy management.</p>"},{"location":"user-guide/observability/#deploying-alongside-gpustack-server","title":"Deploying alongside GPUStack Server","text":"<p>You can deploy GPUStack together with the observability stack using the provided <code>docker-compose.yaml</code> for a one-step setup. For details, refer to the Installation via Docker Compose.</p>"},{"location":"user-guide/observability/#deploying-separately","title":"Deploying Separately","text":"<p>If you started GPUStack using <code>docker run</code> (not Compose), you can deploy the observability components separately and connect them to your existing GPUStack server as follows:</p>"},{"location":"user-guide/observability/#steps","title":"Steps","text":"<ol> <li> <p>Docker Compose Configuration</p> <p>Compose file location: docker-compose.observability.yaml (GitHub)</p> <pre><code>services:\nprometheus:\n    image: prom/prometheus\n    container_name: gpustack-prometheus\n    restart: unless-stopped\n    network_mode: host\n    command:\n    - \"--config.file=/etc/prometheus/prometheus.yml\"\n    - \"--web.enable-remote-write-receiver\"\n    volumes:\n    - ./prometheus:/etc/prometheus\n    - prom_data:/prometheus\n\ngrafana:\n    image: grafana/grafana\n    container_name: gpustack-grafana\n    restart: unless-stopped\n    network_mode: host\n    environment:\n    - GF_SERVER_HTTP_PORT=3000\n    - GF_SECURITY_ADMIN_USER=admin\n    - GF_SECURITY_ADMIN_PASSWORD=grafana\n    - GF_FEATURE_TOGGLES_ENABLE=flameGraph traceqlSearch traceQLStreaming correlations metricsSummary traceqlEditor traceToMetrics traceToProfiles\n    volumes:\n    - ./grafana/grafana_provisioning:/etc/grafana/provisioning:ro\n    - ./grafana/grafana_dashboards:/etc/dashboards:ro\n\nvolumes:\nprom_data: {}\n</code></pre> </li> <li> <p>Edit the Prometheus configuration file <code>prometheus.yml</code>.</p> <p>Configure Prometheus to scrape metrics from GPUStack by editing the <code>prometheus.yml</code>, file location: prometheus.yaml (GitHub)</p> <pre><code>scrape_configs:\n- job_name: gpustack-worker-discovery\n    scrape_interval: 5s\n    http_sd_configs:\n    - url: \"http://&lt;gpustack_server_host&gt;:10161/metrics/targets\"\n        refresh_interval: 1m\n- job_name: gpustack-server\n    scrape_interval: 10s\n    static_configs:\n    - targets: [\"&lt;gpustack_server_host&gt;:10161\"]\n</code></pre> </li> </ol>"},{"location":"user-guide/observability/#accessing-metrics","title":"Accessing Metrics","text":"<ul> <li>GPUStack Metrics Endpoint:   Access metrics at <code>http://&lt;gpustack_server_host&gt;:10161/metrics</code></li> <li>GPUStack Worker Metrics Targets:   Access metrics at <code>http://&lt;gpustack_server_host&gt;:10161/metrics/targets</code></li> <li>Prometheus UI:   Access Prometheus at <code>http://&lt;host&gt;:9090</code></li> <li>Grafana UI:   Access Grafana at <code>http://&lt;host&gt;:3000</code> (default user: <code>admin</code>, password: <code>grafana</code>)</li> </ul>"},{"location":"user-guide/observability/#customizing-metrics-mapping","title":"Customizing Metrics Mapping","text":"<p>GPUStack supports dynamic customization of metrics mapping through its configuration API. This allows you to update how runtime engine metrics are mapped to GPUStack metrics without restarting the service. The configuration is managed centrally on the server and can be accessed or modified via HTTP API.</p>"},{"location":"user-guide/observability/#api-endpoints","title":"API Endpoints","text":"<ul> <li> <p>Get Current Metrics Config</p> </li> <li> <p>GET <code>http://&lt;gpustack_server_host&gt;:&lt;gpustack_server_port&gt;/v1/metrics/config</code></p> </li> <li> <p>Returns the current metrics mapping configuration in JSON format.</p> </li> <li> <p>Update Metrics Config</p> </li> <li> <p>POST <code>http://&lt;gpustack_server_host&gt;:&lt;gpustack_server_port&gt;/v1/metrics/config</code></p> </li> <li> <p>Accepts a JSON payload to update the metrics mapping configuration. Changes take effect immediately for all workers.</p> </li> <li> <p>Get Default Metrics Config</p> </li> <li>GET <code>http://&lt;gpustack_server_host&gt;:&lt;gpustack_server_port&gt;/v1/metrics/default-config</code></li> <li>Returns the default metrics mapping configuration in JSON format, useful for reference or resetting.</li> </ul>"},{"location":"user-guide/observability/#example-usage","title":"Example Usage","text":"<p>Get current config:</p> <pre><code>curl http://&lt;gpustack_server_host&gt;:&lt;gpustack_server_port&gt;/v1/metrics/config\n</code></pre> <p>Update config:</p> <pre><code>curl -X POST http://&lt;gpustack_server_host&gt;:&lt;gpustack_server_port&gt;/v1/metrics/config \\\n     -H \"Content-Type: application/json\" \\\n     -d @custom_metrics_config.json\n</code></pre> <p>(where <code>custom_metrics_config.json</code> is your new config file)</p> <p>Get default config:</p> <pre><code>curl -X POST http://&lt;gpustack_server_host&gt;:&lt;gpustack_server_port&gt;/v1/metrics/default-config\n</code></pre> <p>Note: The configuration should be provided in valid JSON format.</p>"},{"location":"user-guide/openai-compatible-apis/","title":"OpenAI Compatible APIs","text":"<p>GPUStack serves OpenAI-compatible APIs using the <code>/v1</code> path.</p> <p>Note</p> <p>For other APIs, GPUStack allows you to enable the Generic Proxy when deploying a model.</p> <p>With the Generic Proxy enabled, GPUStack determines which model to forward the request to by checking either of the following:</p> <ul> <li> <p>the \"model\" field in the JSON body</p> </li> <li> <p>the <code>X-GPUStack-Model</code> header</p> </li> </ul> <p>For more details, see Enable Generic Proxy.</p>"},{"location":"user-guide/openai-compatible-apis/#supported-endpoints","title":"Supported Endpoints","text":"<p>The following API endpoints are supported:</p> <ul> <li> List Models</li> <li> Create Completion</li> <li> Create Chat Completion</li> <li> Create Embeddings</li> <li> Create Image</li> <li> Create Image Edit</li> <li> Create Speech</li> <li> Create Transcription</li> </ul>"},{"location":"user-guide/openai-compatible-apis/#usage","title":"Usage","text":"<p>The following are examples using the APIs in different languages:</p>"},{"location":"user-guide/openai-compatible-apis/#curl","title":"cURL","text":"<pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"qwen3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"user-guide/openai-compatible-apis/#openai-python-api-library","title":"OpenAI Python API library","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"http://your_gpustack_server_url/v1\", api_key=\"your_api_key\")\n\ncompletion = client.chat.completions.create(\n  model=\"qwen3\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre>"},{"location":"user-guide/openai-compatible-apis/#openai-node-api-library","title":"OpenAI Node API library","text":"<pre><code>const OpenAI = require(\"openai\");\n\nconst openai = new OpenAI({\n  apiKey: \"your_api_key\",\n  baseURL: \"http://your_gpustack_server_url/v1\",\n});\n\nasync function main() {\n  const params = {\n    model: \"qwen3\",\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant.\",\n      },\n      {\n        role: \"user\",\n        content: \"Hello!\",\n      },\n    ],\n  };\n  const chatCompletion = await openai.chat.completions.create(params);\n  console.log(chatCompletion.choices[0].message);\n}\nmain();\n</code></pre>"},{"location":"user-guide/rerank-api/","title":"Rerank API","text":"<p>In the context of Retrieval-Augmented Generation (RAG), reranking refers to the process of selecting the most relevant information from retrieved documents or knowledge sources before presenting them to the user or utilizing them for answer generation.</p> <p>GPUStack serves Jina compatible Rerank API using the <code>/v1/rerank</code> path.</p> <p>Note</p> <p>For other APIs, GPUStack allows you to enable the Generic Proxy when deploying a model.</p> <p>With the Generic Proxy enabled, you can send API requests to a unified gateway and add the <code>X-GPUStack-Model</code> header.</p> <p>This header tells GPUStack which model should handle the request.</p> <p>For more details, see Enable Generic Proxy.</p>"},{"location":"user-guide/rerank-api/#usage","title":"Usage","text":"<p>The following is an example using the Rerank API:</p> <pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"model\": \"bge-reranker-v2-m3\",\n        \"query\": \"What is a panda?\",\n        \"top_n\": 3,\n        \"documents\": [\n            \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n        ]\n    }' | jq\n</code></pre> <p>Example output:</p> <pre><code>{\n  \"model\": \"bge-reranker-v2-m3\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"document\": {\n        \"text\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n      },\n      \"index\": 2,\n      \"relevance_score\": 1.951932668685913\n    },\n    {\n      \"document\": {\n        \"text\": \"it is a bear\"\n      },\n      \"index\": 1,\n      \"relevance_score\": -3.7347371578216553\n    },\n    {\n      \"document\": {\n        \"text\": \"hi\"\n      },\n      \"index\": 0,\n      \"relevance_score\": -6.157620906829834\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 69,\n    \"total_tokens\": 69\n  }\n}\n</code></pre>"},{"location":"user-guide/sso/","title":"Single Sign-On (SSO) Authentication","text":"<p>GPUStack supports Single Sign-On (SSO) authentication methods such as OIDC and SAML. This allows users to log in using their existing credentials from an external identity provider.</p>"},{"location":"user-guide/sso/#oidc","title":"OIDC","text":"<p>Any authentication provider that supports OIDC can be configured. The <code>email</code>, <code>name</code> and <code>picture</code> claims are used if available. The allowed redirect URI should include <code>&lt;server-url&gt;/auth/oidc/callback</code>.</p> <p>The following CLI flags are available for OIDC configuration:</p> Flag Description <code>--oidc-issuer</code> OIDC issuer URL. OIDC discovery under <code>&lt;issuer&gt;/.well-known/openid-configuration</code> will be used to discover the OIDC configuration. <code>--oidc-client-id</code> OIDC client ID. <code>--oidc-client-secret</code> OIDC client secret. <code>--oidc-redirect-uri</code> The redirect URI configured in your OIDC application. This must be set to <code>&lt;server-url&gt;/auth/oidc/callback</code>. <code>--external-auth-name</code> (Optional) Mapping of OIDC user information to username, e.g., <code>preferred_username</code>. By default, the <code>email</code> claim is used if available. <code>--external-auth-full-name</code> (Optional) Mapping of OIDC user information to user's full name. Multiple elements can be combined, e.g., <code>name</code> or <code>firstName+lastName</code>. By default, the <code>name</code> claim is used. <code>--external-auth-avatar-url</code> (Optional) Mapping of OIDC user information to user's avatar URL. By default, the <code>picture</code> claim is used if available. <p>You can also set these options via environment variables instead of CLI flags:</p> <pre><code>GPUSTACK_OIDC_ISSUER=\"your-oidc-issuer-url\"\nGPUSTACK_OIDC_CLIENT_ID=\"your-client-id\"\nGPUSTACK_OIDC_CLIENT_SECRET=\"your-client-secret\"\nGPUSTACK_OIDC_REDIRECT_URI=\"{your-server-url}/auth/oidc/callback\"\n# Optional\nGPUSTACK_EXTERNAL_AUTH_NAME=\"email\"\nGPUSTACK_EXTERNAL_AUTH_FULL_NAME=\"name\"\nGPUSTACK_EXTERNAL_AUTH_AVATAR_URL=\"picture\"\n</code></pre>"},{"location":"user-guide/sso/#example-integrate-with-auth0-oidc","title":"Example: Integrate with Auth0 OIDC","text":"<p>To configure GPUStack with Auth0 as the OIDC provider:</p> <ol> <li>Go to auth0 and create a new application with type <code>Regular Web Applications</code>.</li> </ol> <p></p> <ol> <li>Get the <code>Domain</code>, <code>Client ID</code>, and <code>Client Secret</code> from the application settings.</li> </ol> <p></p> <ol> <li>Add <code>&lt;your-server-url&gt;/auth/oidc/callback</code> in the Allowed Callback URLs. Adapt the URL to match your server's URL.</li> </ol> <p></p> <p>Then, run GPUStack with relevant OIDC configuration. The following example uses Docker with CUDA:</p> <pre><code>sudo docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --privileged \\\n    --network=host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime nvidia \\\n    -e GPUSTACK_OIDC_ISSUER=\"https://&lt;your-auth0-domain&gt;\" \\\n    -e GPUSTACK_OIDC_CLIENT_ID=\"&lt;your-client-id&gt;\" \\\n    -e GPUSTACK_OIDC_CLIENT_SECRET=\"&lt;your-client-secret&gt;\" \\\n    -e GPUSTACK_OIDC_REDIRECT_URI=\"&lt;your-server-url&gt;/auth/oidc/callback\" \\\n    gpustack/gpustack\n</code></pre>"},{"location":"user-guide/sso/#saml","title":"SAML","text":"<p>GPUStack supports SAML authentication for Single Sign-On (SSO). This allows users to log in using their existing credentials from an external identity provider that supports SAML.</p> <p>The following CLI flags are available for SAML configuration:</p> Flag Description <code>--saml-idp-server-url</code> SAML Identity Provider server URL. <code>--saml-idp-entity-id</code> SAML Identity Provider entity ID. <code>--saml-idp-x509-cert</code> SAML Identity Provider X.509 certificate. <code>--saml-sp-entity-id</code> SAML Service Provider entity ID. <code>--saml-sp-acs-url</code> SAML Service Provider Assertion Consumer Service URL. It should be set to <code>&lt;gpustack-server-url&gt;/auth/saml/callback</code>. <code>--saml-sp-x509-cert</code> SAML Service Provider X.509 certificate. <code>--saml-sp-private-key</code> SAML Service Provider private key. <code>--saml-sp-attribute-prefix</code> (Optional) SAML Service Provider attribute prefix, which is used for fetching the attributes that are specified by --external-auth-*. e.g., 'http://schemas.auth0.com/'. <code>--saml-security</code> (Optional) SAML security settings in JSON format. <code>--external-auth-name</code> (Optional) Mapping of SAML user information to username. You must configure the full attribute name like 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress' or simplify with 'emailaddress' by '--saml-sp-attribute-prefix'. <code>--external-auth-full-name</code> (Optional) Mapping of SAML user information to user's full name. Multiple elements can be combined. You must configure the full attribute name like 'http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name' or simplify with 'name' by '--saml-sp-attribute-prefix'. <code>--external-auth-avatar-url</code> (Optional) Mapping of SAML user information to user's avatar URL. You must configure the full attribute name like 'http://schemas.auth0.com/picture' or simplify with 'picture' by '--saml-sp-attribute-prefix'. <p>You can also set these options via environment variables instead of CLI flags:</p> <pre><code>GPUSTACK_SAML_IDP_SERVER_URL=\"https://idp.example.com\"\nGPUSTACK_SAML_IDP_ENTITY_ID=\"your-idp-entity-id\"\nGPUSTACK_SAML_IDP_X509_CERT=\"your-idp-x509-cert\"\nGPUSTACK_SAML_SP_ENTITY_ID=\"your-sp-entity-id\"\nGPUSTACK_SAML_SP_ACS_URL=\"{your-server-url}/auth/saml/callback\"\nGPUSTACK_SAML_SP_X509_CERT=\"your-sp-x509-cert\"\nGPUSTACK_SAML_SP_PRIVATE_KEY=\"your-sp-private-key\"\n# Optional\nGPUSTACK_SAML_SP_ATTRIBUTE_PREFIX=\"http://schemas.auth0.com/\"\nGPUSTACK_SAML_SECURITY=\"{}\"\nGPUSTACK_EXTERNAL_AUTH_NAME=\"emailaddress\"\nGPUSTACK_EXTERNAL_AUTH_FULL_NAME=\"name\"\nGPUSTACK_EXTERNAL_AUTH_AVATAR_URL=\"picture\"\n</code></pre>"},{"location":"user-guide/sso/#example-integrate-with-auth0-saml","title":"Example: Integrate with Auth0 SAML","text":"<p>To configure GPUStack with Auth0 as the SAML provider:</p> <ol> <li>Go to auth0 and create a new application with type <code>Regular Web Applications</code>.</li> </ol> <p></p> <ol> <li>Get the <code>Domain</code> from the application settings and add <code>&lt;your-server-url&gt;/auth/saml/callback</code> in the Allowed Callback URLs. Adapt the URL to match your server's URL.</li> </ol> <p></p> <ol> <li>In Advanced Settings \u2192 Certificates, copy the IdP <code>X.509 Certificate</code>.</li> </ol> <p></p> <ol> <li>In Endpoints tab, find the <code>SAML Protocol URL</code>, which is your IdP server URL.</li> </ol> <p></p> <ol> <li>Generate SP certificate and private key:</li> </ol> <pre><code>openssl req -x509 -newkey rsa:2048 -keyout myservice.key -out myservice.cert -days 365 -nodes -subj \"/CN=myservice.example.com\"\n</code></pre> <p>Note</p> <p>myservice.cert and myservice.key will be used for the SP configuration.</p> <ol> <li>Run GPUStack with relevant SAML configuration. The following example uses Docker with CUDA:</li> </ol> <pre><code>SP_CERT=\"$(cat myservice.cert)\"\nSP_PRIVATE_KEY=\"$(cat myservice.key)\"\nSP_ATTRIBUTE_PREFIX=\"http://schemas.auth0.com/\"\n\nsudo docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --privileged \\\n    --network=host \\\n    --volume /var/run/docker.sock:/var/run/docker.sock \\\n    --volume gpustack-data:/var/lib/gpustack \\\n    --runtime nvidia \\\n    -e GPUSTACK_SAML_IDP_SERVER_URL=\"&lt;auth0-saml-protocol-url&gt;\" \\\n    -e GPUSTACK_SAML_IDP_ENTITY_ID=\"urn:&lt;auth0-domain&gt;\" \\\n    -e GPUSTACK_SAML_IDP_X509_CERT=\"&lt;auth0-x509-cert&gt;\" \\\n    -e GPUSTACK_SAML_SP_ENTITY_ID=\"urn:gpustack:sp\" \\\n    -e GPUSTACK_SAML_SP_ACS_URL=\"&lt;your-gpustack-server-url&gt;/auth/saml/callback\" \\\n    -e GPUSTACK_SAML_SP_X509_CERT=\"$SP_CERT\" \\\n    -e GPUSTACK_SAML_SP_PRIVATE_KEY=\"$SP_PRIVATE_KEY\" \\\n    -e GPUSTACK_SAML_SP_ATTRIBUTE_PREFIX=\"$SP_ATTRIBUTE_PREFIX\" \\\n    gpustack/gpustack\n</code></pre>"},{"location":"user-guide/user-management/","title":"User Management","text":"<p>GPUStack has two user roles: <code>Admin</code> and <code>User</code>.</p> <p>Admins can manage clusters, resources, models, users, and system settings.</p> <p>Users can manage their own API keys and access the model APIs.</p>"},{"location":"user-guide/user-management/#default-admin","title":"Default Admin","text":"<p>On bootstrap, GPUStack creates a default admin user.</p> <p>The initial password is saved in <code>&lt;data-dir&gt;/initial_admin_password</code>.</p> <p>In the default setup, this file is located at <code>/var/lib/gpustack/initial_admin_password</code> inside the server container.</p> <p>You can set a custom password for the default admin by using the <code>--bootstrap-password</code> flag when starting <code>GPUStack</code>.</p>"},{"location":"user-guide/user-management/#create-user","title":"Create User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Click the <code>Add User</code> button.</li> <li>Fill in <code>Name</code>, <code>Full Name</code>, <code>Password</code>, and select <code>Role</code> for the user.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/user-management/#update-user","title":"Update User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to edit.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>Update the attributes as needed.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/user-management/#deactivate-user","title":"Deactivate User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to deactivate.</li> <li>Click the <code>Deactivate Account</code> button in the <code>Operations</code> column.</li> </ol>"},{"location":"user-guide/user-management/#activate-user","title":"Activate User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to activate.</li> <li>Click the <code>Activate Account</code> button in the <code>Operations</code> column.</li> </ol>"},{"location":"user-guide/user-management/#delete-user","title":"Delete User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to delete.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol> <p>Note</p> <p>The admin user cannot be deactivated or deleted from the UI.</p>"},{"location":"user-guide/playground/","title":"Playground","text":"<p>GPUStack offers a playground UI where users can test and experiment with the APIs. Refer to each subpage for detailed instructions and information.</p>"},{"location":"user-guide/playground/audio/","title":"Audio Playground","text":"<p>The Audio Playground is a dedicated space for testing and experimenting with GPUStack\u2019s Text-to-Speech (TTS) and Speech-to-Text (STT) APIs. It allows users to interactively convert text to audio and audio to text, customize parameters, and review code examples for seamless API integration.</p>"},{"location":"user-guide/playground/audio/#text-to-speech","title":"Text to Speech","text":"<p>Switch to the \"Text to Speech\" tab to test TTS models.</p>"},{"location":"user-guide/playground/audio/#text-input","title":"Text Input","text":"<p>Enter the text you want to convert, then click the <code>Submit</code> button to generate the corresponding speech.</p> <p></p>"},{"location":"user-guide/playground/audio/#select-model","title":"Select Model","text":"<p>Select an available TTS model in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/audio/#customize-parameters","title":"Customize Parameters","text":"<p>Customize the voice and format of the audio output.</p> <p>Tip</p> <p>Supported voices may vary between models.</p>"},{"location":"user-guide/playground/audio/#view-code","title":"View Code","text":"<p>After experimenting with input text and parameters, click the <code>View Code</code> button to see how to call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/audio/#speech-to-text","title":"Speech to Text","text":"<p>Switch to the \"Speech to Text\" tab to test STT models.</p>"},{"location":"user-guide/playground/audio/#provide-audio-file","title":"Provide Audio File","text":"<p>You can provide audio for transcription in two ways:</p> <ol> <li>Upload an audio file</li> <li>Record audio online</li> </ol> <p>Note</p> <p>If the online recording is not available, it could be due to one of the following reasons:</p> <ol> <li>For HTTPS or <code>http://localhost</code> access, microphone permissions must be enabled in your browser.</li> <li> <p>For access via <code>http://{host IP}</code>, the URL must be added to your browser's trusted list.</p> <p>Example:   In Chrome, navigate to <code>chrome://flags/</code>, add the GPUStack URL to \"Insecure origins treated as secure\", and enable this option.</p> </li> </ol> <p></p> <p></p>"},{"location":"user-guide/playground/audio/#select-model_1","title":"Select Model","text":"<p>Select an available STT model in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/audio/#copy-text","title":"Copy Text","text":"<p>Copy the transcription results generated by the model.</p>"},{"location":"user-guide/playground/audio/#customize-parameters_1","title":"Customize Parameters","text":"<p>Select the appropriate language for your audio file to optimize transcription accuracy.</p>"},{"location":"user-guide/playground/audio/#view-code_1","title":"View Code","text":"<p>After experimenting with audio files and parameters, click the <code>View Code</code> button to see how to call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/chat/","title":"Chat","text":""},{"location":"user-guide/playground/chat/#chat-playground","title":"Chat Playground","text":"<p>Interact with the chat completions API. The following is an example screenshot:</p> <p></p>"},{"location":"user-guide/playground/chat/#prompts","title":"Prompts","text":"<p>You can adjust the prompt messages on the left side of the playground. There are three role types of prompt messages: system, user, and assistant.</p> <ul> <li>System: Typically a predefined instruction or guidance that sets the context, defines the behavior, or imposes specific constraints on how the model should generate its responses.</li> <li>User: The input or query provided by the user (the person interacting with the LLM).</li> <li>Assistant: The response generated by the LLM.</li> </ul>"},{"location":"user-guide/playground/chat/#edit-system-message","title":"Edit System Message","text":"<p>You can add and edit the system message at the top of the playground.</p>"},{"location":"user-guide/playground/chat/#edit-user-and-assistant-messages","title":"Edit User and Assistant Messages","text":"<p>To add a user or assistant message, click the <code>Add</code> button.</p> <p>To remove a user or assistant message, click the minus button at the right corner of the message.</p> <p>To change the role of a message, click the <code>User</code> or <code>Assistant</code> text at the beginning of the message.</p>"},{"location":"user-guide/playground/chat/#upload-image","title":"Upload Image","text":"<p>You can add images to the prompt by clicking the <code>Upload Image</code> button.</p>"},{"location":"user-guide/playground/chat/#upload-audio","title":"Upload Audio","text":"<p>You can add audio files to the prompt by clicking the <code>Upload Audio</code> button.</p>"},{"location":"user-guide/playground/chat/#clear-prompts","title":"Clear Prompts","text":"<p>Click the <code>Clear</code> button to clear all the prompts.</p>"},{"location":"user-guide/playground/chat/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground. Please refer to Model Deployment Management to learn about how to deploy models.</p>"},{"location":"user-guide/playground/chat/#customize-parameters","title":"Customize Parameters","text":"<p>You can customize completion parameters in the <code>Parameters</code> section.</p>"},{"location":"user-guide/playground/chat/#do-completion","title":"Do Completion","text":"<p>You can do a completion by clicking the <code>Submit</code> button or pressing Enter.</p>"},{"location":"user-guide/playground/chat/#view-code","title":"View Code","text":"<p>Once you've done experimenting with the prompts and parameters, you can click the <code>View Code</code> button to check how you can call the API with the same input by code. Code examples in <code>curl</code>, <code>Python</code>, and <code>Node.js</code> are provided.</p>"},{"location":"user-guide/playground/chat/#compare-playground","title":"Compare Playground","text":"<p>You can compare multiple models in the playground. The following is an example screenshot:</p> <p></p>"},{"location":"user-guide/playground/chat/#comparision-mode","title":"Comparision Mode","text":"<p>You can choose the number of models to compare by clicking the comparison view buttons, including 2, 3, 4 and 6-model comparison.</p>"},{"location":"user-guide/playground/chat/#prompts_1","title":"Prompts","text":"<p>You can adjust the prompt messages similar to the chat playground.</p>"},{"location":"user-guide/playground/chat/#upload-image_1","title":"Upload Image","text":"<p>You can add images to the prompt by clicking the <code>Upload Image</code> button.</p>"},{"location":"user-guide/playground/chat/#upload-audio_1","title":"Upload Audio","text":"<p>You can add audio files to the prompt by clicking the <code>Upload Audio</code> button.</p>"},{"location":"user-guide/playground/chat/#clear-prompts_1","title":"Clear Prompts","text":"<p>Click the <code>Clear</code> button to clear all the prompts.</p>"},{"location":"user-guide/playground/chat/#select-model_1","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-left corner of each model panel.</p>"},{"location":"user-guide/playground/chat/#customize-parameters_1","title":"Customize Parameters","text":"<p>You can customize completion parameters by clicking the settings button of each model.</p>"},{"location":"user-guide/playground/embedding/","title":"Embedding Playground","text":"<p>The Embedding Playground lets you test the model\u2019s ability to convert text into embeddings. It allows you to experiment with multiple text inputs, visualize embeddings, and review code examples for API integration.</p>"},{"location":"user-guide/playground/embedding/#add-text","title":"Add Text","text":"<p>Add at least two text entries and click the <code>Submit</code> button to generate embeddings.</p>"},{"location":"user-guide/playground/embedding/#batch-input-text","title":"Batch Input Text","text":"<p>Enable <code>Batch Input Mode</code> to automatically split multi-line text into separate entries based on line breaks. This is useful for processing multiple text snippets in a single operation.</p>"},{"location":"user-guide/playground/embedding/#visualization","title":"Visualization","text":"<p>Visualize the embedding results using PCA (Principal Component Analysis) to reduce dimensions and display them on a 2D plot. Results can be viewed in two formats:</p> <ol> <li>Chart - Display PCA results visually.</li> <li>JSON - View raw embeddings in JSON format.</li> </ol> <p>In the chart, the distance between points represents the similarity between corresponding texts. Closer points indicate higher similarity.</p> <p></p>"},{"location":"user-guide/playground/embedding/#clear","title":"Clear","text":"<p>Click the <code>Clear</code> button to reset text entries and clear the output.</p>"},{"location":"user-guide/playground/embedding/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/embedding/#view-code","title":"View Code","text":"<p>After experimenting with the text inputs, click the <code>View Code</code> button to see how you can call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/image/","title":"Image Playground","text":"<p>The Image Playground is a dedicated space for testing and experimenting with GPUStack\u2019s image generation APIs. It allows users to interactively explore the capabilities of different models, customize parameters, and review code examples for seamless API integration.</p>"},{"location":"user-guide/playground/image/#generate-image","title":"Generate Image","text":""},{"location":"user-guide/playground/image/#prompt","title":"Prompt","text":"<p>You can input or randomly generate a prompt, then click the <code>Submit</code> button to generate an image.</p> <p></p>"},{"location":"user-guide/playground/image/#edit-image","title":"Edit Image","text":"<p>Upload an image and highlight the areas you want to modify by painting over them. Then, enter a prompt and <code>Submit</code>. If no areas are painted, the entire image will be modified.</p> <p></p>"},{"location":"user-guide/playground/image/#save-mask","title":"Save Mask","text":"<p>Click <code>Save Mask</code> to save the painted areas as a separate image.</p>"},{"location":"user-guide/playground/image/#download-image","title":"Download Image","text":"<p>Click <code>Download Image</code> to save the edited image.</p>"},{"location":"user-guide/playground/image/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/image/#customize-parameters","title":"Customize Parameters","text":"<p>You can customize the image generation parameters by switching between two API styles:</p> <ol> <li>OpenAI Compatible mode</li> <li>Advanced mode</li> </ol> <p></p>"},{"location":"user-guide/playground/image/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Default Description <code>Size</code> <code>512x512</code> The size of the generated image in 'widthxheight' format. <code>Sample Method</code> <code>euler_a</code> The sampler algorithm for image generation. Options include 'euler_a', 'euler', 'heun', 'dpm2', 'dpm++2s_a', 'dpm++2m', 'dpm++2mv2', 'ipndm', 'ipndm_v', and 'lcm'. <code>Schedule Method</code> <code>discrete</code> The noise scheduling method. <code>Sampling Steps</code> <code>10</code> The number of sampling steps to perform. Higher values may improve image quality at the cost of longer processing time. <code>Guidance</code> <code>3.5</code> The scale for classifier-free guidance. A higher value increases adherence to the prompt. <code>CFG Scale</code> <code>4.5</code> The scale for classifier-free guidance. A higher value increases adherence to the prompt. <code>Negative Prompt</code> (empty) A negative prompt to specify what the image should avoid. <code>Seed</code> (empty) Random seed."},{"location":"user-guide/playground/image/#view-code","title":"View Code","text":"<p>After experimenting with prompts and parameters, click the <code>View Code</code> button to see how to call the API with the same inputs. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/rerank/","title":"Rerank Playground","text":"<p>The Rerank Playground allows you to test reranker models that reorder multiple texts based on their relevance to a query. Experiment with various input texts, customize parameters, and review code examples for API integration.</p>"},{"location":"user-guide/playground/rerank/#add-text","title":"Add Text","text":"<p>Add multiple text entries to the document for reranking.</p>"},{"location":"user-guide/playground/rerank/#bach-input-text","title":"Bach Input Text","text":"<p>Enable <code>Batch Input Mode</code> to split multi-line text into separate entries based on line breaks. This is useful for processing multiple text snippets efficiently.</p>"},{"location":"user-guide/playground/rerank/#clear","title":"Clear","text":"<p>Click the <code>Clear</code> button to reset the document and query results.</p>"},{"location":"user-guide/playground/rerank/#query","title":"Query","text":"<p>Input a query and click the <code>Submit</code> button to get a ranked list of texts based on their relevance to the query.</p> <p></p>"},{"location":"user-guide/playground/rerank/#select-model","title":"Select Model","text":"<p>Select an available reranker model in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/rerank/#customize-parameters","title":"Customize Parameters","text":"<p>In the parameter section, set <code>Top N</code> to specify the number of matching texts to retrieve.</p>"},{"location":"user-guide/playground/rerank/#view-code","title":"View Code","text":"<p>After experimenting with the input text and query, click the <code>View Code</code> button to see how to call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"using-models/using-audio-models/","title":"Using Audio Models","text":"<p>GPUStack supports running both Speech-to-Text and Text-to-Speech models. Speech-to-Text models convert audio inputs in various languages into written text, while Text-to-Speech models transform written text into natural and expressive speech.</p> <p>In this guide, we will walk you through deploying and using Speech-to-Text and Text-to-Speech models in GPUStack.</p>"},{"location":"using-models/using-audio-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A Linux system with AMD64 architecture.</li> <li>Access to Hugging Face for downloading the model files.</li> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> </ul>"},{"location":"using-models/using-audio-models/#running-speech-to-text-model","title":"Running Speech-to-Text Model","text":""},{"location":"using-models/using-audio-models/#step-1-deploy-speech-to-text-model","title":"Step 1: Deploy Speech-to-Text Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar in the top left to search for the model name <code>Systran/faster-whisper-large-v3</code>.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model deployment's status on the <code>Deployments</code> page.</p> <p></p>"},{"location":"using-models/using-audio-models/#step-2-interact-with-speech-to-text-model","title":"Step 2: Interact with Speech-to-Text Model","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Audio</code> page in the GPUStack UI.</li> <li>Select the <code>Speech to Text</code> Tab.</li> <li>Select the deployed model from the top-right dropdown.</li> <li>Click the <code>Upload</code> button to upload audio file or click the <code>Microphone</code> button to record audio.</li> <li>Click the <code>Generate Text Content</code> button to generate the text.</li> </ol>"},{"location":"using-models/using-audio-models/#running-text-to-speech-model","title":"Running Text-to-Speech Model","text":""},{"location":"using-models/using-audio-models/#step-1-deploy-text-to-speech-model","title":"Step 1: Deploy Text-to-Speech Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar in the top left to search for the model name <code>gpustack/CosyVoice-300M</code>.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model deployment's status on the <code>Deployments</code> page.</p> <p></p>"},{"location":"using-models/using-audio-models/#step-2-interact-with-text-to-speech-model","title":"Step 2: Interact with Text to Speech Model","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Audio</code> page in the GPUStack UI.</li> <li>Select the <code>Text to Speech</code> Tab.</li> <li>Choose the deployed model from the dropdown menu in the top-right corner. Then, configure the voice and output audio format.</li> <li>Input the text to generate.</li> <li>Click the <code>Submit</code> button to generate the text.</li> </ol>"},{"location":"using-models/using-embedding-models/","title":"Using Embedding Models","text":"<p>Text embeddings are numerical representations of text that capture semantic meaning, enabling machines to understand relationships and similarities between different pieces of text. In essence, they transform text into vectors in a continuous space, where texts with similar meanings are positioned closer together. Text embeddings are widely used in applications such as natural language processing, information retrieval, and recommendation systems.</p> <p>In this guide, we will demonstrate how to deploy embedding models in GPUStack and generate text embeddings using the deployed models.</p>"},{"location":"using-models/using-embedding-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul>"},{"location":"using-models/using-embedding-models/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":"<p>Follow these steps to deploy the model from Catalog:</p> <ol> <li>Navigate to the <code>Catalog</code> page in the GPUStack UI.</li> <li>In the model list page, use dropdown to filter with <code>Embedding</code>.</li> <li>Review the model description, maximum context length and supported sizes.</li> </ol> <p></p> <p>After deployment, you can monitor the model deployment's status on the <code>Deployments</code> page.</p> <p></p>"},{"location":"using-models/using-embedding-models/#step-2-generate-an-api-key","title":"Step 2: Generate an API Key","text":"<p>We will use the GPUStack API to generate text embeddings, and an API key is required:</p> <ol> <li>Hover over the user avatar and navigate to the <code>API Keys</code> page.</li> <li>Click the <code>New API Key</code> button.</li> <li>Enter a name for the API key and click the <code>Save</code> button.</li> <li>Copy the generated API key. You can only view the API key once, so make sure to save it securely.</li> </ol>"},{"location":"using-models/using-embedding-models/#step-3-generate-text-embeddings","title":"Step 3: Generate Text Embeddings","text":"<p>With the model deployed and an API key, you can generate text embeddings via the GPUStack API. Here is an example script using <code>curl</code>:</p> <pre><code>export SERVER_URL=&lt;your-server-url&gt;\nexport GPUSTACK_API_KEY=&lt;your-api-key&gt;\ncurl $SERVER_URL/v1/embeddings \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer ${GPUSTACK_API_KEY}\" \\\n-d '{\n  \"model\": \"qwen3-embedding-4b\",\n  \"input\": \"The food was delicious and the waiter...\",\n  \"encoding_format\": \"float\"\n}'\n</code></pre> <p>Replace <code>&lt;your-server-url&gt;</code> with the URL of your GPUStack server and <code>&lt;your-api-key&gt;</code> with the API key you generated in the previous step.</p> <p>Example response:</p> <pre><code>{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.012189436703920364, 0.016934078186750412, 0.003965042531490326,\n        -0.03453584015369415, -0.07623119652271271, -0.007116147316992283,\n        0.11278388649225235, 0.019714849069714546, 0.010370955802500248,\n        -0.04219457507133484, -0.029902394860982895, 0.01122555136680603,\n        0.022912170737981796, 0.031186765059828758, 0.006303929258137941,\n        # ... additional values\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"qwen3-embedding-4b\",\n  \"object\": \"list\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9,\n    \"completion_tokens\": 0,\n    \"prompt_tokens_details\": null\n  }\n}\n</code></pre>"},{"location":"using-models/using-image-generation-models/","title":"Using Image Generation Models","text":"<p>GPUStack supports deploying and running Image Generation Models. In this guide, you deploy and use the Qwen-Image text-to-image model via the SGLang backend, then generate images from textual prompts in the GPUStack UI.</p>"},{"location":"using-models/using-image-generation-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU with at least 55 GB of VRAM.</li> <li>Access to Hugging Face or ModelScope to download the <code>Qwen/Qwen-Image</code> repository.</li> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> </ul>"},{"location":"using-models/using-image-generation-models/#step-1-deploy-the-qwen-image-model","title":"Step 1: Deploy the Qwen-Image Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar in the top-left to search for the repository <code>Qwen/Qwen-Image</code>.</li> <li>Keep the default settings and click the <code>Save</code> button to deploy. GPUStack will start the SGLang backend for Qwen-Image and download the required files.</li> </ol> <p></p> <p>After deployment, you can monitor the model deployment's status on the <code>Deployments</code> page.</p> <p></p>"},{"location":"using-models/using-image-generation-models/#step-2-use-the-model-for-image-generation","title":"Step 2: Use the Model for Image Generation","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Image</code> page in the GPUStack UI.</li> <li>Verify that the deployed <code>qwen-image</code> model is selected from the top-right <code>Model</code> dropdown.</li> <li>Enter a prompt describing the image you want to generate. For example:</li> </ol> <pre><code>a female character with long, flowing hair that appears to be made of ethereal, swirling patterns resembling the Northern Lights or Aurora Borealis. The background is dominated by deep blues and purples, creating a mysterious and dramatic atmosphere. The character's face is serene, with pale skin and striking features. She wears a dark-colored outfit with subtle patterns. The overall style of the artwork is reminiscent of fantasy or supernatural genres.\n</code></pre> <ol> <li>If your UI shows a <code>Sampler</code> dropdown and <code>Sample Steps</code>, keep the defaults. Qwen-Image uses a Diffusers-based flow matching scheduler under SGLang; GPUStack maps UI settings to appropriate parameters.</li> <li>Click the <code>Submit</code> button to create the image.</li> </ol> <p>The generated image will be displayed in the UI. Results vary with randomness, seeds, and prompt details. </p> <p>After completing an image generation task, memory usage typically increases from ~45 GB (weights only) to ~54 GB due to additional VRAM consumption by diffusion models. If additional models like LoRA are used, VRAM consumption will further increase.</p> <p></p>"},{"location":"using-models/using-image-generation-models/#conclusion","title":"Conclusion","text":"<p>With this setup, you can generate unique and visually compelling images from textual prompts using Qwen-Image served by SGLang. Experiment with different prompts and settings to push the boundaries of what\u2019s possible.</p>"},{"location":"using-models/using-large-language-models/","title":"Using Large Language Models","text":"<p>Large Language Models (LLMs) are powerful AI models capable of understanding and generating human-like text, making them essential for applications such as chatbots, content generation, code completion, and more.</p> <p>In this guide, you will learn how to deploy and interact with LLMs in GPUStack.</p>"},{"location":"using-models/using-large-language-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A Linux machine with one or more GPUs that has at least 30 GB of VRAM in total. We will use the vLLM backend which only supports Linux.</li> <li>Access to Hugging Face or ModelScope for downloading the model files.</li> <li>GPUStack installed and running. If not, refer to the Quickstart Guide.</li> </ul>"},{"location":"using-models/using-large-language-models/#step-1-deploy-large-language-models","title":"Step 1: Deploy Large Language Models","text":""},{"location":"using-models/using-large-language-models/#deoloy-from-catalog","title":"Deoloy from Catalog","text":"<p>Large language models in the catalog are marked with the <code>LLM</code> category. When you select a large language model from the catalog, the default configurations should work as long as you have enough GPU resources and the backend is compatible with your setup (e.g., vLLM backend requires an amd64 Linux worker).</p> <p>Here, we take the deployment of <code>Qwen3 0.6B</code> as an example.</p> <p>Follow these steps to deploy the model from Catalog:</p> <ol> <li>Navigate to the <code>Catalog</code> page in the GPUStack UI.</li> <li>In the model list page, use dropdown to filter with <code>LLM</code>.</li> <li>Review the model description, maximum context length and supported sizes.</li> </ol> <p> </p>"},{"location":"using-models/using-large-language-models/#deployment-using-vllm","title":"Deployment Using vLLM","text":"<ol> <li>Select the <code>Qwen3 0.6B</code> from the catalog.</li> <li>Change the model name and configuration as needed.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p>After deployment, you can monitor the model deployment's status on the <code>Deployments</code> page and wait for it to start running.</p>"},{"location":"using-models/using-large-language-models/#step-2-use-the-llms-for-text-generation","title":"Step 2: Use the LLMs for Text Generation","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Chat</code> page in the GPUStack UI.</li> <li>Verify that the deployed model is selected from the top-right <code>Model</code> dropdown.</li> <li>Provide a prompt for the text generation.</li> <li>Adjust the <code>Parameters</code> on the right based on your needs.</li> <li>Click the <code>Submit</code> button to generate the text.</li> </ol> <p>The generated chain of thought and result will be displayed in the UI.</p> <p></p> <p>By following these steps, you can leverage LLMs for AI-powered text generation and natural language tasks in GPUStack. Experiment with different prompts and settings to explore the full capabilities of LLMs!</p>"},{"location":"using-models/using-reranker-models/","title":"Using Reranker Models","text":"<p>Reranker Models are specialized models designed to improve the ranking of a list of items based on relevance to a given query. They are commonly used in information retrieval and search systems to refine initial search results, prioritizing items that are more likely to meet the user\u2019s intent. Reranker models take the initial document list and reorder items to enhance precision in applications such as search engines, recommendation systems, and question-answering tasks.</p> <p>In this guide, we will demonstrate how to deploy and use reranker models in GPUStack.</p>"},{"location":"using-models/using-reranker-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul>"},{"location":"using-models/using-reranker-models/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":"<p>Follow these steps to deploy the model from Catalog:</p> <ol> <li>Navigate to the <code>Catalog</code> page in the GPUStack UI.</li> <li>In the model list page, use dropdown to filter with <code>Reranker</code>.</li> <li>Review the model description, maximum context length and supported sizes.</li> </ol> <p></p> <p>After deployment, you can monitor the model deployment's status on the <code>Deployments</code> page.</p> <p></p>"},{"location":"using-models/using-reranker-models/#step-2-generate-an-api-key","title":"Step 2: Generate an API Key","text":"<p>We will use the GPUStack API to interact with the model. To do this, you need to generate an API key:</p> <ol> <li>Hover over the user avatar and navigate to the <code>API Keys</code> page.</li> <li>Click the <code>New API Key</code> button.</li> <li>Enter a name for the API key and click the <code>Save</code> button.</li> <li>Copy the generated API key. You can only view the API key once, so make sure to save it securely.</li> </ol>"},{"location":"using-models/using-reranker-models/#step-3-reranking","title":"Step 3: Reranking","text":"<p>With the model deployed and an API key, you can rerank a list of documents via the GPUStack API. Here is an example script using <code>curl</code>:</p> <pre><code>export SERVER_URL=&lt;your-server-url&gt;\nexport GPUSTACK_API_KEY=&lt;your-api-key&gt;\ncurl $SERVER_URL/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"model\": \"qwen3-reranker-4b\",\n        \"query\": \"What is a panda?\",\n        \"top_n\": 3,\n        \"documents\": [\n            \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n        ]\n    }' | jq\n</code></pre> <p>Replace <code>&lt;your-server-url&gt;</code> with the URL of your GPUStack server and <code>&lt;your-api-key&gt;</code> with the API key you generated in the previous step.</p> <p>Example response:</p> <pre><code>{\n  \"model\": \"qwen3-reranker-4b\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"index\": 0,\n      \"document\": {\n        \"text\": \"hi\",\n        \"multi_modal\": null\n      },\n      \"relevance_score\": 0.9996911287307739\n    },\n    {\n      \"index\": 2,\n      \"document\": {\n        \"text\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\",\n        \"multi_modal\": null\n      },\n      \"relevance_score\": 0.8206241726875305\n    },\n    {\n      \"index\": 1,\n      \"document\": {\n        \"text\": \"it is a bear\",\n        \"multi_modal\": null\n      },\n      \"relevance_score\": 0.7244728803634644\n    }\n  ],\n  \"usage\": {\n    \"total_tokens\": 51\n  }\n}\n</code></pre>"},{"location":"using-models/using-vision-language-models/","title":"Using Vision Language Models","text":"<p>Vision Language Models can process both visual (image) and language (text) data simultaneously, making them versatile tools for various applications, such as image captioning, visual question answering, and more. In this guide, you will learn how to deploy and interact with Vision Language Models (VLMs) in GPUStack.</p> <p>The procedure for deploying and interacting with these models in GPUStack is similar. The main difference is the parameters you need to set when deploying the models. For more information on the parameters you can set, please refer to Backend Parameters .</p> <p>In this guide, we will cover the deployment of the following models:</p> <ul> <li>Qwen3-VL</li> <li>Llama3.2-Vision</li> <li>Pixtral</li> <li>Phi3.5-Vision</li> </ul>"},{"location":"using-models/using-vision-language-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A Linux machine with one or more GPUs that has at least 30 GB of VRAM in total. We will use the vLLM backend which only supports Linux.</li> <li>Access to Hugging Face and a Hugging Face API key for downloading the model files.</li> <li>You have been granted access to the above models on Hugging Face. Llama3.2-VL and Pixtral are gated models, and you need to request access to them.</li> </ul> <p>Note</p> <p>An Ubuntu node equipped with one H100 (80GB) GPU is used throughout this guide.</p>"},{"location":"using-models/using-vision-language-models/#step-1-install-gpustack","title":"Step 1: Install GPUStack","text":"<p>Please follow the Installation Documentation to install GPUStack.</p>"},{"location":"using-models/using-vision-language-models/#step-2-log-in-to-gpustack-ui","title":"Step 2: Log in to GPUStack UI","text":"<p>After the server starts, run the following command to get the default admin password:</p> <pre><code>docker exec gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>Open your browser and navigate to <code>http://your_host_ip</code> to access the GPUStack UI. Use the default username <code>admin</code> and the password you retrieved above to log in.</p>"},{"location":"using-models/using-vision-language-models/#step-3-deploy-vision-language-models-with-vllm","title":"Step 3: Deploy Vision Language Models with vLLM","text":""},{"location":"using-models/using-vision-language-models/#deploy-qwen3-vl","title":"Deploy Qwen3-VL","text":"<ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>Qwen/Qwen3-VL-4B-Instruct</code> in the search bar.</li> <li>Click the <code>Save</code> button. The default configurations should work as long as you have enough GPU resources.</li> </ol>"},{"location":"using-models/using-vision-language-models/#deploy-llama32-vision","title":"Deploy Llama3.2-Vision","text":"<ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>meta-llama/Llama-3.2-11B-Vision-Instruct</code> in the search bar.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button multiple times and add the following parameters:</li> </ol> <ul> <li><code>--enforce-eager</code></li> <li><code>--max-num-seqs=16</code></li> <li><code>--max-model-len=8192</code></li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"using-models/using-vision-language-models/#deploy-pixtral","title":"Deploy Pixtral","text":"<ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>mistralai/Pixtral-12B-2409</code> in the search bar.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button multiple times and add the following parameters:</li> </ol> <ul> <li><code>--tokenizer-mode=mistral</code></li> <li><code>--limit-mm-per-prompt=image=4</code></li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"using-models/using-vision-language-models/#deploy-phi35-vision","title":"Deploy Phi3.5-Vision","text":"<ol> <li>Navigate to the <code>Deployments</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>microsoft/Phi-3.5-vision-instruct</code> in the search bar.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button and add the following parameter:</li> </ol> <ul> <li><code>--trust-remote-code</code></li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"using-models/using-vision-language-models/#step-4-interact-with-vision-language-models","title":"Step 4: Interact with Vision Language Models","text":"<ol> <li>Navigate to the <code>Chat</code> page in the GPUStack UI.</li> <li>Select the deployed model from the top-right dropdown.</li> <li>Click on the <code>Upload Image</code> button above the input text area and upload an image.</li> <li>Enter a prompt in the input text area. For example, \"Describe the image.\"</li> <li>Click the <code>Submit</code> button to generate the output.</li> </ol>"}]}