{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"api-reference/","title":"API Reference","text":"<p>GPUStack provides a built-in Swagger UI. You can access it by navigating to <code>&lt;gpustack-server-url&gt;/docs</code> in your browser to view and interact with the APIs.</p> <p></p>"},{"location":"architecture/","title":"Architecture","text":"<p>The following diagram shows the architecture of GPUStack:</p> <p></p>"},{"location":"architecture/#server","title":"Server","text":"<p>The GPUStack server consists of the following components:</p> <ul> <li>API Server: Provides a RESTful interface for clients to interact with the system. It handles authentication and authorization.</li> <li>Scheduler: Responsible for assigning model instances to workers.</li> <li>Model Controller: Manages the rollout and scaling of model instances to match the desired model replicas.</li> <li>HTTP Proxy: Routes completion API requests to backend inference servers.</li> </ul>"},{"location":"architecture/#worker","title":"Worker","text":"<p>GPUStack workers are responsible for:</p> <ul> <li>Running inference servers for model instances assigned to the worker.</li> <li>Reporting status to the server.</li> </ul>"},{"location":"architecture/#sql-database","title":"SQL Database","text":"<p>The GPUStack server connects to a SQL database as the datastore. GPUStack uses SQLite by default, but you can configure it to use an external PostgreSQL as well.</p>"},{"location":"architecture/#inference-server","title":"Inference Server","text":"<p>Inference servers are the backends that performs the inference tasks. GPUStack supports llama-box, vLLM and vox-box as the inference server.</p>"},{"location":"architecture/#rpc-server","title":"RPC Server","text":"<p>The RPC server enables running llama-box backend on a remote host. The Inference Server communicates with one or several instances of RPC server, offloading computations to these remote hosts. This setup allows for distributed LLM inference across multiple workers, enabling the system to load larger models even when individual resources are limited.</p>"},{"location":"architecture/#ray-headworker","title":"Ray Head/Worker","text":"<p>Ray is a distributed computing framework that GPUStack utilizes to run distributed vLLM. Users can enable a Ray cluster in GPUStack to run vLLM across multiple workers. By default, it is disabled.</p>"},{"location":"code-of-conduct/","title":"Contributor Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@gpustack.ai. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p>"},{"location":"contributing/","title":"Contributing to GPUStack","text":"<p>Thanks for taking the time to contribute to GPUStack!</p> <p>Please review and follow the Code of Conduct.</p>"},{"location":"contributing/#filing-issues","title":"Filing Issues","text":"<p>If you find any bugs or are having any trouble, please search the reported issue as someone may have experienced the same issue, or we are actively working on a solution.</p> <p>If you can't find anything related to your issue, contact us by filing an issue. To help us diagnose and resolve, please include as much information as possible, including:</p> <ul> <li>Software: GPUStack version, installation method, operating system info, etc.</li> <li>Hardware: Node info, GPU info, etc.</li> <li>Steps to reproduce: Provide as much detail on how you got into the reported situation.</li> <li>Logs: Please include any relevant logs, such as server logs, worker logs, etc.</li> </ul>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":"<p>For setting up development environment, please refer to Development Guide.</p> <p>If you're fixing a small issue, you can simply submit a PR. However, if you're planning to submit a bigger PR to implement a new feature or fix a relatively complex bug, please open an issue that explains the change and the motivation for it. If you're addressing a bug, please explain how to reproduce it.</p>"},{"location":"contributing/#updating-documentation","title":"Updating Documentation","text":"<p>If you have any updates to our documentation, feel free to file an issue with the <code>documentation</code> label or make a pull request.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<p>Install Python (version 3.10 to 3.12).</p>"},{"location":"development/#set-up-environment","title":"Set Up Environment","text":"<pre><code>make install\n</code></pre>"},{"location":"development/#run","title":"Run","text":"<pre><code>poetry run gpustack\n</code></pre>"},{"location":"development/#build","title":"Build","text":"<pre><code>make build\n</code></pre> <p>And check artifacts in <code>dist</code>.</p>"},{"location":"development/#test","title":"Test","text":"<pre><code>make test\n</code></pre>"},{"location":"development/#update-dependencies","title":"Update Dependencies","text":"<pre><code>poetry add &lt;something&gt;\n</code></pre> <p>Or</p> <pre><code>poetry add --group dev &lt;something&gt;\n</code></pre> <p>For dev/testing dependencies.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#support-matrix","title":"Support Matrix","text":""},{"location":"faq/#hybird-cluster-support","title":"Hybird Cluster Support","text":"<p>It supports a mix of Linux, Windows, and macOS nodes, as well as x86_64 and arm64 architectures. Additionally, It also supports various GPUs, including NVIDIA, Apple Metal, AMD, Ascend, Hygon and Moore Threads.</p>"},{"location":"faq/#distributed-inference-support","title":"Distributed Inference Support","text":"<p>Single-Node Multi-GPU</p> <ul> <li> llama-box (Image Generation models are not supported)</li> <li> vLLM</li> <li> MindIE</li> <li> vox-box</li> </ul> <p>Multi-Node Multi-GPU</p> <ul> <li> llama-box</li> <li> vLLM</li> <li> MindIE</li> </ul> <p>Heterogeneous-Node Multi-GPU</p> <ul> <li> llama-box</li> </ul> <p>Tip</p> <p>Related documentations:</p> <p>vLLM\uff1aDistributed Inference and Serving</p> <p>llama-box\uff1aDistributed LLM inference with llama.cpp</p>"},{"location":"faq/#installation","title":"Installation","text":""},{"location":"faq/#how-can-i-change-the-default-gpustack-port","title":"How can I change the default GPUStack port?","text":"<p>By default, the GPUStack server uses port 80. You can change it using the following method:</p> <p>Script Installation</p> <ul> <li>Linux</li> </ul> <pre><code>sudo vim /etc/systemd/system/gpustack.service\n</code></pre> <p>Add the <code>--port</code> parameter:</p> <pre><code>ExecStart=/root/.local/bin/gpustack start --port 9090\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo vim /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <p>Add the <code>--port</code> parameter:</p> <pre><code>  &lt;array&gt;\n    &lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n    &lt;string&gt;--port&lt;/string&gt;\n    &lt;string&gt;9090&lt;/string&gt;\n  &lt;/array&gt;\n</code></pre> <p>Save and start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <pre><code>nssm edit GPUStack\n</code></pre> <p>Add parameter after <code>start</code>:</p> <pre><code>start --port 9090\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>Add the <code>--port</code> parameter at the end of the <code>docker run</code> command, as shown below:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --port 9090\n</code></pre> <p>If the host network is not used, only the mapped host port needs to be modified:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    -p 9090:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --worker-ip your_host_ip\n</code></pre> <p>pip Installation</p> <p>Add the <code>--port</code> parameter at the end of the <code>gpustack start</code>:</p> <pre><code>gpustack start --port 9090\n</code></pre>"},{"location":"faq/#how-can-i-change-the-registered-worker-name","title":"How can I change the registered worker name?","text":"<p>You can set it to a custom name using the <code>--worker-name</code> parameter when running GPUStack:</p> <p>Script Installation</p> <ul> <li>Linux</li> </ul> <pre><code>sudo vim /etc/systemd/system/gpustack.service\n</code></pre> <p>Add the <code>--worker-name</code> parameter:</p> <pre><code>ExecStart=/root/.local/bin/gpustack start --worker-name New-Name\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo vim /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <p>Add the <code>--worker-name</code> parameter:</p> <pre><code>  &lt;array&gt;\n    &lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n    &lt;string&gt;--worker-name&lt;/string&gt;\n    &lt;string&gt;New-Name&lt;/string&gt;\n  &lt;/array&gt;\n</code></pre> <p>Save and start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <pre><code>nssm edit GPUStack\n</code></pre> <p>Add parameter after <code>start</code>:</p> <pre><code>start --worker-name New-Name\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>Add the <code>--worker-name</code> parameter at the end of the <code>docker run</code> command, as shown below:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --worker-name New-Name\n</code></pre> <p>pip Installation</p> <p>Add the <code>--worker-name</code> parameter at the end of the <code>gpustack start</code>:</p> <pre><code>gpustack start --worker-name New-Name\n</code></pre>"},{"location":"faq/#how-can-i-change-the-registered-worker-ip","title":"How can I change the registered worker IP?","text":"<p>You can set it to a custom IP using the <code>--worker-ip</code> parameter when running GPUStack:</p> <p>Script Installation</p> <ul> <li>Linux</li> </ul> <pre><code>sudo vim /etc/systemd/system/gpustack.service\n</code></pre> <p>Add the <code>--worker-ip</code> parameter:</p> <pre><code>ExecStart=/root/.local/bin/gpustack start --worker-ip xx.xx.xx.xx\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo vim /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <p>Add the <code>--worker-ip</code> parameter:</p> <pre><code>  &lt;array&gt;\n    &lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n    &lt;string&gt;--worker-ip&lt;/string&gt;\n    &lt;string&gt;xx.xx.xx.xx&lt;/string&gt;\n  &lt;/array&gt;\n</code></pre> <p>Save and start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <pre><code>nssm edit GPUStack\n</code></pre> <p>Add parameter after <code>start</code>:</p> <pre><code>start --worker-ip xx.xx.xx.xx\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>Add the <code>--worker-ip</code> parameter at the end of the <code>docker run</code> command, as shown below:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --worker-ip xx.xx.xx.xx\n</code></pre> <p>pip Installation</p> <p>Add the <code>--worker-ip</code> parameter at the end of the <code>gpustack start</code>:</p> <pre><code>gpustack start --worker-ip xx.xx.xx.xx\n</code></pre>"},{"location":"faq/#where-are-gpustacks-data-stored","title":"Where are GPUStack's data stored?","text":"<p>Script Installation</p> <ul> <li>Linux</li> </ul> <p>The default path is as follows:</p> <pre><code>/var/lib/gpustack\n</code></pre> <p>You can set it to a custom path using the <code>--data-dir</code> parameter when running GPUStack:</p> <pre><code>sudo vim /etc/systemd/system/gpustack.service\n</code></pre> <p>Add the <code>--data-dir</code> parameter:</p> <pre><code>ExecStart=/root/.local/bin/gpustack start --data-dir /data/gpustack-data\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <p>The default path is as follows:</p> <pre><code>/var/lib/gpustack\n</code></pre> <p>You can set it to a custom path using the <code>--data-dir</code> parameter when running GPUStack:</p> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo vim /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <pre><code>  &lt;array&gt;\n    &lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n    &lt;string&gt;--data-dir&lt;/string&gt;\n    &lt;string&gt;/Users/gpustack/data/gpustack-data&lt;/string&gt;\n  &lt;/array&gt;\n</code></pre> <p>Save and start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <p>The default path is as follows:</p> <pre><code>\"$env:APPDATA\\gpustack\"\n</code></pre> <p>You can set it to a custom path using the <code>--data-dir</code> parameter when running GPUStack:</p> <pre><code>nssm edit GPUStack\n</code></pre> <p>Add parameter after <code>start</code>:</p> <pre><code>start --data-dir D:\\gpustack-data\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>When running the GPUStack container, the Docker volume is mounted using <code>-v</code> parameter. The default data path is under the Docker data directory, specifically in the volumes subdirectory, and the default path is:</p> <pre><code>/var/lib/docker/volumes/gpustack-data/_data\n</code></pre> <p>You can check it by the following method:</p> <pre><code>docker volume ls\ndocker volume inspect gpustack-data\n</code></pre> <p>If you need to change it to a custom path, modify the mount configuration when running container. For example, to mount the host directory <code>/data/gpustack</code>:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v /data/gpustack:/var/lib/gpustack  \\\n    gpustack/gpustack\n</code></pre> <p>pip Installation</p> <p>Add the <code>--data-dir</code> parameter at the end of the <code>gpustack start</code>:</p> <pre><code>gpustack start --data-dir /data/gpustack-data\n</code></pre>"},{"location":"faq/#where-are-model-files-stored","title":"Where are model files stored?","text":"<p>Script Installation</p> <ul> <li>Linux</li> </ul> <p>The default path is as follows:</p> <pre><code>/var/lib/gpustack/cache\n</code></pre> <p>You can set it to a custom path using the <code>--cache-dir</code> parameter when running GPUStack:</p> <pre><code>sudo vim /etc/systemd/system/gpustack.service\n</code></pre> <p>Add the <code>--cache-dir</code> parameter:</p> <pre><code>ExecStart=/root/.local/bin/gpustack start --cache-dir /data/model-cache\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <p>The default path is as follows:</p> <pre><code>/var/lib/gpustack/cache\n</code></pre> <p>You can set it to a custom path using the <code>--cache-dir</code> parameter when running GPUStack:</p> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo vim /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <pre><code>  &lt;array&gt;\n    &lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n    &lt;string&gt;--cache-dir&lt;/string&gt;\n    &lt;string&gt;/Users/gpustack/data/model-cache&lt;/string&gt;\n  &lt;/array&gt;\n</code></pre> <p>Save and start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <p>The default path is as follows:</p> <pre><code>\"$env:APPDATA\\gpustack\\cache\"\n</code></pre> <p>You can set it to a custom path using the <code>--cache-dir</code> parameter when running GPUStack:</p> <pre><code>nssm edit GPUStack\n</code></pre> <p>Add parameter after <code>start</code>:</p> <pre><code>start --cache-dir D:\\model-cache\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>When running the GPUStack container, the Docker volume is mounted using <code>-v</code> parameter. The default cache path is under the Docker data directory, specifically in the volumes subdirectory, and the default path is:</p> <pre><code>/var/lib/docker/volumes/gpustack-data/_data/cache\n</code></pre> <p>You can check it by the following method:</p> <pre><code>docker volume ls\ndocker volume inspect gpustack-data\n</code></pre> <p>If you need to change it to a custom path, modify the mount configuration when running container.</p> <p>Note: If the data directory is already mounted, the cache directory should not be mounted inside the data directory. You need to specify a different path using the <code>--cache-dir</code> parameter.</p> <p>For example, to mount the host directory <code>/data/model-cache</code>:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v /data/gpustack:/var/lib/gpustack  \\\n    -v /data/model-cache:/data/model-cache \\\n    gpustack/gpustack \\\n    --cache-dir /data/model-cache\n</code></pre> <p>pip Installation</p> <p>Add the <code>--cache-dir</code> parameter at the end of the <code>gpustack start</code>:</p> <pre><code>gpustack start --cache-dir /data/model-cache\n</code></pre>"},{"location":"faq/#what-parameters-can-i-set-when-starting-gpustack","title":"What parameters can I set when starting GPUStack?","text":"<p>Please refer to: gpustack start</p>"},{"location":"faq/#upgrade","title":"Upgrade","text":""},{"location":"faq/#how-can-i-upgrade-the-built-in-vllm","title":"How can I upgrade the built-in vLLM?","text":"<p>GPUStack supports multiple versions of inference backends. When deploying a model, you can specify the backend version in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Version</code> to use a newly released vLLM version. GPUStack will automatically create a virtual environment using pipx to install it:</p> <p></p> <p>If you still need to upgrade the built-in vLLM, you can upgrade vLLM on all worker nodes using the following method:</p> <p>Script Installation</p> <pre><code>pipx runpip gpustack list | grep vllm\npipx runpip gpustack install -U vllm\n</code></pre> <p>Docker Installation</p> <pre><code>docker exec -it gpustack bash\npip list | grep vllm\npip install -U vllm\n</code></pre> <p>pip Installation</p> <pre><code>pip list | grep vllm\npip install -U vllm\n</code></pre>"},{"location":"faq/#how-can-i-upgrade-the-built-in-transformers","title":"How can I upgrade the built-in Transformers?","text":"<p>Script Installation</p> <pre><code>pipx runpip gpustack list | grep transformers\npipx runpip gpustack install -U transformers\n</code></pre> <p>Docker Installation</p> <pre><code>docker exec -it gpustack bash\npip list | grep transformers\npip install -U transformers\n</code></pre> <p>pip Installation</p> <pre><code>pip list | grep transformers\npip install -U transformers\n</code></pre>"},{"location":"faq/#how-can-i-upgrade-the-built-in-llama-box","title":"How can I upgrade the built-in llama-box?","text":"<p>GPUStack supports multiple versions of inference backends. When deploying a model, you can specify the backend version in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Version</code> to use a newly released llama-box version. GPUStack will automatically download and configure it:</p> <p></p> <p>If you are using distributed inference, you should upgrade llama-box on all worker nodes using the following method:</p> <p>Download a newly released llama-box binary from llama-box releases.</p> <p>And you need to stop the GPUStack first, then replace the binary, and finally restart the GPUStack. You can check the file location through some directories, for example:</p> <p>Script &amp; pip Installation</p> <pre><code>ps -ef | grep llama-box\n</code></pre> <p>Docker Installation</p> <pre><code>docker exec -it gpustack bash\nps -ef | grep llama-box\n</code></pre>"},{"location":"faq/#view-logs","title":"View Logs","text":""},{"location":"faq/#how-can-i-view-the-gpustack-logs","title":"How can I view the GPUStack logs?","text":"<p>The GPUStack logs provide information on the startup status, calculated model resource requirements, and more. Refer to the Troubleshooting for viewing the GPUStack logs.</p>"},{"location":"faq/#how-can-i-enable-debug-mode-in-gpustack","title":"How can I enable debug mode in GPUStack?","text":"<p>You can temporarily enable debug mode without interrupting the GPUStack service. Refer to the Troubleshooting for guidance.</p> <p>If you want to enable debug mode persistently, both server and worker can add the <code>--debug</code> parameter when running GPUStack:</p> <p>Script Installation</p> <ul> <li>Linux</li> </ul> <pre><code>sudo vim /etc/systemd/system/gpustack.service\n</code></pre> <pre><code>ExecStart=/root/.local/bin/gpustack start --debug\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo vim /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <pre><code>  &lt;array&gt;\n    &lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n    &lt;string&gt;--debug&lt;/string&gt;\n  &lt;/array&gt;\n</code></pre> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <pre><code>nssm edit GPUStack\n</code></pre> <p>Add parameter after <code>start</code>:</p> <pre><code>start --debug\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>Add the <code>--debug</code> parameter at the end of the <code>docker run</code> command, as shown below:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --debug\n</code></pre> <p>pip Installation</p> <p>Add the <code>--debug</code> parameter at the end of the <code>gpustack start</code>:</p> <pre><code>gpustack start --debug\n</code></pre>"},{"location":"faq/#how-can-i-view-the-rpc-server-logs","title":"How can I view the RPC server logs?","text":"<p>RPC Server is used for distributed inference of GGUF models. If the model starts abnormally or if there are issues with distributed inference, you can check the RPC Server logs on the corresponding node:</p> <p>Script Installation</p> <ul> <li>Linux &amp; macOS</li> </ul> <p>The default path is as follows. If the <code>--data-dir</code> or <code>--log-dir</code> parameters are set, please modify it to the actual path you have configured:</p> <pre><code>tail -200f /var/lib/gpustack/log/rpc_server/gpu-0.log\n</code></pre> <p>Each GPU corresponds to an RPC Server. For other GPU indices, modify it to the actual index:</p> <pre><code>tail -200f /var/lib/gpustack/log/rpc_server/gpu-n.log\n</code></pre> <ul> <li>Windows</li> </ul> <p>The default path is as follows. If the <code>--data-dir</code> or <code>--log-dir</code> parameters are set, please modify it to the actual path you have configured:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\rpc_server\\gpu-0.log\" -Tail 200 -Wait\n</code></pre> <p>Each GPU corresponds to an RPC Server. For other GPU indices, modify it to the actual index:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\rpc_server\\gpu-n.log\" -Tail 200 -Wait\n</code></pre> <p>Docker Installation</p> <p>The default path is as follows. If the <code>--data-dir</code> or <code>--log-dir</code> parameters are set, please modify it to the actual path you have configured:</p> <pre><code>docker exec -it gpustack tail -200f /var/lib/gpustack/log/rpc_server/gpu-0.log\n</code></pre> <p>Each GPU corresponds to an RPC Server. For other GPU indices, modify it to the actual index:</p> <pre><code>docker exec -it gpustack tail -200f /var/lib/gpustack/log/rpc_server/gpu-n.log\n</code></pre>"},{"location":"faq/#where-are-the-model-logs-stored","title":"Where are the model logs stored?","text":"<p>The model instance logs are stored in the <code>/var/lib/gpustack/log/serve/</code> directory of the corresponding worker node or worker container, with the log file named <code>id.log</code>, where id is the model instance ID. If the <code>--data-dir</code> or <code>--log-dir</code> parameter is set, the logs will be stored in the actual path specified by the parameter.</p>"},{"location":"faq/#how-can-i-enable-the-backend-debug-mode","title":"How can I enable the backend debug mode?","text":"<p>llama-box backend (GGUF models)</p> <p>Add the <code>--verbose</code> parameter in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Parameters</code> and recreate the model instance:</p> <p></p> <p>vLLM backends (Safetensors models)</p> <p>Add the <code>VLLM_LOGGING_LEVEL=DEBUG</code> environment variable in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Environment Variables</code> and recreate the model instance:</p> <p></p>"},{"location":"faq/#managing-workers","title":"Managing Workers","text":""},{"location":"faq/#what-should-i-do-if-the-worker-is-stuck-in-unreachable-state","title":"What should I do if the worker is stuck in <code>Unreachable</code> state?","text":"<p>Try accessing the URL shown in the error from the server. If the server is running in container, you need to enter the server container to execute the command:</p> <pre><code>curl http://10.10.10.1:10150/healthz\n</code></pre>"},{"location":"faq/#what-should-i-do-if-the-worker-is-stuck-in-notready-state","title":"What should I do if the worker is stuck in <code>NotReady</code> state?","text":"<p>Check the GPUStack logs on the corresponding worker here. If there are no abnormalities in the logs, verify that the time zones and system clocks are consistent across all nodes.</p>"},{"location":"faq/#detect-gpus","title":"Detect GPUs","text":""},{"location":"faq/#why-did-it-fail-to-detect-the-ascend-npus","title":"Why did it fail to detect the Ascend NPUs?","text":"<p>Check if <code>npu-smi</code> can be executed in the container:</p> <pre><code>docker exec -it gpustack bash\nnpu-smi info\n</code></pre> <p>When the following error occurs, it indicates that other containers are also mounting the NPU device, and sharing is not supported:</p> <pre><code>dcmi model initialized failed, because the device is used. ret is -8020\n</code></pre> <p>Check if any containers on the host have mounted NPU devices:</p> <pre><code>if [ $(docker ps | wc -l) -gt 1 ]; then docker ps | grep -v CONT | awk '{print $1}' | xargs docker inspect --format='{{printf \"%.5s\" .ID}} {{range .HostConfig.Devices}}{{.PathOnHost}} {{end}}' | sort -k2; fi; echo ok\n</code></pre> <p>Only mount NPUs that are not mounted by other containers, specify them using the <code>--device</code>:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci4 \\\n    --device /dev/davinci5 \\\n    --device /dev/davinci6 \\\n    --device /dev/davinci7 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu\n</code></pre>"},{"location":"faq/#managing-models","title":"Managing Models","text":""},{"location":"faq/#how-can-i-deploy-the-model","title":"How can I deploy the model?","text":""},{"location":"faq/#how-can-i-deploy-the-model-from-hugging-face","title":"How can I deploy the model from Hugging Face?","text":"<p>To deploy models from Hugging Face, the server node and the worker nodes where the model instances are scheduled must have access to Hugging Face, or you can use a mirror.</p> <p>For example, configure the <code>hf-mirror.com</code> mirror:</p> <p>Script Installation</p> <ul> <li>Linux</li> </ul> <p>Create or edit <code>/etc/default/gpustack</code> on all nodes , add the <code>HF_ENDPOINT</code> environment variable to use <code>https://hf-mirror.com</code> as the Hugging Face mirror:</p> <pre><code>vim /etc/default/gpustack\n</code></pre> <pre><code>HF_ENDPOINT=https://hf-mirror.com\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>systemctl restart gpustack\n</code></pre> <p>Docker Installation</p> <p>Add the <code>HF_ENDPOINT</code> environment variable when running container, as shown below:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    -e HF_ENDPOINT=https://hf-mirror.com \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack\n</code></pre> <p>pip Installation</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com gpustack start\n</code></pre>"},{"location":"faq/#how-can-i-deploy-the-model-from-local-path","title":"How can I deploy the model from Local Path?","text":"<p>When deploying models from Local Path, it is recommended to upload the model files to each node and maintain the same absolute path. Alternatively, the model instance should be manually scheduled to nodes that have the model files via manual scheduling or label selection. Another option is to mount a shared storage across multiple nodes.</p> <p>When deploying GGUF models from Local Path, the path must point to the absolute path of the <code>.gguf</code> file. For sharded model files, use the absolute path of the first <code>.gguf</code> file (00001). If using container installation, the model files must be mounted into the container, and the path should point to the container\u2019s path, not the host\u2019s path.</p> <p>When deploying Safetensors models from Local Path, the path must point to the absolute path of the model directory which contain <code>*.safetensors</code>, <code>config.json</code>, and other files. If using container installation, the model files must be mounted into the container, and the path should point to the container\u2019s path, not the host\u2019s path.</p> <p></p>"},{"location":"faq/#how-can-i-deploy-a-locally-downloaded-ollama-model","title":"How can I deploy a locally downloaded Ollama model?","text":"<p>Use the following command to find the full path of the model file and deploy it via the Local Path. The example below uses <code>deepseek-r1:14b-qwen-distill-q4_K_M</code>, be sure to replace it with your actual Ollama model name:</p> <pre><code>ollama show deepseek-r1:14b-qwen-distill-q4_K_M --modelfile | grep FROM | grep blobs | sed 's/^FROM[[:space:]]*//'\n</code></pre> <p></p>"},{"location":"faq/#what-should-i-do-if-the-model-is-stuck-in-pending-state","title":"What should I do if the model is stuck in <code>Pending</code> state?","text":"<p><code>Pending</code> means that there are currently no workers meeting the model\u2019s requirements, move the mouse over the <code>Pending</code> status to view the reason.</p> <p>First, check the <code>Resources</code>-<code>Workers</code> section to ensure that the worker status is Ready.</p> <p>Then, for different backends:</p> <ul> <li>llama-box</li> </ul> <p>llama-box uses the GGUF Parser to calculate the model\u2019s memory requirements. You need to ensure that the allocatable memory is greater than the calculated memory requirements of the model. Note that even if other models are in an <code>Error</code> or <code>Downloading</code> state, the GPU memory has already been allocated. If you are unsure how much GPU memory the model requires, you can use the GGUF Parser to calculate it.</p> <p>The context size for the model also affects the required GPU memory. You can adjust the <code>--ctx-size</code> parameter to set a smaller context. In GPUStack, if this parameter is not set, its default value is <code>8192</code>. If it is specified in the backend parameters, the actual setting will take effect.</p> <p>You can adjust it to a smaller context in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Parameters</code> as needed, for example, <code>--ctx-size=2048</code>. However, keep in mind that the max tokens for each inference request is influenced by both the <code>--ctx-size</code> and <code>--parallel</code> parameters: <code>max tokens = context size / parallel</code></p> <p>The default value of <code>--parallel</code> is <code>4</code>, so in this case, the max tokens would be <code>512</code>. If the token count exceeds the max tokens, the inference output will be truncated.</p> <p>On the other hand, the <code>--parallel</code> parameter represents the number of parallel sequences to decode, which can roughly be considered as a setting for the model\u2019s concurrent request handling.</p> <p>Therefore, it is important to appropriately set the <code>--ctx-size</code> and <code>--parallel</code> parameters, ensuring that the max tokens for a single request is within the limits and that the available GPU memory can support the specified context size.</p> <p>If you need to align with Ollama\u2019s configuration, you can refer to the following examples:</p> <p>Set the following parameters in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Parameters</code>:</p> <pre><code>--ctx-size=8192\n--parallel=4\n</code></pre> <p>If your GPU memory is insufficient, try launching with a lower configuration:</p> <pre><code>--ctx-size=2048\n--parallel=1\n</code></pre> <ul> <li>vLLM</li> </ul> <p>vLLM requires that all GPUs have more than 90% of their memory available by default (controlled by the <code>--gpu-memory-utilization</code> parameter). Ensure that there is enough allocatable GPU memory exceeding 90%. Note that even if other models are in an <code>Error</code> or <code>Downloading</code> state, the GPU memory has already been allocated.</p> <p>If all GPUs have more than 90% available memory but still show <code>Pending</code>, it indicates insufficient memory. For <code>safetensors</code> models in BF16 format, the required GPU memory (GB) can be estimated as:</p> <pre><code>GPU Memory (GB) = Number of Parameters (B) * 2 * 1.2 + 2\n</code></pre> <p>If the allocatable GPU memory is less than 90%, but you are sure the model can run with a lower allocation, you can adjust the <code>--gpu-memory-utilization</code> parameter. For example, add <code>--gpu-memory-utilization=0.5</code> in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Parameters</code> to allocate 50% of the GPU memory.</p> <p>Note: If the model encounters an error after running and the logs show <code>CUDA: out of memory</code>, it means the allocated GPU memory is insufficient. You will need to further adjust <code>--gpu-memory-utilization</code>, add more resources, or deploy a smaller model.</p> <p>The context size for the model also affects the required GPU memory. You can adjust the <code>--max-model-len</code> parameter to set a smaller context. In GPUStack, if this parameter is not set, its default value is 8192. If it is specified in the backend parameters, the actual setting will take effect.</p> <p>You can adjust it to a smaller context as needed, for example, <code>--max-model-len=2048</code>. However, keep in mind that the max tokens for each inference request cannot exceed the value of <code>--max-model-len</code>. Therefore, setting a very small context may cause inference truncation.</p> <p>The <code>--enforce-eager</code> parameter also helps reduce GPU memory usage. However, this parameter in vLLM forces the model to execute in eager execution mode, meaning that operations are executed immediately as they are called, rather than being deferred for optimization in graph-based execution (like in lazy execution). This can make the execution slower but easier to debug. However, it can also reduce performance due to the lack of optimizations provided by graph execution.</p>"},{"location":"faq/#what-should-i-do-if-the-model-is-stuck-in-scheduled-state","title":"What should I do if the model is stuck in <code>Scheduled</code> state?","text":"<p>Try restarting the GPUStack service where the model is scheduled. If the issue persists, check the worker logs here to analyze the cause.</p>"},{"location":"faq/#what-should-i-do-if-the-model-is-stuck-in-error-state","title":"What should I do if the model is stuck in <code>Error</code> state?","text":"<p>Move the mouse over the <code>Error</code> status to view the reason. If there is a <code>View More</code> button, click it to check the error messages in the model logs and analyze the cause of the error.</p>"},{"location":"faq/#how-can-i-resolve-the-error-so-cannot-open-shared-object-file-no-such-file-or-directory","title":"How can I resolve the error *.so: cannot open shared object file: No such file or directory?","text":"<p>If the error occurs during model startup indicating that any <code>.so</code> file cannot be opened, for example:</p> <pre><code>llama-box: error while loading shared libraries: libcudart.so.12: cannot open shared object file: No such file or directory\n</code></pre> <p>The cause is that GPUStack doesn\u2019t recognize the <code>LD_LIBRARY_PATH</code> environment variable, which may be due to a missing environment variable or unconfigured toolkits (such as CUDA, CANN, etc.) during GPUStack installation.</p> <p>To check if the environment variable is set:</p> <pre><code>echo $LD_LIBRARY_PATH\n</code></pre> <p>If not configured, here\u2019s an example configuration for CUDA.</p> <p>Ensure that the <code>nvidia-smi</code> is executable and the NVIDIA driver version is <code>550</code> or later:</p> <pre><code>nvidia-smi\n</code></pre> <p>Configure the CUDA environment variables. If not installed, install <code>CUDA 12.4</code> or later:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/targets/x86_64-linux/lib\nexport PATH=$PATH:/usr/local/cuda/bin\necho $LD_LIBRARY_PATH\necho $PATH\n</code></pre> <p>Create or edit <code>/etc/default/gpustack</code> , add the <code>PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables:</p> <pre><code>vim /etc/default/gpustack\n</code></pre> <pre><code>LD_LIBRARY_PATH=......\nPATH=......\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>systemctl restart gpustack\n</code></pre>"},{"location":"faq/#why-did-it-fail-to-load-the-model-when-using-the-local-path","title":"Why did it fail to load the model when using the local path?","text":"<p>When deploying a model using Local Path and encountering a <code>failed to load model</code> error, you need to check whether the model files exist on the node that the model instance is scheduled to, and if the absolute path is correct.</p> <p>For GGUF models, you need to specify the absolute path to the <code>.gguf</code> file. For sharded models, use the absolute path to the first <code>.gguf</code> file (typically 00001).</p> <p>If using Docker installation, the model files must be mounted into the container. Make sure the path you provide is the one inside the container, not the host path.</p> <p></p>"},{"location":"faq/#why-doesnt-deleting-a-model-free-up-disk-space","title":"Why doesn\u2019t deleting a model free up disk space?","text":"<p>This is to avoid re-downloading the model when redeploying. You need to clean it up in <code>Resources</code> \u2192 <code>Model Files</code> manually.</p>"},{"location":"faq/#why-does-each-gpu-have-a-llama-box-process-by-default","title":"Why does each GPU have a llama-box process by default?","text":"<p>This process is the RPC server used for llama-box\u2019s distributed inference. If you are sure that you do not need distributed inference with llama-box, you can disable the RPC server service by adding the <code>--disable-rpc-servers</code> parameter when running GPUStack.</p>"},{"location":"faq/#backend-parameters","title":"Backend Parameters","text":""},{"location":"faq/#how-can-i-know-the-purpose-of-the-backend-parameters","title":"How can I know the purpose of the backend parameters?","text":"<ul> <li> <p>llama-box</p> </li> <li> <p>vLLM</p> </li> <li> <p>MindIE</p> </li> </ul>"},{"location":"faq/#how-can-i-set-the-models-context-length","title":"How can I set the model\u2019s context length?","text":"<p>llama-box backend (GGUF models)</p> <p>GPUStack sets the default context length for models to 8K. You can customize the context length using the <code>--ctx-size</code> parameter, but it cannot exceed the model\u2019s maximum context length:</p> <p></p> <p>If editing, save the change and then recreate the model instance to take effect.</p> <p>vLLM backend (Safetensors models)</p> <p>GPUStack sets the default context length for models to 8K. You can customize the context length using the <code>--max-model-len</code> parameter, but it cannot exceed the model\u2019s maximum context length:</p> <p></p> <p>MindIE backend (Safetensors models)</p> <p>GPUStack sets the default context length for models to 8K. You can customize the context length using the <code>--max-seq-len</code> parameter, but it cannot exceed the model\u2019s maximum context length:</p> <p></p> <p>If editing, save the change and then recreate the model instance to take effect.</p>"},{"location":"faq/#using-models","title":"Using Models","text":""},{"location":"faq/#using-vision-language-models","title":"Using Vision Language Models","text":""},{"location":"faq/#how-can-i-resolve-the-error-at-most-1-images-may-be-provided-in-one-request","title":"How can I resolve the error At most 1 image(s) may be provided in one request?","text":"<p>This is a limitation of vLLM. You can adjust the <code>--limit-mm-per-prompt</code> parameter in <code>Edit Model</code> \u2192 <code>Advanced</code> \u2192 <code>Backend Parameters</code> as needed. For example, <code>--limit-mm-per-prompt=image=4</code> means that it supports up to 4 images per inference request, see the details here.</p>"},{"location":"faq/#managing-gpustack","title":"Managing GPUStack","text":""},{"location":"faq/#how-can-i-manage-the-gpustack-service","title":"How can I manage the GPUStack service?","text":"<p>Script Installation</p> <ul> <li>Linux</li> </ul> <p>Stop GPUStack:</p> <pre><code>sudo systemctl stop gpustack\n</code></pre> <p>Start GPUStack:</p> <pre><code>sudo systemctl start gpustack\n</code></pre> <p>Restart GPUStack:</p> <pre><code>sudo systemctl restart gpustack\n</code></pre> <ul> <li>macOS</li> </ul> <p>Stop GPUStack:</p> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <p>Start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <p>Restart GPUStack:</p> <pre><code>sudo launchctl bootout system /Library/LaunchDaemons/ai.gpustack.plist\nsudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <ul> <li>Windows</li> </ul> <p>Run PowerShell as administrator (avoid using PowerShell ISE).</p> <p>Stop GPUStack:</p> <pre><code>Stop-Service -Name \"GPUStack\"\n</code></pre> <p>Start GPUStack:</p> <pre><code>Start-Service -Name \"GPUStack\"\n</code></pre> <p>Restart GPUStack:</p> <pre><code>Restart-Service -Name \"GPUStack\"\n</code></pre> <p>Docker Installation</p> <p>Restart GPUStack container:</p> <pre><code>docker restart gpustack\n</code></pre>"},{"location":"faq/#how-do-i-use-gpustack-behind-a-proxy","title":"How do I use GPUStack behind a proxy?","text":"<p>Script Installation</p> <ul> <li>Linux &amp; macOS</li> </ul> <p>Create or edit <code>/etc/default/gpustack</code> and add the proxy configuration:</p> <pre><code>vim /etc/default/gpustack\n</code></pre> <pre><code>http_proxy=\"http://username:password@proxy-server:port\"\nhttps_proxy=\"http://username:password@proxy-server:port\"\nall_proxy=\"socks5://username:password@proxy-server:port\"\nno_proxy=\"localhost,127.0.0.1,192.168.0.0/24,172.16.0.0/16,10.0.0.0/8\"\n</code></pre> <p>Save and restart GPUStack:</p> <pre><code>systemctl restart gpustack\n</code></pre> <p>Docker Installation</p> <p>Pass environment variables when running GPUStack:</p> <pre><code>docker run -e http_proxy=\"http://username:password@proxy-server:port\" \\\n           -e https_proxy=\"http://username:password@proxy-server:port\" \\\n           -e all_proxy=\"socks5://username:password@proxy-server:port\" \\\n           -e no_proxy=\"localhost,127.0.0.1,192.168.0.0/24,172.16.0.0/16,10.0.0.0/8\" \\\n           \u2026\u2026\n</code></pre>"},{"location":"overview/","title":"GPUStack","text":"<p>GPUStack is an open-source GPU cluster manager for running AI models.</p>"},{"location":"overview/#key-features","title":"Key Features","text":"<ul> <li>Broad Hardware Compatibility: Run with different brands of GPUs in Apple Macs, Windows PCs, and Linux servers.</li> <li>Broad Model Support: From LLMs to diffusion models, audio, embedding, and reranker models.</li> <li>Scales with Your GPU Inventory: Easily add more GPUs or nodes to scale up your operations.</li> <li>Distributed Inference: Supports both single-node multi-GPU and multi-node inference and serving.</li> <li>Multiple Inference Backends: Supports llama-box (llama.cpp &amp; stable-diffusion.cpp), vox-box and vLLM as the inference backends.</li> <li>Lightweight Python Package: Minimal dependencies and operational overhead.</li> <li>OpenAI-compatible APIs: Serve APIs that are compatible with OpenAI standards.</li> <li>User and API key management: Simplified management of users and API keys.</li> <li>GPU metrics monitoring: Monitor GPU performance and utilization in real-time.</li> <li>Token usage and rate metrics: Track token usage and manage rate limits effectively.</li> </ul>"},{"location":"overview/#supported-platforms","title":"Supported Platforms","text":"<ul> <li> macOS</li> <li> Windows</li> <li> Linux</li> </ul>"},{"location":"overview/#supported-accelerators","title":"Supported Accelerators","text":"<ul> <li> NVIDIA CUDA (Compute Capability 6.0 and above)</li> <li> Apple Metal (M-series chips)</li> <li> AMD ROCm</li> <li> Ascend CANN</li> <li> Hygon DTK</li> <li> Moore Threads MUSA</li> </ul> <p>We plan to support the following accelerators in future releases.</p> <ul> <li> Intel oneAPI</li> <li> Qualcomm AI Engine</li> </ul>"},{"location":"overview/#supported-models","title":"Supported Models","text":"<p>GPUStack uses llama-box (bundled llama.cpp and stable-diffusion.cpp server), vLLM and vox-box as the backends and supports a wide range of models. Models from the following sources are supported:</p> <ol> <li> <p>Hugging Face</p> </li> <li> <p>ModelScope</p> </li> <li> <p>Ollama Library</p> </li> <li> <p>Local File Path</p> </li> </ol>"},{"location":"overview/#example-models","title":"Example Models:","text":"Category Models Large Language Models(LLMs) Qwen, LLaMA, Mistral, DeepSeek, Phi, Gemma Vision Language Models(VLMs) Llama3.2-Vision, Pixtral , Qwen2.5-VL, LLaVA, InternVL2.5 Diffusion Models Stable Diffusion, FLUX Embedding Models BGE, BCE, Jina Reranker Models BGE, BCE, Jina Audio Models Whisper (Speech-to-Text), CosyVoice (Text-to-Speech) <p>For full list of supported models, please refer to the supported models section in the inference backends documentation.</p>"},{"location":"overview/#openai-compatible-apis","title":"OpenAI-Compatible APIs","text":"<p>GPUStack serves OpenAI compatible APIs. For details, please refer to OpenAI Compatible APIs</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation-script","title":"Installation Script","text":"LinuxmacOSWindows <p>GPUStack provides a script to install it as a systemd service on Linux with default port 80. To install GPUStack using this method, just run:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>GPUStack provides a script to install it as a launchd service on macOS with default port 80. To install GPUStack using this method, just run:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>Run PowerShell as administrator (avoid using PowerShell ISE), then run the following command to install GPUStack with default port 80:</p> <pre><code>Invoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre>"},{"location":"quickstart/#other-installation-methods","title":"Other Installation Methods","text":"<p>For Docker installation, pip installation or detailed configuration options, please refer to the Installation Documentation.</p>"},{"location":"quickstart/#getting-started","title":"Getting Started","text":"<ol> <li>Run and chat with the llama3.2 model:</li> </ol> <pre><code>gpustack chat llama3.2 \"tell me a joke.\"\n</code></pre> <ol> <li>Run and generate an image with the stable-diffusion-v3-5-large-turbo model:</li> </ol> <p>Tip</p> <p>This command downloads the model (~12GB) from Hugging Face. The download time depends on your network speed. Ensure you have enough disk space and VRAM (12GB) to run the model. If you encounter issues, you can skip this step and move to the next one.</p> <pre><code>gpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \\\n\"A minion holding a sign that says 'GPUStack'. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors.\" \\\n--sample-steps 5 --show\n</code></pre> <p>Once the command completes, the generated image will appear in the default viewer. You can experiment with the prompt and CLI options to customize the output.</p> <p></p> <ol> <li>Open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</li> </ol> LinuxmacOSWindows <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> <ol> <li>Click <code>Playground - Chat</code> in the navigation menu. Now you can chat with the LLM in the UI playground.</li> </ol> <p></p> <ol> <li> <p>Click <code>API Keys</code> in the navigation menu, then click the <code>New API Key</code> button.</p> </li> <li> <p>Fill in the <code>Name</code> and click the <code>Save</code> button.</p> </li> <li> <p>Copy the generated API key and save it somewhere safe. Please note that you can only see it once on creation.</p> </li> <li> <p>Now you can use the API key to access the OpenAI-compatible API. For example, use curl as the following:</p> </li> </ol> <pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"quickstart/#cleanup","title":"Cleanup","text":"<p>After you complete using the deployed models, you can go to the <code>Models</code> page in the GPUStack UI and delete the models to free up resources.</p>"},{"location":"scheduler/","title":"Scheduler","text":""},{"location":"scheduler/#summary","title":"Summary","text":"<p>The scheduler's primary responsibility is to calculate the resources required by models instance and to evaluate and select the optimal workers/GPUs for model instances through a series of strategies. This ensures that model instances can run efficiently. This document provides a detailed overview of the policies and processes used by the scheduler.</p>"},{"location":"scheduler/#scheduling-process","title":"Scheduling Process","text":""},{"location":"scheduler/#filtering-phase","title":"Filtering Phase","text":"<p>The filtering phase aims to narrow down the available workers or GPUs to those that meet specific criteria. The main policies involved are:</p> <ul> <li>Label Matching Policy</li> <li>Status Policy</li> <li>Resource Fit Policy</li> </ul>"},{"location":"scheduler/#label-matching-policy","title":"Label Matching Policy","text":"<p>This policy filters workers based on the label selectors configured for the model. If no label selectors are defined for the model, all workers are considered. Otherwise, the system checks whether the labels of each worker node match the model's label selectors, retaining only those workers that match.</p>"},{"location":"scheduler/#status-policy","title":"Status Policy","text":"<p>This policy filters workers based on their status, retaining only those that are in a READY state.</p>"},{"location":"scheduler/#resource-fit-policy","title":"Resource Fit Policy","text":"<p>The Resource Fit Policy is a critical strategy in the scheduling system, used to filter workers or GPUs based on resource compatibility. The goal of this policy is to ensure that model instances can run on the selected nodes without exceeding resource limits. The Resource Fit Policy prioritizes candidates in the following order:</p> <ul> <li>Single Worker Node, Single GPU Full Offload: Identifies candidates where a single GPU on a single worker can fully offload the model, which usually offers the best performance.</li> <li>Single Worker Node, Multiple GPU Full Offload: Identifies candidates where multiple GPUs on a single worker can fully the offload the model.</li> <li>Single Worker Node Partial Offload: Identifies candidates on a single worker that can handle a partial offload, used only when partial offloading is allowed.</li> <li>Distributed Inference Across Multiple Workers: Identifies candidates where a combination of GPUs across multiple workers can handle full or partial offloading, used only when distributed inference across nodes is permitted.</li> <li>Single Worker Node, CPU: When no GPUs are available, the system will use the CPU for inference, identifying candidates where memory resources on a single worker are sufficient.</li> </ul>"},{"location":"scheduler/#scoring-phase","title":"Scoring Phase","text":"<p>The scoring phase evaluates the filtered candidates, scoring them to select the optimal deployment location. The primary strategy involved is:</p> <ul> <li>Placement Strategy Policy</li> </ul>"},{"location":"scheduler/#placement-strategy-policy","title":"Placement Strategy Policy","text":"<ul> <li>Binpack</li> </ul> <p>This strategy aims to \"pack\" as many model instances as possible into the fewest number of \"bins\" (e.g., Workers/GPUs) to optimize resource utilization. The goal is to minimize the number of bins used while maximizing resource efficiency, ensuring each bin is filled as efficiently as possible without exceeding its capacity. Model instances are placed in the bin with the least remaining space to minimize leftover capacity in each bin.</p> <ul> <li>Spread</li> </ul> <p>This strategy seeks to distribute multiple model instances across different worker nodes as evenly as possible, improving system fault tolerance and load balancing.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#view-gpustack-logs","title":"View GPUStack Logs","text":"<p>If you installed GPUStack using the installation script or Docker, you can view GPUStack logs with the following commands for the default setup:</p> LinuxmacOSWindowsDocker <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre> <pre><code>docker logs -f gpustack\n</code></pre>"},{"location":"troubleshooting/#configure-log-level","title":"Configure Log Level","text":"<p>You can enable the DEBUG log level for <code>gpustack start</code> by setting the <code>--debug</code> parameter.</p> <p>You can configure log level of the GPUStack server at runtime by running the following command on the server node:</p> <pre><code>curl -X PUT http://localhost/debug/log_level -d \"debug\"\n</code></pre> <p>The same applies to GPUStack workers:</p> <pre><code>curl -X PUT http://localhost:10150/debug/log_level -d \"debug\"\n</code></pre> <p>The available log levels are: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>critical</code>.</p>"},{"location":"troubleshooting/#reset-admin-password","title":"Reset Admin Password","text":"<p>In case you forgot the admin password, you can reset it by running the following command on the server node or inside the server container:</p> <pre><code>gpustack reset-admin-password\n</code></pre> <p>If the default port has been changed, specify the GPUStack URL using the <code>--server-url</code> parameter. It must be run locally on the server and accessed via <code>localhost</code>:</p> <pre><code>gpustack reset-admin-password --server-url http://localhost:9090\n</code></pre>"},{"location":"upgrade/","title":"Upgrade","text":"<p>You can upgrade GPUStack using the installation script or by manually installing the desired version of the GPUStack Python package.</p> <p>Note</p> <ol> <li>When upgrading, upgrade the GPUStack server first, then upgrade the workers.</li> <li>Please DO NOT upgrade from/to the main(dev) version or a release candidate(rc) version, as they may contain breaking changes. Use a fresh installation if you want to try the main or rc versions.</li> </ol>"},{"location":"upgrade/#upgrade-gpustack-using-the-installation-script","title":"Upgrade GPUStack Using the Installation Script","text":"<p>To upgrade GPUStack from an older version, re-run the installation script using the same configuration options you originally used.</p> <p>Running the installation script will:</p> <ol> <li>Install the latest version of the GPUStack Python package.</li> <li>Update the system service (systemd, launchd, or Windows) init script to reflect the arguments passed to the installation script.</li> <li>Restart the GPUStack service.</li> </ol>"},{"location":"upgrade/#linux-or-macos","title":"Linux or macOS","text":"<p>For example, to upgrade GPUStack to the latest version on a Linux system and macOS:</p> <pre><code>curl -sfL https://get.gpustack.ai | &lt;EXISTING_INSTALL_ENV&gt; sh -s - &lt;EXISTING_GPUSTACK_ARGS&gt;\n</code></pre> <p>Note</p> <p><code>&lt;EXISTING_INSTALL_ENV&gt;</code> are the environment variables you set during the initial installation, and <code>&lt;EXISTING_GPUSTACK_ARGS&gt;</code> are the startup parameters you configured back then.</p> <p>Simply execute the same installation command again, and the system will automatically perform an upgrade.</p> <p>To upgrade to a specific version, specify the <code>INSTALL_PACKAGE_SPEC</code> environment variable similar to the <code>pip install</code> command:</p> <pre><code>curl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack==x.y.z &lt;EXISTING_INSTALL_ENV&gt; sh -s - &lt;EXISTING_GPUSTACK_ARGS&gt;\n</code></pre>"},{"location":"upgrade/#windows","title":"Windows","text":"<p>To upgrade GPUStack to the latest version on a Windows system:</p> <pre><code>$env:&lt;EXISTING_INSTALL_ENV&gt; = &lt;EXISTING_INSTALL_ENV_VALUE&gt;\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>Note</p> <p><code>&lt;EXISTING_INSTALL_ENV&gt;</code> are the environment variables you set during the initial installation, and <code>&lt;EXISTING_GPUSTACK_ARGS&gt;</code> are the startup parameters you configured back then.</p> <p>Simply execute the same installation command again, and the system will automatically perform an upgrade.</p> <p>To upgrade to a specific version:</p> <pre><code>$env:INSTALL_PACKAGE_SPEC = gpustack==x.y.z\n$env:&lt;EXISTING_INSTALL_ENV&gt; = &lt;EXISTING_INSTALL_ENV_VALUE&gt;\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } &lt;EXISTING_GPUSTACK_ARGS&gt;\"\n</code></pre>"},{"location":"upgrade/#docker-upgrade","title":"Docker Upgrade","text":"<p>If you installed GPUStack using Docker, upgrade to the a new version by pulling the Docker image with the desired version tag.</p> <p>For example:</p> <pre><code>docker pull gpustack/gpustack:vX.Y.Z\n</code></pre> <p>Then restart the GPUStack service with the new image.</p>"},{"location":"upgrade/#pip-upgrade","title":"pip Upgrade","text":"<p>If you install GPUStack manually using pip, upgrade using the common <code>pip</code> workflow.</p> <p>For example, to upgrade GPUStack to the latest version:</p> <pre><code>pip install --upgrade gpustack\n</code></pre> <p>Then restart the GPUStack service according to your setup.</p>"},{"location":"cli-reference/chat/","title":"gpustack chat","text":"<p>Chat with a large language model.</p> <pre><code>gpustack chat model [prompt]\n</code></pre>"},{"location":"cli-reference/chat/#positional-arguments","title":"Positional Arguments","text":"Name Description model The model to use for chat. prompt The prompt to send to the model. [Optional]"},{"location":"cli-reference/chat/#one-time-chat-with-a-prompt","title":"One-time Chat with a Prompt","text":"<p>If a prompt is provided, it performs a one-time inference. For example:</p> <pre><code>gpustack chat llama3 \"tell me a joke.\"\n</code></pre> <p>Example output:</p> <pre><code>Why couldn't the bicycle stand up by itself?\n\nBecause it was two-tired!\n</code></pre>"},{"location":"cli-reference/chat/#interactive-chat","title":"Interactive Chat","text":"<p>If the <code>prompt</code> argument is not provided, you can chat with the large language model interactively. For example:</p> <pre><code>gpustack chat llama3\n</code></pre> <p>Example output:</p> <pre><code>&gt;tell me a joke.\nHere's one:\n\nWhy couldn't the bicycle stand up by itself?\n\n(wait for it...)\n\nBecause it was two-tired!\n\nHope that made you smile!\n&gt;Do you have a better one?\nHere's another one:\n\nWhy did the scarecrow win an award?\n\n(think about it for a sec...)\n\nBecause he was outstanding in his field!\n\nHope that one stuck with you!\n\nDo you want to hear another one?\n&gt;\\quit\n</code></pre>"},{"location":"cli-reference/chat/#interactive-commands","title":"Interactive Commands","text":"<p>Followings are available commands in interactive chat:</p> <pre><code>Commands:\n  \\q or \\quit - Quit the chat\n  \\c or \\clear - Clear chat context in prompt\n  \\? or \\h or \\help - Print this help message\n</code></pre>"},{"location":"cli-reference/chat/#connect-to-external-gpustack-server","title":"Connect to External GPUStack Server","text":"<p>If you are not running <code>gpustack chat</code> on the server node, or if you are serving on a custom host or port, you should provide the following environment variables:</p> Name Description GPUSTACK_SERVER_URL URL of the GPUStack server, e.g., <code>http://your_host_ip</code>. GPUSTACK_API_KEY GPUStack API key."},{"location":"cli-reference/download-tools/","title":"gpustack download-tools","text":"<p>Download dependency tools, including llama-box, gguf-parser, and fastfetch.</p> <pre><code>gpustack download-tools [OPTIONS]\n</code></pre>"},{"location":"cli-reference/download-tools/#configurations","title":"Configurations","text":"Flag Default Description <code>----tools-download-base-url</code> value (empty) Base URL to download dependency tools. <code>--save-archive</code> value (empty) Path to save downloaded tools as a tar archive. <code>--load-archive</code> value (empty) Path to load downloaded tools from a tar archive, instead of downloading. <code>--system</code> value Default is the current OS. Operating system to download tools for. Options: <code>linux</code>, <code>windows</code>, <code>macos</code>. <code>--arch</code> value Default is the current architecture. Architecture to download tools for. Options: <code>amd64</code>, <code>arm64</code>. <code>--device</code> value Default is the current device. Device to download tools for. Options: <code>cuda</code>, <code>mps</code>, <code>npu</code>, <code>musa</code>, <code>dcu</code>, <code>cpu</code>."},{"location":"cli-reference/draw/","title":"gpustack draw","text":"<p>Generate an image with a diffusion model.</p> <pre><code>gpustack draw [model] [prompt]\n</code></pre>"},{"location":"cli-reference/draw/#positional-arguments","title":"Positional Arguments","text":"Name Description model The model to use for image generation. prompt Text prompt to use for image generation. <p>The <code>model</code> can be either of the following:</p> <ol> <li>Name of a GPUStack model. You need to create a model in GPUStack before using it here.</li> <li>Reference to a Hugging Face GGUF diffusion model in Ollama style. When using this option, the model will be deployed if it is not already available. When not specified the default <code>Q4_0</code> tag is used. Examples:</li> </ol> <ul> <li><code>hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF</code></li> <li><code>hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:FP16</code></li> <li><code>hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf</code></li> </ul>"},{"location":"cli-reference/draw/#configurations","title":"Configurations","text":"Flag Default Description <code>--size</code> value <code>512x512</code> Size of the image to generate, specified as <code>widthxheight</code>. <code>--sampler</code> value <code>euler</code> Sampling method. Options include: euler_a, euler, heun, dpm2, dpm++2s_a, dpm++2m, lcm, etc. <code>--sample-steps</code> value (Empty) Number of sampling steps. <code>--cfg-scale</code> value (Empty) Classifier-free guidance scale for balancing prompt adherence and creativity. <code>--seed</code> value (Empty) Seed for random number generation. Useful for reproducibility. <code>--negative-prompt</code> value (Empty) Text prompt for what to avoid in the image. <code>--output</code> value (Empty) Path to save the generated image. <code>--show</code> <code>False</code> If True, opens the generated image in the default image viewer. <code>-d</code>, <code>--debug</code> <code>False</code> Enable debug mode."},{"location":"cli-reference/start/","title":"gpustack start","text":"<p>Run GPUStack server or worker.</p> <pre><code>gpustack start [OPTIONS]\n</code></pre>"},{"location":"cli-reference/start/#configurations","title":"Configurations","text":""},{"location":"cli-reference/start/#common-options","title":"Common Options","text":"Flag Default Description <code>--config-file</code> value (empty) Path to the YAML config file. <code>-d</code> value, <code>--debug</code> value <code>False</code> To enable debug mode, the short flag -d is not supported in Windows because this flag is reserved by PowerShell for CommonParameters. <code>--data-dir</code> value (empty) Directory to store data. Default is OS specific. <code>--cache-dir</code> value (empty) Directory to store cache (e.g., model files). Defaults to /cache. <code>-t</code> value, <code>--token</code> value Auto-generated. Shared secret used to add a worker. <code>--huggingface-token</code> value (empty) User Access Token to authenticate to the Hugging Face Hub. Can also be configured via the <code>HF_TOKEN</code> environment variable. <code>--ollama-library-base-url</code> value <code>https://registry.ollama.ai</code> Base URL for the Ollama library. <code>--enable-ray</code> <code>False</code> Enable Ray for running distributed vLLM across multiple workers. Only supported on Linux. <code>--ray-args</code> value (empty) Arguments to pass to Ray. Use <code>=</code> to avoid the CLI recognizing ray-args as a GPUStack argument. This can be used multiple times to pass a list of arguments. Example: <code>--ray-args=--port=6379 --ray-args=--verbose</code>. See Ray docs for more details."},{"location":"cli-reference/start/#server-options","title":"Server Options","text":"Flag Default Description <code>--host</code> value <code>0.0.0.0</code> Host to bind the server to. <code>--port</code> value <code>80</code> Port to bind the server to. <code>--disable-worker</code> <code>False</code> Disable embedded worker. <code>--bootstrap-password</code> value Auto-generated. Initial password for the default admin user. <code>--database-url</code> value <code>sqlite:///&lt;data-dir&gt;/database.db</code> URL of the database. Example: postgresql://user:password@hostname:port/db_name <code>--ssl-keyfile</code> value (empty) Path to the SSL key file. <code>--ssl-certfile</code> value (empty) Path to the SSL certificate file. <code>--force-auth-localhost</code> <code>False</code> Force authentication for requests originating from localhost (127.0.0.1).When set to True, all requests from localhost will require authentication. <code>--disable-update-check</code> <code>False</code> Disable update check. <code>--model-catalog-file</code> value (empty) Path or URL to the model catalog file. <code>--ray-port</code> value <code>40096</code> Port of Ray (GCS server). Used when Ray is enabled. <code>--ray-client-server-port</code> value <code>40097</code> Port of Ray Client Server. Used when Ray is enabled. <code>--enable_cors</code> <code>False</code> Enable CORS in server. <code>--allow-origins</code> value <code>[\"*\"]</code> A list of origins that should be permitted to make cross-origin requests. <code>--allow-credentials</code> <code>False</code> Indicate that cookies should be supported for cross-origin requests. <code>--allow-methods</code> value <code>[\"GET\", \"POST\"]</code> A list of HTTP methods that should be allowed for cross-origin requests. <code>--allow-headers</code> value <code>[\"Authorization\", \"Content-Type\"]</code> A list of HTTP request headers that should be supported for cross-origin requests."},{"location":"cli-reference/start/#worker-options","title":"Worker Options","text":"Flag Default Description <code>-s</code> value, <code>--server-url</code> value (empty) Server to connect to. <code>--worker-name</code> value (empty) Name of the worker node. Use the hostname by default. <code>--worker-ip</code> value (empty) IP address of the worker node. Auto-detected by default. <code>--disable-metrics</code> <code>False</code> Disable metrics. <code>--disable-rpc-servers</code> <code>False</code> Disable RPC servers. <code>--metrics-port</code> value <code>10151</code> Port to expose metrics. <code>--worker-port</code> value <code>10150</code> Port to bind the worker to. Use a consistent value for all workers. <code>--service-port-range</code> value <code>40000-40063</code> Port range for inference services, specified as a string in the form 'N1-N2'. Both ends of the range are inclusive. <code>--rpc-server-port-range</code> value <code>40064-40095</code> Port range for llama-box RPC servers, specified as a string in the form 'N1-N2'. Both ends of the range are inclusive. <code>--ray-node-manager-port</code> value <code>40098</code> Port of Ray node manager. Used when Ray is enabled. <code>--ray-object-manager-port</code> value <code>40099</code> Port of Ray object manager. Used when Ray is enabled. <code>--ray-worker-port-range</code> value <code>40100-40131</code> Port range for Ray worker processes, specified as a string in the form 'N1-N2'. Both ends of the range are inclusive. <code>--log-dir</code> value (empty) Directory to store logs. <code>--rpc-server-args</code> value (empty) Arguments to pass to the RPC servers. Use <code>=</code> to avoid the CLI recognizing rpc-server-args as a server argument. This can be used multiple times to pass a list of arguments. Example: <code>--rpc-server-args=--verbose --rpc-server-args=--log-colors</code> <code>--system-reserved</code> value <code>\"{\\\"ram\\\": 2, \\\"vram\\\": 1}\"</code> The system reserves resources for the worker during scheduling, measured in GiB. By default, 2 GiB of RAM and 1G of VRAM is reserved, Note: '{\\\"memory\\\": 2, \\\"gpu_memory\\\": 1}' is also supported, but it is deprecated and will be removed in future releases. <code>--tools-download-base-url</code> value Base URL for downloading dependency tools. <code>--enable-hf-transfer</code> <code>False</code> Enable faster downloads from the Hugging Face Hub using hf_transfer. https://huggingface.co/docs/huggingface_hub/v0.29.3/package_reference/environment_variables#hfhubenablehftransfer"},{"location":"cli-reference/start/#available-environment-variables","title":"Available Environment Variables","text":"<p>Most of the options can be set via environment variables. The environment variables are prefixed with <code>GPUSTACK_</code> and are in uppercase. For example, <code>--data-dir</code> can be set via the <code>GPUSTACK_DATA_DIR</code> environment variable.</p> <p>Below are additional environment variables that can be set:</p> Flag Description <code>HF_ENDPOINT</code> Hugging Face Hub endpoint. e.g., <code>https://hf-mirror.com</code>"},{"location":"cli-reference/start/#config-file","title":"Config File","text":"<p>You can configure start options using a YAML-format config file when starting GPUStack server or worker. Here is a complete example:</p> <pre><code># Common Options\ndebug: false\ndata_dir: /path/to/data_dir\ncache_dir: /path/to/cache_dir\ntoken: mytoken\nollama_library_base_url: https://registry.mycompany.com\nenable_ray: false\nray_args: [\"--port=6379\", \"--verbose\"]\n\n# Server Options\nhost: 0.0.0.0\nport: 80\ndisable_worker: false\ndatabase_url: postgresql://user:password@hostname:port/db_name\nssl_keyfile: /path/to/keyfile\nssl_certfile: /path/to/certfile\nforce_auth_localhost: false\nbootstrap_password: myadminpassword\ndisable_update_check: false\nmodel_catalog_file: /path_or_url/to/model_catalog_file\nray_port: 40096\nray_client_server_port: 40097\nenable_cors: false\nallow_origins: [\"*\"]\nallow_credentials: false\nallow_methods: [\"GET\", \"POST\"]\nallow_headers: [\"Authorization\", \"Content-Type\"]\n\n# Worker Options\nserver_url: http://your_gpustack_server_url\nworker_name: myworker\nworker_ip: 192.168.1.101\ndisable_metrics: false\ndisable_rpc_servers: false\nmetrics_port: 10151\nworker_port: 10150\nservice_port_range: 40000-40063\nrpc_server_port_range: 40064-40095\nray_node_manager_port: 40098\nray_object_manager_port: 40099\nray_worker_port_range: 40100-40131\nlog_dir: /path/to/log_dir\nrpc_server_args: [\"--verbose\"]\nsystem_reserved:\n  ram: 2\n  vram: 1\ntools_download_base_url: https://mirror.mycompany.com\nenable_hf_transfer: false\n</code></pre>"},{"location":"installation/apple-metal-installation/","title":"Apple Metal Installation","text":""},{"location":"installation/apple-metal-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> Apple Metal (M-series chips)</li> </ul>"},{"location":"installation/apple-metal-installation/#supported-platforms","title":"Supported Platforms","text":"OS Version Arch Supported methods macOS 14 Sonoma15 Sequoia ARM64 Installation Script (Recommended)pip Installation"},{"location":"installation/apple-metal-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> llama-box</li> <li> vox-box (CPU backend)</li> </ul>"},{"location":"installation/apple-metal-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 ~ 3.12</li> </ul> <p>Check the Python version:</p> <pre><code>python -V\n</code></pre>"},{"location":"installation/apple-metal-installation/#installation-script","title":"Installation Script","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p>"},{"location":"installation/apple-metal-installation/#run-gpustack","title":"Run GPUStack","text":"<pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>If you need support for audio models, run:</p> <pre><code>curl -sfL https://get.gpustack.ai | INSTALL_SKIP_BUILD_DEPENDENCIES=0 sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p>"},{"location":"installation/apple-metal-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>If you need support for audio models, run:</p> <pre><code>curl -sfL https://get.gpustack.ai | INSTALL_SKIP_BUILD_DEPENDENCIES=0 sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"installation/apple-metal-installation/#pip-installation","title":"pip Installation","text":""},{"location":"installation/apple-metal-installation/#install-gpustack","title":"Install GPUStack","text":"<p>Run the following to install GPUStack:</p> <pre><code>pip install gpustack\n</code></pre> <p>If you need support for audio models, run:</p> <pre><code>pip install \"gpustack[audio]\"\n</code></pre> <p>To verify, run:</p> <pre><code>gpustack version\n</code></pre>"},{"location":"installation/apple-metal-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> <pre><code>sudo gpustack start\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <p>By default, GPUStack uses <code>/var/lib/gpustack</code> as the data directory so you need <code>sudo</code> or proper permission for that. You can also set a custom data directory by running:</p> <pre><code>gpustack start --data-dir mypath\n</code></pre> <p>You can refer to the CLI Reference for available CLI Flags.</p>"},{"location":"installation/apple-metal-installation/#optional-add-worker_1","title":"(Optional) Add Worker","text":"<p>To add a worker to the GPUStack cluster, you need to specify the server URL and the authentication token.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To start a GPUStack worker and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>sudo gpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre>"},{"location":"installation/apple-metal-installation/#run-gpustack-as-a-launchd-service","title":"Run GPUStack as a Launchd Service","text":"<p>A recommended way is to run GPUStack as a startup service. For example, using launchd:</p> <p>Create a service file in <code>/Library/LaunchDaemons/ai.gpustack.plist</code>:</p> <pre><code>sudo tee /Library/LaunchDaemons/ai.gpustack.plist &gt; /dev/null &lt;&lt;EOF\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;dict&gt;\n  &lt;key&gt;Label&lt;/key&gt;\n  &lt;string&gt;ai.gpustack&lt;/string&gt;\n  &lt;key&gt;ProgramArguments&lt;/key&gt;\n  &lt;array&gt;\n    &lt;string&gt;$(command -v gpustack)&lt;/string&gt;\n    &lt;string&gt;start&lt;/string&gt;\n  &lt;/array&gt;\n  &lt;key&gt;RunAtLoad&lt;/key&gt;\n  &lt;true/&gt;\n  &lt;key&gt;KeepAlive&lt;/key&gt;\n  &lt;true/&gt;\n  &lt;key&gt;EnableTransactions&lt;/key&gt;\n  &lt;true/&gt;\n  &lt;key&gt;StandardOutPath&lt;/key&gt;\n  &lt;string&gt;/var/log/gpustack.log&lt;/string&gt;\n  &lt;key&gt;StandardErrorPath&lt;/key&gt;\n  &lt;string&gt;/var/log/gpustack.log&lt;/string&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\nEOF\n</code></pre> <p>Then start GPUStack:</p> <pre><code>sudo launchctl bootstrap system /Library/LaunchDaemons/ai.gpustack.plist\n</code></pre> <p>Check the service status:</p> <pre><code>sudo launchctl print system/ai.gpustack\n</code></pre> <p>And ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"installation/installation-requirements/","title":"Installation Requirements","text":"<p>This page describes the software and networking requirements for the nodes where GPUStack will be installed.</p>"},{"location":"installation/installation-requirements/#python-requirements","title":"Python Requirements","text":"<p>GPUStack requires Python version 3.10 to 3.12.</p>"},{"location":"installation/installation-requirements/#operating-system-requirements","title":"Operating System Requirements","text":"<p>GPUStack is supported on the following operating systems:</p> <ul> <li> macOS</li> <li> Windows</li> <li> Linux</li> </ul> <p>GPUStack has been tested and verified to work on the following operating systems:</p> OS Versions Windows 10, 11 macOS &gt;= 14 Ubuntu &gt;= 20.04 Debian &gt;= 11 RHEL &gt;= 8 Rocky &gt;= 8 Fedora &gt;= 36 OpenSUSE &gt;= 15.3 (leap) OpenEuler &gt;= 22.03 <p>Note</p> <p>The installation of GPUStack worker on a Linux system requires that the GLIBC version be 2.29 or higher. If your system uses a lower GLIBC version, consider using the <code>Docker Installation</code> method as an alternative.</p> <p>Use the following command to check the GLIBC version:</p> <pre><code>ldd --version\n</code></pre>"},{"location":"installation/installation-requirements/#supported-architectures","title":"Supported Architectures","text":"<p>GPUStack supports both AMD64 and ARM64 architectures, with the following notes:</p> <ul> <li>On Linux and macOS, when using Python versions below 3.12, ensure that the installed Python distribution corresponds to your system architecture.</li> <li>On Windows, please use the AMD64 distribution of Python, as wheel packages for certain dependencies are unavailable for ARM64. If you use tools like <code>conda</code>, this will be handled automatically, as conda installs the AMD64 distribution by default.</li> </ul>"},{"location":"installation/installation-requirements/#accelerator-runtime-requirements","title":"Accelerator Runtime Requirements","text":"<p>GPUStack supports the following accelerators:</p> <ul> <li> NVIDIA CUDA (Compute Capability 6.0 and above)</li> <li> Apple Metal (M-series chips)</li> <li> AMD ROCm</li> <li> Ascend CANN</li> <li> Hygon DTK</li> <li> Moore Threads MUSA</li> </ul> <p>Ensure all necessary drivers and libraries are installed on the system prior to installing GPUStack.</p>"},{"location":"installation/installation-requirements/#nvidia-cuda","title":"NVIDIA CUDA","text":"<p>To use NVIDIA CUDA as an accelerator, ensure the following components are installed:</p> <ul> <li>NVIDIA Driver</li> <li>NVIDIA CUDA Toolkit 12 (Optional, required for non-Docker installations)</li> <li>NVIDIA cuDNN 9 (Optional, required for audio models when not using Docker)</li> <li>NVIDIA Container Toolkit (Optional, required for Docker installation)</li> </ul>"},{"location":"installation/installation-requirements/#amd-rocm","title":"AMD ROCm","text":"<p>To use AMD ROCm as an accelerator, ensure the following components are installed:</p> <ul> <li>ROCm</li> </ul>"},{"location":"installation/installation-requirements/#ascend-cann","title":"Ascend CANN","text":"<p>For Ascend CANN as an accelerator, ensure the following components are installed:</p> <ul> <li>Ascend NPU Driver &amp; Firmware</li> <li>Ascend CANN Toolkit &amp; Kernels (Optional, required for non-Docker installations)</li> </ul>"},{"location":"installation/installation-requirements/#hygon-dtk","title":"Hygon DTK","text":"<p>To use Hygon DTK as an accelerator, ensure the following components are installed:</p> <ul> <li>DCU Driver</li> <li>DCU Toolkit</li> </ul>"},{"location":"installation/installation-requirements/#moore-threads-musa","title":"Moore Threads MUSA","text":"<p>To use Moore Threads MUSA as an accelerator, ensure the following components are installed:</p> <ul> <li>MUSA SDK</li> <li>MT Container Toolkits (Optional, required for docker installation)</li> </ul>"},{"location":"installation/installation-requirements/#networking-requirements","title":"Networking Requirements","text":""},{"location":"installation/installation-requirements/#connectivity-requirements","title":"Connectivity Requirements","text":"<p>The following network connectivity is required to ensure GPUStack functions properly:</p> <p>Server-to-Worker: The server must be able to reach the workers for proxying inference requests.</p> <p>Worker-to-Server: Workers must be able to reach the server to register themselves and send updates.</p> <p>Worker-to-Worker: Necessary for distributed inference across multiple workers</p>"},{"location":"installation/installation-requirements/#port-requirements","title":"Port Requirements","text":"<p>GPUStack uses the following ports for communication:</p>"},{"location":"installation/installation-requirements/#server-ports","title":"Server Ports","text":"Port Description TCP 80 Default port for the GPUStack UI and API endpoints TCP 443 Default port for the GPUStack UI and API endpoints (when TLS is enabled) <p>The following ports are used on GPUStack server when Ray is enabled for distributed vLLM across workers:</p> Ray Port Description TCP 8265 Default Port for Ray dashboard TCP 40096 Default port for Ray (GCS server) TCP 40097 Default port for Ray Client Server <p>The default ports in GPUStack may differ from Ray\u2019s default ports to simplify port exposure, especially when using Docker. For more information about Ray ports, refer to the Ray documentation.</p>"},{"location":"installation/installation-requirements/#worker-ports","title":"Worker Ports","text":"Port Description TCP 10150 Default port for the GPUStack worker TCP 10151 Default port for exposing metrics TCP 40000-40063 Port range allocated for inference services TCP 40064-40095 Port range allocated for llama-box RPC servers <p>The following ports are used on GPUStack worker when Ray is enabled for distributed vLLM across workers:</p> Ray Port Description TCP 40098 Default port for Ray node manager TCP 40099 Default port for Ray object manager TCP 40100-40131 Port range for Ray worker processes"},{"location":"installation/installation-script/","title":"Installation Script","text":""},{"location":"installation/installation-script/#linux-and-macos","title":"Linux and macOS","text":"<p>You can use the installation script available at <code>https://get.gpustack.ai</code> to install GPUStack as a service on systemd and launchd based systems.</p> <p>You can set additional environment variables and CLI flags when running the script. The following are examples running the installation script with different configurations:</p> <pre><code># Run server with the built-in worker.\ncurl -sfL https://get.gpustack.ai | sh -s -\n\n# Run server with non-default port.\ncurl -sfL https://get.gpustack.ai | sh -s - --port 8080\n\n# Run server with a custom data path.\ncurl -sfL https://get.gpustack.ai | sh -s - --data-dir /data/gpustack-data\n\n# Run server without the built-in worker.\ncurl -sfL https://get.gpustack.ai | sh -s - --disable-worker\n\n# Run server with TLS.\ncurl -sfL https://get.gpustack.ai | sh -s - --ssl-keyfile /path/to/keyfile --ssl-certfile /path/to/certfile\n\n# Run server with external postgresql database.\ncurl -sfL https://get.gpustack.ai | sh -s - --database-url \"postgresql://username:password@host:port/database_name\"\n\n# Run worker with specified IP.\ncurl -sfL https://get.gpustack.ai | sh -s - --server-url http://myserver --token mytoken --worker-ip 192.168.1.100\n\n# Install with a custom PyPI mirror.\ncurl -sfL https://get.gpustack.ai | INSTALL_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple sh -s -\n\n# Install a custom wheel package other than releases form pypi.org.\ncurl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=https://repo.mycompany.com/my-gpustack.whl sh -s -\n\n# Install a specific version with extra audio dependencies.\ncurl -sfL https://get.gpustack.ai | INSTALL_PACKAGE_SPEC=gpustack[audio]==0.6.0 sh -s -\n</code></pre>"},{"location":"installation/installation-script/#windows","title":"Windows","text":"<p>You can use the installation script available at <code>https://get.gpustack.ai</code> to install GPUStack as a service on Windows Service Manager.</p> <p>You can set additional environment variables and CLI flags when running the script. The following are examples running the installation script with different configurations:</p> <pre><code># Run server with the built-in worker.\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Run server with non-default port.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --port 8080\"\n\n# Run server with a custom data path.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --data-dir 'D:\\gpustack-data'\"\n\n# Run server without the built-in worker.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --disable-worker\"\n\n# Run server with TLS.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --ssl-keyfile 'C:\\path\\to\\keyfile' --ssl-certfile 'C:\\path\\to\\certfile'\"\n\n# Run server with external postgresql database.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --database-url 'postgresql://username:password@host:port/database_name'\"\n\n# Run worker with specified IP.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url 'http://myserver' --token 'mytoken' --worker-ip '192.168.1.100'\"\n\n# Run worker with customize reserved resource.\nInvoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url 'http://myserver' --token 'mytoken' --system-reserved '{\"\"ram\"\":5, \"\"vram\"\":5}'\"\n\n# Install with a custom PyPI mirror.\n$env:INSTALL_INDEX_URL = \"https://pypi.tuna.tsinghua.edu.cn/simple\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Install a custom wheel package other than releases form pypi.org.\n$env:INSTALL_PACKAGE_SPEC = \"https://repo.mycompany.com/my-gpustack.whl\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n\n# Install a specific version with extra audio dependencies.\n$env:INSTALL_PACKAGE_SPEC = \"gpustack[audio]==0.6.0\"\nInvoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>Warning</p> <p>Avoid using PowerShell ISE as it is not compatible with the installation script.</p>"},{"location":"installation/installation-script/#available-environment-variables-for-the-installation-script","title":"Available Environment Variables for the Installation Script","text":"Name Default Description <code>INSTALL_INDEX_URL</code> (empty) Base URL of the Python Package Index. <code>INSTALL_PACKAGE_SPEC</code> <code>gpustack[all]</code> or <code>gpustack[audio]</code> The package spec to install. The install script will automatically decide based on the platform. It supports PYPI package names, URLs, and local paths. See the pip install documentation for details. <ul><li><code>gpustack[all]</code>: With all inference backends: llama-box, vllm, vox-box.</li><li><code>gpustack[vllm]</code>: With inference backends: llama-box, vllm.</li><li><code>gpustack[audio]</code>: With inference backends: llama-box, vox-box.</li></ul> <code>INSTALL_SKIP_POST_CHECK</code> (empty) If set to 1, the installation script will skip the post-installation check. <code>INSTALL_SKIP_BUILD_DEPENDENCIES</code> <code>1</code> If set to 1 will skip the build dependencies. <code>INSTALL_SKIP_IOGPU_WIRED_LIMIT</code> (empty) If set to 1 will skip setting the GPU wired memory limit on macOS. <code>INSTALL_IOGPU_WIRED_LIMIT_MB</code> (empty) This sets the maximum amount of wired memory that the GPU can allocate on macOS."},{"location":"installation/installation-script/#set-environment-variables-for-the-gpustack-service","title":"Set Environment Variables for the GPUStack Service","text":"<p>You can set environment variables for the GPUStack service in an environment file located at:</p> <ul> <li>Linux and macOS: <code>/etc/default/gpustack</code></li> <li>Windows: <code>$env:APPDATA\\gpustack\\gpustack.env</code></li> </ul> <p>The following is an example of the content of the file:</p> <pre><code>HF_TOKEN=\"mytoken\"\nHF_ENDPOINT=\"https://my-hf-endpoint\"\n</code></pre> <p>Note</p> <p>Unlike Systemd, Launchd and Windows services do not natively support reading environment variables from a file. Configuration via the environment file is implemented by the installation script. It reads the file and applies the variables to the service configuration. After modifying the environment file on Windows and macOS, you need to re-run the installation script to apply changes to the GPUStack service.</p>"},{"location":"installation/installation-script/#available-cli-flags","title":"Available CLI Flags","text":"<p>The appended CLI flags of the installation script are passed directly as flags for the <code>gpustack start</code> command. You can refer to the CLI Reference for details.</p>"},{"location":"installation/uninstallation/","title":"Uninstallation","text":""},{"location":"installation/uninstallation/#script","title":"Script","text":"<p>Warning</p> <p>Uninstallation script deletes the data in local datastore(sqlite), configuration, model cache, and all of the scripts and CLI tools. It does not remove any data from external datastores.</p> <p>If you installed GPUStack using the installation script, a script to uninstall GPUStack was generated during installation.</p> <p>Run the following command to uninstall GPUStack:</p> LinuxmacOSWindows <pre><code>sudo /var/lib/gpustack/uninstall.sh\n</code></pre> <pre><code>sudo /var/lib/gpustack/uninstall.sh\n</code></pre> <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; &amp; \"$env:APPDATA\\gpustack\\uninstall.ps1\"\n</code></pre>"},{"location":"installation/uninstallation/#docker","title":"Docker","text":"<p>If you install GPUStack using Docker, the followings are example commands to uninstall GPUStack. You can modify according to your setup:</p> <pre><code># Remove the container\ndocker rm -f gpustack\n# Remove the data volume\ndocker volume rm gpustack-data\n</code></pre>"},{"location":"installation/uninstallation/#pip","title":"pip","text":"<p>If you install GPUStack using pip, the followings are example commands to uninstall GPUStack. You can modify according to your setup:</p> <pre><code># Stop and remove the service\nsystemctl stop gpustack.service\nrm -f /etc/systemd/system/gpustack.service\nsystemctl daemon-reload\n# Uninstall the CLI\npip uninstall gpustack\n# Remove the data directory\nrm -rf /var/lib/gpustack\n</code></pre>"},{"location":"installation/amd-rocm/air-gapped-installation/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment. An air-gapped environment refers to a setup where GPUStack will be installed offline.</p> <p>The following methods are available for installing GPUStack in an air-gapped environment:</p> OS Arch Supported methods Linux AMD64 Docker Installation"},{"location":"installation/amd-rocm/air-gapped-installation/#supported-devices","title":"Supported Devices","text":"Devices Supported Backends gfx1100: AMD Radeon RX 7900 XTX/7900 XT/7900 GRE llama-box, vLLM gfx1101: AMD Radeon RX 7800 XT/7700 XT llama-box, vLLM gfx1102: AMD Radeon RX 7600 XT/7600 llama-box, vLLM gfx942: AMD Instinct MI300X/MI300A llama-box, vLLM gfx90a: AMD Instinct MI250X/MI250/MI210 llama-box, vLLM gfx1030: AMD Radeon RX 6950 XT/6900 XT/6800 XT/6800 llama-box gfx1031: AMD Radeon RX 6750 XT/6700 XT/6700 llama-box gfx1032: AMD Radeon RX 6650 XT/6600 XT/6600 llama-box gfx908: AMD Instinct MI100 llama-box gfx906: AMD Instinct MI50 llama-box"},{"location":"installation/amd-rocm/air-gapped-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>lscpu | grep avx2\n</code></pre>"},{"location":"installation/amd-rocm/air-gapped-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/amd-rocm/air-gapped-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>ROCm 6.2.4</li> </ul> <p>Select the appropriate installation method for your system. Here, we provide steps for Linux (Ubuntu 22.04). For other systems, refer to the ROCm documentation.</p> <ol> <li>Install ROCm:</li> </ol> <pre><code>sudo apt update\nwget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/jammy/amdgpu-install_6.2.60204-1_all.deb\nsudo apt install ./amdgpu-install_6.2.60204-1_all.deb\namdgpu-install -y --usecase=graphics,rocm\nsudo reboot\n</code></pre> <ol> <li>Set Groups permissions:</li> </ol> <pre><code>sudo usermod -a -G render,video $LOGNAME\nsudo reboot\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <p>Verify that the current user is added to the render and video groups:</p> <pre><code>groups\n</code></pre> <p>Check if amdgpu kernel driver is installed:</p> <pre><code>dkms status\n</code></pre> <p>Check if the GPU is listed as an agent:</p> <pre><code>rocminfo\n</code></pre> <p>Check <code>rocm-smi</code>:</p> <pre><code>rocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname --showuniqueid --json\n</code></pre>"},{"location":"installation/amd-rocm/air-gapped-installation/#run-gpustack","title":"Run GPUStack","text":"<p>When running GPUStack with Docker, it works out of the box in an air-gapped environment as long as the Docker images are available. To do this, follow these steps:</p> <ol> <li>Pull GPUStack docker image in an online environment:</li> </ol> <pre><code>docker pull gpustack/gpustack:latest-rocm\n</code></pre> <p>If your online environment differs from the air-gapped environment in terms of OS or arch, specify the OS and arch of the air-gapped environment when pulling the image:</p> <pre><code>docker pull --platform linux/amd64 gpustack/gpustack:latest-rocm\n</code></pre> <ol> <li>Publish docker image to a private registry or load it directly in the air-gapped environment.</li> <li>Refer to the Docker Installation guide to run GPUStack using Docker.</li> </ol>"},{"location":"installation/amd-rocm/online-installation/","title":"Online Installation","text":""},{"location":"installation/amd-rocm/online-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> AMD GPUs</li> </ul>"},{"location":"installation/amd-rocm/online-installation/#supported-platforms","title":"Supported Platforms","text":"OS Version Arch Supported methods Linux Ubuntu 22.04Ubuntu 24.04 AMD64 Docker Installation (Recommended)Installation Script Windows 1011Server 2022 AMD64 Installation Script"},{"location":"installation/amd-rocm/online-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2</li> </ul> LinuxWindows <p>Check if the CPU is supported:</p> <pre><code>lscpu | grep avx2\n</code></pre> <p>Windows users need to manually verify support for the above instructions.</p>"},{"location":"installation/amd-rocm/online-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/amd-rocm/online-installation/#supported-devices_1","title":"Supported Devices","text":"Devices Supported Backends gfx1100: AMD Radeon RX 7900 XTX/7900 XT/7900 GRE llama-box, vLLM gfx1101: AMD Radeon RX 7800 XT/7700 XT llama-box, vLLM gfx1102: AMD Radeon RX 7600 XT/7600 llama-box, vLLM gfx942: AMD Instinct MI300X/MI300A llama-box, vLLM gfx90a: AMD Instinct MI250X/MI250/MI210 llama-box, vLLM gfx1030: AMD Radeon RX 6950 XT/6900 XT/6800 XT/6800 llama-box gfx1031: AMD Radeon RX 6750 XT/6700 XT/6700 llama-box gfx1032: AMD Radeon RX 6650 XT/6600 XT/6600 llama-box gfx908: AMD Instinct MI100 llama-box gfx906: AMD Instinct MI50 llama-box"},{"location":"installation/amd-rocm/online-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>ROCm 6.2.4</li> </ul> <p>Select the appropriate installation method for your system. Here, we provide steps for Linux (Ubuntu 22.04). For other systems, refer to the ROCm documentation.</p> <ol> <li>Install ROCm:</li> </ol> <pre><code>sudo apt update\nwget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/jammy/amdgpu-install_6.2.60204-1_all.deb\nsudo apt install ./amdgpu-install_6.2.60204-1_all.deb\namdgpu-install -y --usecase=graphics,rocm\nsudo reboot\n</code></pre> <ol> <li>Set Groups permissions:</li> </ol> <pre><code>sudo usermod -a -G render,video $LOGNAME\nsudo reboot\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <p>Verify that the current user is added to the render and video groups:</p> <pre><code>groups\n</code></pre> <p>Check if amdgpu kernel driver is installed:</p> <pre><code>dkms status\n</code></pre> <p>Check if the GPU is listed as an agent:</p> <pre><code>rocminfo\n</code></pre> <p>Check <code>rocm-smi</code>:</p> <pre><code>rocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname --showuniqueid --json\n</code></pre>"},{"location":"installation/amd-rocm/online-installation/#run-gpustack","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/dri \\\n    --network=host \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-rocm\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/dri \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-rocm \\\n    --worker-ip your_host_ip\n</code></pre> <p>You can refer to the CLI Reference for available startup flags.</p> <p>Check if the startup logs are normal:</p> <pre><code>docker logs -f gpustack\n</code></pre> <p>If the logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/amd-rocm/online-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more GPU nodes to GPUStack to form a GPU cluster. You need to add workers on other GPU nodes and specify the <code>--server-url</code> and <code>--token</code> parameters to join GPUStack.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/token\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/dri \\\n    --network=host \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-rocm \\\n    --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/dri \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-rocm \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Note</p> <ol> <li> <p>Heterogeneous cluster is supported. No matter what type of device it is, you can add it to the current GPUStack as a worker by specifying the <code>--server-url</code> and <code>--token</code> parameters.</p> </li> <li> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For configuration details, please refer to the CLI Reference.</p> </li> <li> <p>You can either use the <code>--ipc=host</code> flag or <code>--shm-size</code> flag to allow the container to access the host\u2019s shared memory. It is used by vLLM and pyTorch to share data between processes under the hood, particularly for tensor parallel inference.</p> </li> <li> <p>The  <code>-p 40064-40131:40064-40131</code> flag is used to ensure connectivity for distributed inference across workers. For more details, please refer to the Port Requirements. You can omit this flag if you don't need distributed inference across workers.</p> </li> </ol>"},{"location":"installation/amd-rocm/online-installation/#installation-script","title":"Installation Script","text":"LinuxWindows"},{"location":"installation/amd-rocm/online-installation/#supported-devices_2","title":"Supported Devices","text":"Devices Supported Backends gfx1100: AMD Radeon RX 7900 XTX/7900 XT/7900 GRE llama-box gfx1101: AMD Radeon RX 7800 XT/7700 XT llama-box gfx1102: AMD Radeon RX 7600 XT/7600 llama-box gfx1030: AMD Radeon RX 6950 XT/6900 XT/6800 XT/6800 llama-box gfx1031: AMD Radeon RX 6750 XT/6700 XT/6700 llama-box gfx1032: AMD Radeon RX 6650 XT/6600 XT/6600 llama-box gfx942: AMD Instinct MI300X/MI300A llama-box gfx90a: AMD Instinct MI250X/MI250/MI210 llama-box gfx908: AMD Instinct MI100 llama-box gfx906: AMD Instinct MI50 llama-box <p>View more details here.</p>"},{"location":"installation/amd-rocm/online-installation/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>ROCm 6.2.4</li> </ul> <ol> <li>Install ROCm 6.2.4:</li> </ol> <pre><code>sudo apt update\nwget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/jammy/amdgpu-install_6.2.60204-1_all.deb\nsudo apt install ./amdgpu-install_6.2.60204-1_all.deb\namdgpu-install -y --usecase=graphics,rocm\nsudo reboot\n</code></pre> <ol> <li>Set Groups permissions:</li> </ol> <pre><code>sudo usermod -a -G render,video $LOGNAME\nsudo reboot\n</code></pre> <ol> <li>Verify Installation:</li> </ol> <p>Verify that the current user is added to the render and video groups:</p> <pre><code>groups\n</code></pre> <p>Check if amdgpu kernel driver is installed:</p> <pre><code>dkms status\n</code></pre> <p>Check if the GPU is listed as an agent:</p> <pre><code>rocminfo\n</code></pre> <p>Check <code>rocm-smi</code>:</p> <pre><code>rocm-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname --showuniqueid --json\n</code></pre>"},{"location":"installation/amd-rocm/online-installation/#supported-devices_3","title":"Supported Devices","text":"Devices Supported Backends gfx1100: AMD Radeon RX 7900 XTX/7900 XT llama-box gfx1101: AMD Radeon RX 7800 XT/7700 XT llama-box gfx1102: AMD Radeon RX 7600 XT/7600 llama-box gfx1030: AMD Radeon RX 6950 XT/6900 XT/6800 XT/6800 llama-box gfx1031: AMD Radeon RX 6750 XT/6700 XT/6700 llama-box gfx1032: AMD Radeon RX 6650 XT/6600 XT/6600 llama-box <p>View more details here.</p>"},{"location":"installation/amd-rocm/online-installation/#prerequisites_3","title":"Prerequisites","text":"<ul> <li>HIP SDK 6.2.4</li> </ul> <ol> <li>Type the following command on your system from PowerShell to confirm that the obtained information matches that listed in Supported SKUs:</li> </ol> <pre><code>Get-ComputerInfo | Format-Table CsSystemType,OSName,OSDisplayVersion\n</code></pre> <ol> <li> <p>Download the installer from the HIP-SDK download page.</p> </li> <li> <p>Launch the installer.</p> </li> </ol>"},{"location":"installation/amd-rocm/online-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p> LinuxWindows <ul> <li>Install Server</li> </ul> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <ul> <li>(Optional) Add Worker</li> </ul> <p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <ul> <li>Install Server</li> </ul> <pre><code>Invoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <p><pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <ul> <li>(Optional) Add Worker</li> </ul> <p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>Invoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url http://your_gpustack_url --token your_gpustack_token\"\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre>"},{"location":"installation/ascend-cann/air-gapped-installation/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment. An air-gapped environment refers to a setup where GPUStack will be installed offline.</p> <p>The following methods are available for installing GPUStack in an air-gapped environment:</p> OS Arch Supported methods Linux ARM64 Docker Installation"},{"location":"installation/ascend-cann/air-gapped-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/ascend-cann/air-gapped-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: ARM64 with NEON</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>grep -E -i \"neon|asimd\" /proc/cpuinfo\n</code></pre> <ul> <li>NPU Driver and Firmware (Must support CANN 8.0.0.beta1)</li> </ul> <p>Check if the NPU driver is installed:</p> <pre><code>npu-smi info\n</code></pre> <ul> <li>Docker</li> </ul>"},{"location":"installation/ascend-cann/air-gapped-installation/#run-gpustack","title":"Run GPUStack","text":"<p>When running GPUStack with Docker, it works out of the box in an air-gapped environment as long as the Docker images are available. To do this, follow these steps:</p> <ol> <li>Pull GPUStack docker image in an online environment:</li> </ol> Ascend 910BAscend 310P <pre><code>docker pull gpustack/gpustack:latest-npu\n</code></pre> <pre><code>docker pull gpustack/gpustack:latest-npu-310p\n</code></pre> <p>If your online environment differs from the air-gapped environment in terms of OS or arch, specify the OS and arch of the air-gapped environment when pulling the image:</p> Ascend 910BAscend 310P <pre><code>docker pull --platform linux/arm64 gpustack/gpustack:latest-npu\n</code></pre> <pre><code>docker pull --platform linux/arm64 gpustack/gpustack:latest-npu-310p\n</code></pre> <ol> <li>Publish docker image to a private registry or load it directly in the air-gapped environment.</li> <li>Refer to the Docker Installation guide to run GPUStack using Docker.</li> </ol>"},{"location":"installation/ascend-cann/online-installation/","title":"Online Installation","text":""},{"location":"installation/ascend-cann/online-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> Ascend 910B series (910B1 ~ 910B4)</li> <li> Ascend 310P3</li> </ul>"},{"location":"installation/ascend-cann/online-installation/#supported-platforms","title":"Supported Platforms","text":"OS Arch Supported methods Linux ARM64 Docker Installation (Recommended)Installation Script"},{"location":"installation/ascend-cann/online-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: ARM64 with NEON</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>grep -E -i \"neon|asimd\" /proc/cpuinfo\n</code></pre> <ul> <li>NPU Driver and Firmware (Must support CANN 8.0.0.beta1)</li> </ul> <p>Check if the NPU driver is installed:</p> <pre><code>npu-smi info\n</code></pre>"},{"location":"installation/ascend-cann/online-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/ascend-cann/online-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> llama-box (Only supports FP16 precision)</li> <li> MindIE</li> </ul>"},{"location":"installation/ascend-cann/online-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> </ul>"},{"location":"installation/ascend-cann/online-installation/#run-gpustack","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker (Set <code>--device /dev/davinci{index}</code> to the required GPU indices):</p> Ascend 910BAscend 310P <p>Follow the steps below to install GPUStack on the Ascend 910B:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu \\\n    --worker-ip your_host_ip\n</code></pre> <p>Follow the steps below to install GPUStack on the Ascend 310P:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu-310p\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu-310p \\\n    --worker-ip your_host_ip\n</code></pre> <p>You can refer to the CLI Reference for available startup flags.</p> <p>Check if the startup logs are normal:</p> <pre><code>docker logs -f gpustack\n</code></pre> <p>If the logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/ascend-cann/online-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more GPU nodes to GPUStack to form a GPU cluster. You need to add workers on other GPU nodes and specify the <code>--server-url</code> and <code>--token</code> parameters to join GPUStack.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/token\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server (Set <code>ASCEND_VISIBLE_DEVICES</code> to the required GPU indices), run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> Ascend 910BAscend 310P <p>Follow the steps below to add workers on the Ascend 910B:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Follow the steps below to add workers on the Ascend 310P:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu-310p \\\n    --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device /dev/davinci0 \\\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-npu-310p \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Note</p> <ol> <li> <p>Heterogeneous cluster is supported. No matter what type of device it is, you can add it to the current GPUStack as a worker by specifying the <code>--server-url</code> and <code>--token</code> parameters.</p> </li> <li> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For configuration details, please refer to the CLI Reference.</p> </li> <li> <p>You can either use the <code>--ipc=host</code> flag or <code>--shm-size</code> flag to allow the container to access the host\u2019s shared memory. It is used by vLLM and pyTorch to share data between processes under the hood, particularly for tensor parallel inference.</p> </li> <li> <p>The  <code>-p 40064-40131:40064-40131</code> flag is used to ensure connectivity for distributed inference across workers. For more details, please refer to the Port Requirements. You can omit this flag if you don't need distributed inference across workers.</p> </li> </ol>"},{"location":"installation/ascend-cann/online-installation/#installation-script","title":"Installation Script","text":""},{"location":"installation/ascend-cann/online-installation/#supported-backends_1","title":"Supported backends","text":"<ul> <li> llama-box (Only supports Ascend 910B and FP16 precision)</li> </ul>"},{"location":"installation/ascend-cann/online-installation/#prerequites","title":"Prerequites","text":"<ul> <li>Ascend CANN Toolkit 8.0.0.beta1 &amp; Kernels</li> </ul> <p>Check if CANN is installed and verify that its version is 8.0.0:</p> <pre><code>cat /usr/local/Ascend/ascend-toolkit/latest/version.cfg\n</code></pre> <p>Check if CANN kernels are installed and verify that its version is 8.0.0:</p> <pre><code>cat /usr/local/Ascend/ascend-toolkit/latest/version.cfg | grep opp\n</code></pre>"},{"location":"installation/ascend-cann/online-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p>"},{"location":"installation/ascend-cann/online-installation/#optional-add-worker_1","title":"(Optional) Add Worker","text":"<p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"installation/cpu/air-gapped-installation/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment. An air-gapped environment refers to a setup where GPUStack will be installed offline.</p> <p>The following methods are available for installing GPUStack in an air-gapped environment:</p> OS Arch Supported methods Linux AMD64ARM64 Docker Installation (Recommended)pip Installation Windows AMD64ARM64 pip Installation"},{"location":"installation/cpu/air-gapped-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPUs (AMD64 with AVX2 or ARM64 with NEON)</li> </ul> LinuxWindows <p>Check if the CPU is supported:</p> AMD64ARM64 <pre><code>lscpu | grep avx2\n</code></pre> <pre><code>grep -E -i \"neon|asimd\" /proc/cpuinfo\n</code></pre> <p>Windows users need to manually verify support for the above instructions.</p>"},{"location":"installation/cpu/air-gapped-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/cpu/air-gapped-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> </ul>"},{"location":"installation/cpu/air-gapped-installation/#run-gpustack","title":"Run GPUStack","text":"<p>When running GPUStack with Docker, it works out of the box in an air-gapped environment as long as the Docker images are available. To do this, follow these steps:</p> <ol> <li>Pull GPUStack docker image in an online environment:</li> </ol> <pre><code>docker pull gpustack/gpustack:latest-cpu\n</code></pre> <p>If your online environment differs from the air-gapped environment in terms of OS or arch, specify the OS and arch of the air-gapped environment when pulling the image:</p> <pre><code>docker pull --platform linux/amd64 gpustack/gpustack:latest-cpu\n</code></pre> <ol> <li>Publish docker image to a private registry or load it directly in the air-gapped environment.</li> <li>Refer to the Docker Installation guide to run GPUStack using Docker.</li> </ol>"},{"location":"installation/cpu/air-gapped-installation/#pip-installation","title":"pip Installation","text":""},{"location":"installation/cpu/air-gapped-installation/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Python 3.10 ~ 3.12</li> </ul> <p>Check the Python version:</p> <pre><code>python -V\n</code></pre>"},{"location":"installation/cpu/air-gapped-installation/#install-gpustack","title":"Install GPUStack","text":"<p>For manually pip installation, you need to prepare the required packages and tools in an online environment and then transfer them to the air-gapped environment.</p> <p>Set up an online environment identical to the air-gapped environment, including OS, architecture, and Python version.</p>"},{"location":"installation/cpu/air-gapped-installation/#step-1-download-the-required-packages","title":"Step 1: Download the Required Packages","text":"<p>Run the following commands in an online environment:</p> LinuxWindows <pre><code>PACKAGE_SPEC=\"gpustack[audio]\"\n# To install a specific version\n# PACKAGE_SPEC=\"gpustack[audio]==0.6.0\"\n</code></pre> <p>If you don\u2019t need support for audio models, just set:</p> <pre><code>PACKAGE_SPEC=\"gpustack\"\n</code></pre> <pre><code>$PACKAGE_SPEC = \"gpustack[audio]\"\n# To install a specific version\n# $PACKAGE_SPEC = \"gpustack[audio]==0.6.0\"\n</code></pre> <p>If you don\u2019t need support for audio models, just set:</p> <pre><code>$PACKAGE_SPEC = \"gpustack\"\n</code></pre> <p>Download all required packages:</p> <pre><code>pip wheel $PACKAGE_SPEC -w gpustack_offline_packages\n</code></pre> <p>Install GPUStack to use its CLI:</p> <pre><code>pip install gpustack\n</code></pre> <p>Download dependency tools and save them as an archive:</p> <pre><code>gpustack download-tools --save-archive gpustack_offline_tools.tar.gz\n</code></pre> <p>If your online environment differs from the air-gapped environment, specify the OS, architecture, and device explicitly:</p> <pre><code>gpustack download-tools --save-archive gpustack_offline_tools.tar.gz --system linux --arch amd64 --device cpu\n</code></pre>"},{"location":"installation/cpu/air-gapped-installation/#step-2-transfer-the-packages","title":"Step 2: Transfer the Packages","text":"<p>Transfer the following files from the online environment to the air-gapped environment.</p> <ul> <li><code>gpustack_offline_packages</code> directory.</li> <li><code>gpustack_offline_tools.tar.gz</code> file.</li> </ul>"},{"location":"installation/cpu/air-gapped-installation/#step-3-install-gpustack","title":"Step 3: Install GPUStack","text":"<p>In the air-gapped environment, run the following commands:</p> <pre><code># Install GPUStack from the downloaded packages\npip install --no-index --find-links=gpustack_offline_packages gpustack\n\n# Load and apply the pre-downloaded tools archive\ngpustack download-tools --load-archive gpustack_offline_tools.tar.gz\n</code></pre> <p>Now you can run GPUStack by following the instructions in the pip Installation guide.</p>"},{"location":"installation/cpu/online-installation/","title":"Online Installation","text":"<p>In GPUStack, <code>llama-box</code> and <code>vox-box</code> backends support CPU inference. However, compared to GPUs, CPU performance is significantly lower, so it is only recommended for testing or small-scale use cases.</p>"},{"location":"installation/cpu/online-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> CPUs (AMD64 with AVX2 or ARM64 with NEON)</li> </ul>"},{"location":"installation/cpu/online-installation/#supported-platforms","title":"Supported Platforms","text":"OS Arch Supported methods Linux AMD64ARM64 Installation ScriptDocker Installation (Recommended)pip Installation Windows AMD64ARM64 Installation Scriptpip Installation"},{"location":"installation/cpu/online-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> llama-box</li> <li> vox-box</li> </ul>"},{"location":"installation/cpu/online-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPUs (AMD64 with AVX2 or ARM64 with NEON)</li> </ul> LinuxWindows <p>Check if the CPU is supported:</p> AMD64ARM64 <pre><code>lscpu | grep avx2\n</code></pre> <pre><code>grep -E -i \"neon|asimd\" /proc/cpuinfo\n</code></pre> <p>Windows users need to manually verify support for the above instructions.</p>"},{"location":"installation/cpu/online-installation/#installation-script","title":"Installation Script","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p> LinuxWindows <ul> <li>Install Server</li> </ul> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <ul> <li>(Optional) Add Worker</li> </ul> <p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <ul> <li>Install Server</li> </ul> <pre><code>Invoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <ul> <li>(Optional) Add Worker</li> </ul> <p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>Invoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url http://your_gpustack_url --token your_gpustack_token\"\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre>"},{"location":"installation/cpu/online-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/cpu/online-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> </ul>"},{"location":"installation/cpu/online-installation/#run-gpustack","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --network=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-cpu\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-cpu \\\n    --worker-ip your_host_ip\n</code></pre> <p>You can refer to the CLI Reference for available startup flags.</p> <p>Check if the startup logs are normal:</p> <pre><code>docker logs -f gpustack\n</code></pre> <p>If the logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/cpu/online-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more CPU nodes to GPUStack. You need to add workers on other CPU nodes and specify the <code>--server-url</code> and <code>--token</code> parameters to join GPUStack.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/token\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --network=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-cpu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 10150:10150 \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-cpu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Note</p> <ol> <li> <p>Heterogeneous cluster is supported. No matter what type of device it is, you can add it to the current GPUStack as a worker by specifying the <code>--server-url</code> and <code>--token</code> parameters.</p> </li> <li> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For configuration details, please refer to the CLI Reference.</p> </li> </ol>"},{"location":"installation/cpu/online-installation/#pip-installation","title":"pip Installation","text":""},{"location":"installation/cpu/online-installation/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Python 3.10 ~ 3.12</li> </ul> <p>Check the Python version:</p> <pre><code>python -V\n</code></pre>"},{"location":"installation/cpu/online-installation/#install-gpustack","title":"Install GPUStack","text":"<p>Run the following to install GPUStack.</p> <pre><code>pip install \"gpustack[audio]\"\n</code></pre> <p>If you don\u2019t need support for audio models, just run:</p> <pre><code>pip install gpustack\n</code></pre> <p>To verify, run:</p> <pre><code>gpustack version\n</code></pre>"},{"location":"installation/cpu/online-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> <pre><code>gpustack start\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> LinuxWindows <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> <p>By default, GPUStack uses <code>/var/lib/gpustack</code> as the data directory so you need <code>sudo</code> or proper permission for that. You can also set a custom data directory by running:</p> <pre><code>gpustack start --data-dir mypath\n</code></pre> <p>You can refer to the CLI Reference for available CLI Flags.</p>"},{"location":"installation/cpu/online-installation/#optional-add-worker_1","title":"(Optional) Add Worker","text":"<p>To add a worker to the GPUStack cluster, you need to specify the server URL and the authentication token.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> LinuxWindows <pre><code>cat /var/lib/gpustack/token\n</code></pre> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> <pre><code>gpustack start --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre>"},{"location":"installation/cpu/online-installation/#run-gpustack-as-a-system-service","title":"Run GPUStack as a System Service","text":"<p>A recommended way is to run GPUStack as a startup service. For example, using systemd:</p> <p>Create a service file in <code>/etc/systemd/system/gpustack.service</code>:</p> <pre><code>sudo tee /etc/systemd/system/gpustack.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=GPUStack Service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nEnvironmentFile=-/etc/default/%N\nExecStart=$(command -v gpustack) start\nRestart=always\nStandardOutput=append:/var/log/gpustack.log\nStandardError=append:/var/log/gpustack.log\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p>Then start GPUStack:</p> <pre><code>systemctl daemon-reload &amp;&amp; systemctl enable gpustack --now\n</code></pre> <p>Check the service status:</p> <pre><code>systemctl status gpustack\n</code></pre> <p>And ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"installation/hygon-dtk/air-gapped-installation/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment. An air-gapped environment refers to a setup where GPUStack will be installed offline.</p> <p>The following methods are available for installing GPUStack in an air-gapped environment:</p> OS Arch Supported methods Linux AMD64 Docker Installation"},{"location":"installation/hygon-dtk/air-gapped-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/hygon-dtk/air-gapped-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>lscpu | grep avx2\n</code></pre> <ul> <li>Docker</li> <li>DCU Driver rock-6.3</li> </ul> <p>Check if the driver is installed:</p> <pre><code>lsmod | grep dcu\n</code></pre>"},{"location":"installation/hygon-dtk/air-gapped-installation/#run-gpustack","title":"Run GPUStack","text":"<p>When running GPUStack with Docker, it works out of the box in an air-gapped environment as long as the Docker images are available. To do this, follow these steps:</p> <ol> <li>Pull GPUStack docker image in an online environment:</li> </ol> <pre><code>docker pull gpustack/gpustack:latest-dcu\n</code></pre> <p>If your online environment differs from the air-gapped environment in terms of OS or arch, specify the OS and arch of the air-gapped environment when pulling the image:</p> <pre><code>docker pull --platform linux/amd64 gpustack/gpustack:latest-dcu\n</code></pre> <ol> <li>Publish docker image to a private registry or load it directly in the air-gapped environment.</li> <li>Refer to the Docker Installation guide to run GPUStack using Docker.</li> </ol>"},{"location":"installation/hygon-dtk/online-installation/","title":"Online Installation","text":""},{"location":"installation/hygon-dtk/online-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> Hygon DCUs (K100_AI (Verified), Z100/Z100L/K100(Not Verified))</li> </ul>"},{"location":"installation/hygon-dtk/online-installation/#supported-platforms","title":"Supported Platforms","text":"OS Arch Supported methods Linux AMD64 Docker Installation (Recommended)Installation Script"},{"location":"installation/hygon-dtk/online-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>lscpu | grep avx2\n</code></pre> <ul> <li>DCU Driver rock-6.3</li> </ul> <p>Check if the driver is installed:</p> <pre><code>lsmod | grep dcu\n</code></pre>"},{"location":"installation/hygon-dtk/online-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/hygon-dtk/online-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> vLLM (Only supports K100_AI)</li> <li> llama-box</li> </ul>"},{"location":"installation/hygon-dtk/online-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> </ul>"},{"location":"installation/hygon-dtk/online-installation/#run-gpustack","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/mkfd \\\n    --device=/dev/dri \\\n    -v /opt/hyhal:/opt/hyhal:ro \\\n    --network=host \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-dcu\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/mkfd \\\n    --device=/dev/dri \\\n    -v /opt/hyhal:/opt/hyhal:ro \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-dcu \\\n    --worker-ip your_host_ip\n</code></pre> <p>You can refer to the CLI Reference for available startup flags.</p> <p>Check if the startup logs are normal:</p> <pre><code>docker logs -f gpustack\n</code></pre> <p>If the logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/hygon-dtk/online-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more GPU nodes to GPUStack to form a GPU cluster. You need to add workers on other GPU nodes and specify the <code>--server-url</code> and <code>--token</code> parameters to join GPUStack.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/token\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/mkfd \\\n    --device=/dev/dri \\\n    -v /opt/hyhal:/opt/hyhal:ro \\\n    --network=host \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-dcu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --device=/dev/kfd \\\n    --device=/dev/mkfd \\\n    --device=/dev/dri \\\n    -v /opt/hyhal:/opt/hyhal:ro \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    --group-add video \\\n    --cap-add=SYS_PTRACE \\\n    --security-opt seccomp=unconfined \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-dcu \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Note</p> <ol> <li> <p>Heterogeneous cluster is supported. No matter what type of device it is, you can add it to the current GPUStack as a worker by specifying the <code>--server-url</code> and <code>--token</code> parameters.</p> </li> <li> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For configuration details, please refer to the CLI Reference.</p> </li> <li> <p>You can either use the <code>--ipc=host</code> flag or <code>--shm-size</code> flag to allow the container to access the host\u2019s shared memory. It is used by vLLM and pyTorch to share data between processes under the hood, particularly for tensor parallel inference.</p> </li> <li> <p>The  <code>-p 40064-40131:40064-40131</code> flag is used to ensure connectivity for distributed inference across workers. For more details, please refer to the Port Requirements. You can omit this flag if you don't need distributed inference across workers.</p> </li> </ol>"},{"location":"installation/hygon-dtk/online-installation/#installation-script","title":"Installation Script","text":""},{"location":"installation/hygon-dtk/online-installation/#supported-backends_1","title":"Supported backends","text":"<ul> <li> llama-box</li> </ul>"},{"location":"installation/hygon-dtk/online-installation/#prerequites","title":"Prerequites","text":"<ul> <li>DCU Toolkit 25.04</li> </ul> <p>Check if the GPU is listed as an agent:</p> <pre><code>rocminfo\n</code></pre> <p>Check <code>hy-smi</code>:</p> <pre><code>/opt/hyhal/bin/hy-smi -i --showmeminfo vram --showpower --showserial --showuse --showtemp --showproductname --showuniqueid --json\n</code></pre>"},{"location":"installation/hygon-dtk/online-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p>"},{"location":"installation/hygon-dtk/online-installation/#optional-add-worker_1","title":"(Optional) Add Worker","text":"<p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"installation/moorethreads-musa/air-gapped-installation/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment. An air-gapped environment refers to a setup where GPUStack will be installed offline.</p> <p>The following methods are available for installing GPUStack in an air-gapped environment:</p> OS Arch Supported methods Linux AMD64 Docker Installation"},{"location":"installation/moorethreads-musa/air-gapped-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/moorethreads-musa/air-gapped-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>lscpu | grep avx2\n</code></pre> <ul> <li>Driver for MTT S80/S3000/S4000</li> </ul> <p>Check if the driver is installed:</p> <pre><code>mthreads-gmi\n</code></pre> <ul> <li>Docker</li> <li>MT Container Toolkits</li> </ul> <p>Check if the MT Container Toolkits are installed and set as the default runtime:</p> <pre><code># cd /usr/bin/musa &amp;&amp; sudo ./docker setup $PWD\ndocker info | grep Runtimes | grep mthreads\n</code></pre>"},{"location":"installation/moorethreads-musa/air-gapped-installation/#run-gpustack","title":"Run GPUStack","text":"<p>When running GPUStack with Docker, it works out of the box in an air-gapped environment as long as the Docker images are available. To do this, follow these steps:</p> <ol> <li>Pull GPUStack docker image in an online environment:</li> </ol> <pre><code>docker pull gpustack/gpustack:latest-musa\n</code></pre> <p>If your online environment differs from the air-gapped environment in terms of OS or arch, specify the OS and arch of the air-gapped environment when pulling the image:</p> <pre><code>docker pull --platform linux/amd64 gpustack/gpustack:latest-musa\n</code></pre> <ol> <li>Publish docker image to a private registry or load it directly in the air-gapped environment.</li> <li>Refer to the Docker Installation guide to run GPUStack using Docker.</li> </ol>"},{"location":"installation/moorethreads-musa/online-installation/","title":"Online Installation","text":""},{"location":"installation/moorethreads-musa/online-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> Moore Threads GPUs (MTT S80, MTT S3000, MTT S4000)</li> </ul>"},{"location":"installation/moorethreads-musa/online-installation/#supported-platforms","title":"Supported Platforms","text":"OS Arch Supported methods Linux AMD64 Docker Installation (Recommended)Installation Script"},{"location":"installation/moorethreads-musa/online-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> llama-box</li> </ul>"},{"location":"installation/moorethreads-musa/online-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2</li> </ul> <p>Check if the CPU is supported:</p> <pre><code>lscpu | grep avx2\n</code></pre> <ul> <li>Driver for MTT S80/S3000/S4000</li> </ul> <p>Check if the driver is installed:</p> <pre><code>mthreads-gmi\n</code></pre>"},{"location":"installation/moorethreads-musa/online-installation/#docker-installation","title":"Docker Installation","text":"<ul> <li>Docker</li> <li>MT Container Toolkits</li> </ul> <p>Check if the MT Container Toolkits are installed and set as the default runtime:</p> <pre><code># cd /usr/bin/musa &amp;&amp; sudo ./docker setup $PWD\ndocker info | grep Runtimes | grep mthreads\n</code></pre>"},{"location":"installation/moorethreads-musa/online-installation/#run-gpustack","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-musa\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-musa \\\n    --worker-ip your_host_ip\n</code></pre> <p>You can refer to the CLI Reference for available startup flags.</p> <p>Check if the startup logs are normal:</p> <pre><code>docker logs -f gpustack\n</code></pre> <p>If the logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/moorethreads-musa/online-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more GPU nodes to GPUStack to form a GPU cluster. You need to add workers on other GPU nodes and specify the <code>--server-url</code> and <code>--token</code> parameters to join GPUStack.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/token\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-musa \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack:latest-musa \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Note</p> <ol> <li> <p>Heterogeneous cluster is supported. No matter what type of device it is, you can add it to the current GPUStack as a worker by specifying the <code>--server-url</code> and <code>--token</code> parameters.</p> </li> <li> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For configuration details, please refer to the CLI Reference.</p> </li> <li> <p>The  <code>-p 40064-40131:40064-40131</code> flag is used to ensure connectivity for distributed inference across workers. For more details, please refer to the Port Requirements. You can omit this flag if you don't need distributed inference across workers.</p> </li> </ol>"},{"location":"installation/moorethreads-musa/online-installation/#installation-script","title":"Installation Script","text":""},{"location":"installation/moorethreads-musa/online-installation/#prerequites","title":"Prerequites","text":"<ul> <li>MUSA SDK</li> </ul>"},{"location":"installation/moorethreads-musa/online-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p>"},{"location":"installation/moorethreads-musa/online-installation/#optional-add-worker_1","title":"(Optional) Add Worker","text":"<p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"installation/nvidia-cuda/air-gapped-installation/","title":"Air-Gapped Installation","text":"<p>You can install GPUStack in an air-gapped environment. An air-gapped environment refers to a setup where GPUStack will be installed offline.</p> <p>The following methods are available for installing GPUStack in an air-gapped environment:</p> OS Arch Supported methods Linux AMD64ARM64 Docker Installation (Recommended)pip Installation Windows AMD64 pip Installation"},{"location":"installation/nvidia-cuda/air-gapped-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> vLLM (Compute Capability 7.0 and above, only supports Linux AMD64)</li> <li> llama-box</li> <li> vox-box</li> </ul>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2, or ARM64 with NEON</li> </ul> LinuxWindows <p>Check if the CPU is supported:</p> AMD64ARM64 <pre><code>lscpu | grep avx2\n</code></pre> <pre><code>grep -E -i \"neon|asimd\" /proc/cpuinfo\n</code></pre> <p>Windows users need to manually verify support for the above instructions.</p> <ul> <li>NVIDIA Driver</li> </ul> <p>Check if the NVIDIA driver is installed:</p> <pre><code>nvidia-smi --format=csv,noheader --query-gpu=index,name,memory.total,memory.used,utilization.gpu,temperature.gpu\n</code></pre> <p>And ensure the driver supports CUDA 12.4 or higher:</p> LinuxWindows <pre><code>nvidia-smi | grep \"CUDA Version\"\n</code></pre> <pre><code>nvidia-smi | findstr \"CUDA Version\"\n</code></pre>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/nvidia-cuda/air-gapped-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>NVIDIA Container Toolkit</li> </ul> <p>Check if Docker and NVIDIA Container Toolkit are installed:</p> <pre><code>docker info | grep Runtimes | grep nvidia\n</code></pre> <ul> <li>Disabling Systemd Cgroup Management in Docker</li> </ul> <p>Note</p> <p>When systemd is used to manage the cgroups of the container and it is triggered to reload any Unit files that have references to NVIDIA GPUs (e.g. systemctl daemon-reload), containerized GPU workloads may suddenly lose access to their GPUs.</p> <p>In GPUStack, GPUs may be lost in the Resources menu, and running <code>nvidia-smi</code> inside the GPUStack container may result in the error: <code>Failed to initialize NVML: Unknown Error</code></p> <p>To prevent this issue, disabling systemd cgroup management in Docker is required.</p> <p>Set the parameter \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] in the <code>/etc/docker/daemon.json</code> file and restart docker, such as:</p> <pre><code>vim /etc/docker/daemon.json\n</code></pre> <pre><code>{\n  \"runtimes\": {\n    \"nvidia\": {\n      \"args\": [],\n      \"path\": \"nvidia-container-runtime\"\n    }\n  },\n  \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]\n}\n</code></pre> <pre><code>systemctl daemon-reload &amp;&amp; systemctl restart docker\n</code></pre>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#run-gpustack","title":"Run GPUStack","text":"<p>When running GPUStack with Docker, it works out of the box in an air-gapped environment as long as the Docker images are available. To do this, follow these steps:</p> <ol> <li>Pull GPUStack docker image in an online environment:</li> </ol> <pre><code>docker pull gpustack/gpustack\n</code></pre> <p>If your online environment differs from the air-gapped environment in terms of OS or arch, specify the OS and arch of the air-gapped environment when pulling the image:</p> <pre><code>docker pull --platform linux/amd64 gpustack/gpustack\n</code></pre> <ol> <li>Publish docker image to a private registry or load it directly in the air-gapped environment.</li> <li>Refer to the Docker Installation guide to run GPUStack using Docker.</li> </ol>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#pip-installation","title":"pip Installation","text":""},{"location":"installation/nvidia-cuda/air-gapped-installation/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Python 3.10 ~ 3.12</li> </ul> <p>Check the Python version:</p> <pre><code>python -V\n</code></pre> <ul> <li>NVIDIA CUDA Toolkit 12</li> </ul> <p>Check if CUDA is installed and verify that its version is at least 12.4:</p> <pre><code>nvcc -V\n</code></pre> <ul> <li>NVIDIA cuDNN 9 (Optional, required for audio models)</li> </ul> <p>Check if cuDNN 9 is installed:</p> LinuxWindows <pre><code>ldconfig -p | grep libcudnn\n</code></pre> <pre><code>Get-ChildItem -Path C:\\ -Recurse -Filter \"cudnn*.dll\" -ErrorAction SilentlyContinue\n</code></pre>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#install-gpustack","title":"Install GPUStack","text":"<p>For manually pip installation, you need to prepare the required packages and tools in an online environment and then transfer them to the air-gapped environment.</p> <p>Set up an online environment identical to the air-gapped environment, including OS, architecture, and Python version.</p>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#step-1-download-the-required-packages","title":"Step 1: Download the Required Packages","text":"LinuxWindows <p>Run the following commands in an online environment:</p> AMD64ARM64 <pre><code># Extra dependencies options are \"vllm\", \"audio\" and \"all\"\n# \"vllm\" is only available for Linux AMD64\nPACKAGE_SPEC=\"gpustack[all]\"\n# To install a specific version\n# PACKAGE_SPEC=\"gpustack[all]==0.6.0\"\n</code></pre> <pre><code>PACKAGE_SPEC=\"gpustack[audio]\"\n# To install a specific version\n# PACKAGE_SPEC=\"gpustack[audio]==0.6.0\"\n</code></pre> <p>If you don\u2019t need the vLLM backend and support for audio models, just set:</p> <pre><code>PACKAGE_SPEC=\"gpustack\"\n</code></pre> <p>Run the following commands in an online environment:</p> <pre><code>$PACKAGE_SPEC = \"gpustack[audio]\"\n# To install a specific version\n# $PACKAGE_SPEC = \"gpustack[audio]==0.6.0\"\n</code></pre> <p>If you don\u2019t need support for audio models, just set:</p> <pre><code>$PACKAGE_SPEC = \"gpustack\"\n</code></pre> <p>Download all required packages:</p> <pre><code>pip wheel $PACKAGE_SPEC -w gpustack_offline_packages\n</code></pre> <p>Install GPUStack to use its CLI:</p> <pre><code>pip install gpustack\n</code></pre> <p>Download dependency tools and save them as an archive:</p> <pre><code>gpustack download-tools --save-archive gpustack_offline_tools.tar.gz\n</code></pre> <p>If your online environment differs from the air-gapped environment, specify the OS, architecture, and device explicitly:</p> <pre><code>gpustack download-tools --save-archive gpustack_offline_tools.tar.gz --system linux --arch amd64 --device cuda\n</code></pre> <p>Note</p> <p>This instruction assumes that the online environment uses the same GPU type as the air-gapped environment. If the GPU types differ, use the <code>--device</code> flag to specify the device type for the air-gapped environment. Refer to the download-tools command for more information.</p>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#step-2-transfer-the-packages","title":"Step 2: Transfer the Packages","text":"<p>Transfer the following files from the online environment to the air-gapped environment.</p> <ul> <li><code>gpustack_offline_packages</code> directory.</li> <li><code>gpustack_offline_tools.tar.gz</code> file.</li> </ul>"},{"location":"installation/nvidia-cuda/air-gapped-installation/#step-3-install-gpustack","title":"Step 3: Install GPUStack","text":"<p>In the air-gapped environment, run the following commands.</p> <p>Install GPUStack from the downloaded packages:</p> <pre><code>pip install --no-index --find-links=gpustack_offline_packages gpustack\n</code></pre> <p>Load and apply the pre-downloaded tools archive:</p> <pre><code>gpustack download-tools --load-archive gpustack_offline_tools.tar.gz\n</code></pre> <p>Now you can run GPUStack by following the instructions in the pip Installation guide.</p>"},{"location":"installation/nvidia-cuda/online-installation/","title":"Online Installation","text":""},{"location":"installation/nvidia-cuda/online-installation/#supported-devices","title":"Supported Devices","text":"<ul> <li> NVIDIA GPUs (Compute Capability 6.0 and above, check Your GPU Compute Capability)</li> </ul>"},{"location":"installation/nvidia-cuda/online-installation/#supported-platforms","title":"Supported Platforms","text":"OS Arch Supported methods Linux AMD64ARM64 Installation ScriptDocker Installation (Recommended)pip Installation Windows AMD64 Installation Scriptpip Installation"},{"location":"installation/nvidia-cuda/online-installation/#supported-backends","title":"Supported backends","text":"<ul> <li> vLLM (Compute Capability 7.0 and above, only supports AMD64 Linux)</li> <li> llama-box</li> <li> vox-box</li> </ul>"},{"location":"installation/nvidia-cuda/online-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Port Requirements</li> <li>CPU support for llama-box backend: AMD64 with AVX2, or ARM64 with NEON</li> </ul> LinuxWindows <p>Check if the CPU is supported:</p> AMD64ARM64 <pre><code>lscpu | grep avx2\n</code></pre> <pre><code>grep -E -i \"neon|asimd\" /proc/cpuinfo\n</code></pre> <p>Windows users need to manually verify support for the above instructions.</p> <ul> <li>NVIDIA Driver</li> </ul> <p>Check if the NVIDIA driver is installed:</p> <pre><code>nvidia-smi --format=csv,noheader --query-gpu=index,name,memory.total,memory.used,utilization.gpu,temperature.gpu\n</code></pre> <p>And ensure the driver supports CUDA 12.4 or higher:</p> LinuxWindows <pre><code>nvidia-smi | grep \"CUDA Version\"\n</code></pre> <pre><code>nvidia-smi | findstr \"CUDA Version\"\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#installation-script","title":"Installation Script","text":""},{"location":"installation/nvidia-cuda/online-installation/#prerequites","title":"Prerequites","text":"<ul> <li>NVIDIA CUDA Toolkit 12</li> </ul> <p>Check if CUDA is installed and verify that its version is at least 12.4:</p> <pre><code>nvcc -V\n</code></pre> <ul> <li>NVIDIA cuDNN 9 (Optional, required for audio models)</li> </ul> <p>Check if cuDNN 9 is installed:</p> LinuxWindows <pre><code>ldconfig -p | grep libcudnn\n</code></pre> <pre><code>Get-ChildItem -Path C:\\ -Recurse -Filter \"cudnn*.dll\" -ErrorAction SilentlyContinue\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#run-gpustack","title":"Run GPUStack","text":"<p>GPUStack provides a script to install it as a service with default port 80.</p> LinuxWindows <ul> <li>Install Server</li> </ul> <pre><code>curl -sfL https://get.gpustack.ai | sh -s -\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <ul> <li>(Optional) Add Worker</li> </ul> <p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>cat /var/lib/gpustack/token\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre> <ul> <li>Install Server</li> </ul> <pre><code>Invoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>To configure additional environment variables and startup flags when running the script, refer to the Installation Script.</p> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <p><pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>initial_admin_password</code> file will be located in the specified directory.</p> <ul> <li>(Optional) Add Worker</li> </ul> <p>To add workers to the GPUStack cluster, you need to specify the server URL and authentication token when installing GPUStack on the workers.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n</code></pre> <p>If you specify the <code>--data-dir</code> parameter to set the data directory, the <code>token</code> file will be located in the specified directory.</p> <p>To install GPUStack and start it as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL and token with your specific values:</p> <pre><code>Invoke-Expression \"&amp; { $((Invoke-WebRequest -Uri 'https://get.gpustack.ai' -UseBasicParsing).Content) } -- --server-url http://your_gpustack_url --token your_gpustack_token\"\n</code></pre> <p>After installed, ensure that the GPUStack startup logs are normal:</p> <pre><code>Get-Content \"$env:APPDATA\\gpustack\\log\\gpustack.log\" -Tail 200 -Wait\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#docker-installation","title":"Docker Installation","text":""},{"location":"installation/nvidia-cuda/online-installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>NVIDIA Container Toolkit</li> </ul> <p>Check if Docker and NVIDIA Container Toolkit are installed:</p> <pre><code>docker info | grep Runtimes | grep nvidia\n</code></pre> <ul> <li>Disabling Systemd Cgroup Management in Docker</li> </ul> <p>Note</p> <p>When systemd is used to manage the cgroups of the container and it is triggered to reload any Unit files that have references to NVIDIA GPUs (e.g. systemctl daemon-reload), containerized GPU workloads may suddenly lose access to their GPUs.</p> <p>In GPUStack, GPUs may be lost in the Resources menu, and running <code>nvidia-smi</code> inside the GPUStack container may result in the error: <code>Failed to initialize NVML: Unknown Error</code></p> <p>To prevent this issue, disabling systemd cgroup management in Docker is required.</p> <p>Set the parameter \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"] in the <code>/etc/docker/daemon.json</code> file and restart docker, such as:</p> <pre><code>vim /etc/docker/daemon.json\n</code></pre> <pre><code>{\n  \"runtimes\": {\n    \"nvidia\": {\n      \"args\": [],\n      \"path\": \"nvidia-container-runtime\"\n    }\n  },\n  \"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]\n}\n</code></pre> <pre><code>systemctl daemon-reload &amp;&amp; systemctl restart docker\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#run-gpustack_1","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    -p 80:80 \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --worker-ip your_host_ip\n</code></pre> <p>You can refer to the CLI Reference for available startup flags.</p> <p>Check if the startup logs are normal:</p> <pre><code>docker logs -f gpustack\n</code></pre> <p>If the logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/initial_admin_password\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#optional-add-worker","title":"(Optional) Add Worker","text":"<p>You can add more GPU nodes to GPUStack to form a GPU cluster. You need to add workers on other GPU nodes and specify the <code>--server-url</code> and <code>--token</code> parameters to join GPUStack.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> <pre><code>docker exec -it gpustack cat /var/lib/gpustack/token\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> Host NetworkPort Mapping <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --server-url http://your_gpustack_url --token your_gpustack_token\n</code></pre> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    -p 10150:10150 \\\n    -p 40064-40131:40064-40131 \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    gpustack/gpustack \\\n    --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre> <p>Note</p> <ol> <li> <p>Heterogeneous cluster is supported. No matter what type of device it is, you can add it to the current GPUStack as a worker by specifying the <code>--server-url</code> and <code>--token</code> parameters.</p> </li> <li> <p>You can set additional flags for the <code>gpustack start</code> command by appending them to the docker run command. For configuration details, please refer to the CLI Reference.</p> </li> <li> <p>You can either use the <code>--ipc=host</code> flag or <code>--shm-size</code> flag to allow the container to access the host\u2019s shared memory. It is used by vLLM and pyTorch to share data between processes under the hood, particularly for tensor parallel inference.</p> </li> <li> <p>The  <code>-p 40064-40131:40064-40131</code> flag is used to ensure connectivity for distributed inference across workers. For more details, please refer to the Port Requirements. You can omit this flag if you don't need distributed inference across workers.</p> </li> </ol>"},{"location":"installation/nvidia-cuda/online-installation/#build-your-own-docker-image","title":"Build Your Own Docker Image","text":"<p>For example, the official GPUStack NVIDIA CUDA image is built with CUDA 12.4. If you want to use a different CUDA version, you can build your own Docker image.</p> <pre><code># Example Dockerfile\nARG CUDA_VERSION=12.4.1\n\nFROM nvidia/cuda:$CUDA_VERSION-cudnn-runtime-ubuntu22.04\n\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    curl \\\n    wget \\\n    tzdata \\\n    iproute2 \\\n    python3 \\\n    python3-pip \\\n    python3-venv \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nCOPY . /workspace/gpustack\nRUN cd /workspace/gpustack &amp;&amp; \\\n    make build\n\nRUN if [ \"$TARGETPLATFORM\" = \"linux/amd64\" ]; then \\\n    # Install vllm dependencies for x86_64\n    WHEEL_PACKAGE=\"$(ls /workspace/gpustack/dist/*.whl)[all]\"; \\\n    else  \\\n    WHEEL_PACKAGE=\"$(ls /workspace/gpustack/dist/*.whl)[audio]\"; \\\n    fi &amp;&amp; \\\n    pip install pipx &amp;&amp; \\\n    pip install $WHEEL_PACKAGE &amp;&amp; \\\n    pip cache purge &amp;&amp; \\\n    rm -rf /workspace/gpustack\n\nRUN gpustack download-tools\n\nENTRYPOINT [ \"gpustack\", \"start\" ]\n</code></pre> <p>Run the following command to build the Docker image:</p> <pre><code>docker build -t gpustack:cuda-12.8 --build-arg CUDA_VERSION=12.8.1 .\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#pip-installation","title":"pip Installation","text":""},{"location":"installation/nvidia-cuda/online-installation/#prerequisites_2","title":"Prerequisites","text":"<ul> <li>Python 3.10 ~ 3.12</li> </ul> <p>Check the Python version:</p> <pre><code>python -V\n</code></pre> <ul> <li>NVIDIA CUDA Toolkit 12</li> </ul> <p>Check if CUDA is installed and verify that its version is at least 12.4:</p> <pre><code>nvcc -V\n</code></pre> <ul> <li>NVIDIA cuDNN 9 (Optional, required for audio models)</li> </ul> <p>Check if cuDNN 9 is installed:</p> LinuxWindows <pre><code>ldconfig -p | grep libcudnn\n</code></pre> <pre><code>Get-ChildItem -Path C:\\ -Recurse -Filter \"cudnn*.dll\" -ErrorAction SilentlyContinue\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#install-gpustack","title":"Install GPUStack","text":"<p>Run the following to install GPUStack.</p> Linux AMD64Linux ARM64 or Windows <pre><code># Extra dependencies options are \"vllm\", \"audio\" and \"all\"\n# \"vllm\" is only available for Linux AMD64\npip install \"gpustack[all]\"\n</code></pre> <pre><code>pip install \"gpustack[audio]\"\n</code></pre> <p>If you don\u2019t need the vLLM backend and support for audio models, just run:</p> <pre><code>pip install gpustack\n</code></pre> <p>To verify, run:</p> <pre><code>gpustack version\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#run-gpustack_2","title":"Run GPUStack","text":"<p>Run the following command to start the GPUStack server and built-in worker:</p> <pre><code>gpustack start\n</code></pre> <p>If the startup logs are normal, open <code>http://your_host_ip</code> in the browser to access the GPUStack UI. Log in to GPUStack with username <code>admin</code> and the default password. You can run the following command to get the password for the default setup:</p> LinuxWindows <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\initial_admin_password\" -Raw\n</code></pre> <p>By default, GPUStack uses <code>/var/lib/gpustack</code> as the data directory so you need <code>sudo</code> or proper permission for that. You can also set a custom data directory by running:</p> <pre><code>gpustack start --data-dir mypath\n</code></pre> <p>You can refer to the CLI Reference for available CLI Flags.</p>"},{"location":"installation/nvidia-cuda/online-installation/#optional-add-worker_1","title":"(Optional) Add Worker","text":"<p>To add a worker to the GPUStack cluster, you need to specify the server URL and the authentication token.</p> <p>To get the token used for adding workers, run the following command on the GPUStack server node:</p> LinuxWindows <pre><code>cat /var/lib/gpustack/token\n</code></pre> <pre><code>Get-Content -Path \"$env:APPDATA\\gpustack\\token\" -Raw\n</code></pre> <p>To start GPUStack as a worker, and register it with the GPUStack server, run the following command on the worker node. Be sure to replace the URL, token and node IP with your specific values:</p> <pre><code>gpustack start --server-url http://your_gpustack_url --token your_gpustack_token --worker-ip your_worker_host_ip\n</code></pre>"},{"location":"installation/nvidia-cuda/online-installation/#run-gpustack-as-a-system-service","title":"Run GPUStack as a System Service","text":"<p>A recommended way is to run GPUStack as a startup service. For example, using systemd:</p> <p>Create a service file in <code>/etc/systemd/system/gpustack.service</code>:</p> <pre><code>tee /etc/systemd/system/gpustack.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=GPUStack Service\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nEnvironmentFile=-/etc/default/%N\nExecStart=$(command -v gpustack) start\nRestart=always\nStandardOutput=append:/var/log/gpustack.log\nStandardError=append:/var/log/gpustack.log\n\n[Install]\nWantedBy=multi-user.target\nEOF\n</code></pre> <p>Then start GPUStack:</p> <pre><code>systemctl daemon-reload &amp;&amp; systemctl enable gpustack --now\n</code></pre> <p>Check the service status:</p> <pre><code>systemctl status gpustack\n</code></pre> <p>And ensure that the GPUStack startup logs are normal:</p> <pre><code>tail -200f /var/log/gpustack.log\n</code></pre>"},{"location":"integrations/interate-with-dify/","title":"Integrate with Dify","text":"<p>Dify can integrate with GPUStack to leverage locally deployed LLMs, embeddings, reranking, image generation, Speech-to-Text and Text-to-Speech capabilities.</p>"},{"location":"integrations/interate-with-dify/#deploying-models","title":"Deploying Models","text":"<p>In GPUStack UI, navigate to the <code>Models</code> page and click on <code>Deploy Model</code> to deploy the models you need.</p>"},{"location":"integrations/interate-with-dify/#create-an-api-key","title":"Create an API Key","text":"<ol> <li> <p>Navigate to the <code>API Keys</code> page and click on <code>New API Key</code>.</p> </li> <li> <p>Fill in the name, then click <code>Save</code>.</p> </li> <li> <p>Copy the API key and save it for later use.</p> </li> </ol>"},{"location":"integrations/interate-with-dify/#integrating-gpustack-into-dify","title":"Integrating GPUStack into Dify","text":"<p>Go to <code>Settings &gt; Model Provider &gt; GPUStack</code> and fill in:</p> <ul> <li> <p>Model Type: Select the model type based on the model.</p> </li> <li> <p>Model Name: The name must match the model name deployed on GPUStack.</p> </li> <li> <p>Server URL: <code>http://your-gpustack-url</code>, the URL should not include the path and cannot be <code>localhost</code>, as <code>localhost</code> is limited to the container\u2019s internal network. Ensure the URL is accessible from within the Dify container. You can test this by using <code>curl</code>.</p> </li> <li> <p>API Key: Input the API key you copied from previous steps.</p> </li> </ul> <p>Click <code>Save</code> to add the model:</p> <p></p> <p>Select the added models in the <code>System Model Settings</code> and save:</p> <p></p> <p>You can now use the models in the application.</p>"},{"location":"integrations/interate-with-ragflow/","title":"Integrate with RAGFlow","text":"<p>RAGFlow can integrate with GPUStack to leverage locally deployed LLMs, embeddings, reranking, Speech-to-Text and Text-to-Speech capabilities.</p>"},{"location":"integrations/interate-with-ragflow/#deploying-models","title":"Deploying Models","text":"<p>In GPUStack UI, navigate to the <code>Models</code> page and click on <code>Deploy Model</code> to deploy the models you need.</p>"},{"location":"integrations/interate-with-ragflow/#create-an-api-key","title":"Create an API Key","text":"<ol> <li> <p>Navigate to the <code>API Keys</code> page and click on <code>New API Key</code>.</p> </li> <li> <p>Fill in the name, then click <code>Save</code>.</p> </li> <li> <p>Copy the API key and save it for later use.</p> </li> </ol>"},{"location":"integrations/interate-with-ragflow/#integrating-gpustack-into-ragflow","title":"Integrating GPUStack into RAGFlow","text":"<p>Go to <code>Profile &gt; Model Providers &gt; GPUStack</code> and fill in:</p> <ul> <li> <p>Model type: Select the model type based on the model.</p> </li> <li> <p>Model name: The name must match the model name deployed on GPUStack.</p> </li> <li> <p>Base url: <code>http://your-gpustack-url</code>, the URL should not include the path and cannot be <code>localhost</code>, as <code>localhost</code> is limited to the container\u2019s internal network. Ensure the URL is accessible from within the RAGFlow container. You can test this by using <code>curl</code>.</p> </li> <li> <p>API-Key: Input the API key you copied from previous steps.</p> </li> </ul> <p>Click <code>Save</code> to add the model:</p> <p></p> <p>Select the added models in the <code>System Model Settings</code> and save:</p> <p></p> <p>You can now use the models in the application.</p>"},{"location":"integrations/openai-compatible-apis/","title":"OpenAI Compatible APIs","text":"<p>GPUStack serves OpenAI-compatible APIs using the <code>/v1-openai</code> path. Most of the APIs also work under the <code>/v1</code> path as an alias, except for the <code>models</code> endpoint, which is reserved for GPUStack management APIs.</p> <p>For all applications and frameworks that support the OpenAI-compatible API, you can integrate and use the models deployed on GPUStack through the OpenAI-compatible API provided by GPUStack.</p>"},{"location":"integrations/openai-compatible-apis/#supported-endpoints","title":"Supported Endpoints","text":"<p>The following API endpoints are supported:</p> <ul> <li> List Models</li> <li> Create Completion</li> <li> Create Chat Completion</li> <li> Create Embeddings</li> <li> Create Image</li> <li> Create Image Edit</li> <li> Create Speech</li> <li> Create Transcription</li> </ul>"},{"location":"integrations/openai-compatible-apis/#rerank-api","title":"Rerank API","text":"<p>In the context of Retrieval-Augmented Generation (RAG), reranking refers to the process of selecting the most relevant information from retrieved documents or knowledge sources before presenting them to the user or utilizing them for answer generation.</p> <p>It is important to note that the OpenAI-compatible APIs does not provide a <code>rerank</code> API, so GPUStack serves Jina compatible Rerank API using the <code>/v1/rerank</code> path.</p>"},{"location":"tutorials/inference-on-cpus/","title":"Running Inference on CPUs","text":"<p>GPUStack supports inference on CPUs, offering flexibility when GPU resources are limited or when model sizes exceed available GPU memory. The following CPU inference modes are available:</p> <ul> <li>CPU+GPU Hybrid Inference: Enables partial acceleration by offloading portions of large models to the CPU when VRAM capacity is insufficient.</li> <li>Full CPU Inference: Operates entirely on CPU when no GPU resources are available.</li> </ul> <p>Note</p> <p>CPU inference is supported when using the llama-box backend.</p> <p>To deploy a model with CPU offloading, enable the <code>Allow CPU Offloading</code> option in the deployment configuration (this setting is enabled by default).</p> <p></p> <p>After deployment, you can view the number of model layers offloaded to the CPU.</p> <p></p>"},{"location":"tutorials/inference-with-tool-calling/","title":"Inference with Tool Calling","text":"<p>Tool calling allows you to connect models to external tools and systems. This is useful for many things such as empowering AI assistants with capabilities, or building deep integrations between your applications and the models.</p> <p>In this tutorial, you\u2019ll learn how to set up and use tool calling within GPUStack to extend your AI\u2019s capabilities.</p> <p>Note</p> <ol> <li>Tool calling is supported in both llama-box and vLLM inference backends.</li> <li>Tool calling is essentially achieved through prompt engineering, requiring models to be trained with internalized templates to enable this capability. Therefore, not all LLMs support tool calling.</li> </ol>"},{"location":"tutorials/inference-with-tool-calling/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following:</p> <ul> <li>GPUStack is installed and running.</li> <li>A Linux worker node with a GPU is available. We'll use Qwen2.5-7B-Instruct as the model for this tutorial. The model requires a GPU with at least 18GB VRAM.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul>"},{"location":"tutorials/inference-with-tool-calling/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":""},{"location":"tutorials/inference-with-tool-calling/#deploy-from-catalog","title":"Deploy from Catalog","text":"<p>LLMs that support tool calling are marked with the <code>tools</code> capability in the catalog. When you select such a model from the catalog, tool calling is enabled by default.</p>"},{"location":"tutorials/inference-with-tool-calling/#example-of-custom-deployment-using-llama-box","title":"Example of Custom Deployment Using llama-box","text":"<p>When you deploy GGUF models using llama-box, tool calling is enabled by default for models that support it.</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI and click the <code>Deploy Model</code> button. In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Enable the <code>GGUF</code> checkbox to filter models by GGUF format.</li> <li>Use the search bar to find the <code>Qwen/Qwen2.5-7B-Instruct-GGUF</code> model.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p></p>"},{"location":"tutorials/inference-with-tool-calling/#example-of-custom-deployment-using-vllm","title":"Example of Custom Deployment Using vLLM","text":"<p>When you deploy models using vLLM, you need to enable tool calling with additional parameters.</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI and click the <code>Deploy Model</code> button. In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar to find the <code>Qwen/Qwen2.5-7B-Instruct</code> model.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button and add the following parameters:</li> </ol> <ul> <li><code>--enable-auto-tool-choice</code></li> <li><code>--tool-call-parser=hermes</code></li> </ul> <ol> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p>"},{"location":"tutorials/inference-with-tool-calling/#step-2-generate-an-api-key","title":"Step 2: Generate an API Key","text":"<p>We will use the GPUStack API to interact with the model. To do this, you need to generate an API key:</p> <ol> <li>Navigate to the <code>API Keys</code> page in the GPUStack UI.</li> <li>Click the <code>New API Key</code> button.</li> <li>Enter a name for the API key and click the <code>Save</code> button.</li> <li>Copy the generated API key for later use.</li> </ol>"},{"location":"tutorials/inference-with-tool-calling/#step-3-do-inference","title":"Step 3: Do Inference","text":"<p>With the model deployed and an API key, you can call the model via the GPUStack API. Here is an example script using <code>curl</code> (replace <code>&lt;your-server-url&gt;</code> with your GPUStack server URL and <code>&lt;your-api-key&gt;</code> with the API key generated in the previous step):</p> <pre><code>export GPUSTACK_SERVER_URL=&lt;your-server-url&gt;\nexport GPUSTACK_API_KEY=&lt;your-api-key&gt;\ncurl $GPUSTACK_SERVER_URL/v1-openai/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n-d '{\n  \"model\": \"qwen2.5-7b-instruct\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What'\\''s the weather like in Boston today?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather in a given location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The city and state, e.g. San Francisco, CA\"\n            },\n            \"unit\": {\n              \"type\": \"string\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\"]\n        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"\n}'\n</code></pre> <p>Example response:</p> <pre><code>{\n  \"model\": \"qwen2.5-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"tool_calls\": [\n          {\n            \"id\": \"chatcmpl-tool-b99d32848b324eaea4bac5a5830d00b8\",\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"get_current_weather\",\n              \"arguments\": \"{\\\"location\\\": \\\"Boston, MA\\\", \\\"unit\\\": \\\"fahrenheit\\\"}\"\n            }\n          }\n        ]\n      },\n      \"finish_reason\": \"tool_calls\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 212,\n    \"total_tokens\": 242,\n    \"completion_tokens\": 30\n  }\n}\n</code></pre>"},{"location":"tutorials/performing-distributed-inference-across-workers-llama-box/","title":"Performing Distributed Inference Across Workers (llama-box)","text":"<p>This tutorial will guide you through the process of configuring and running distributed inference across multiple workers using GPUStack. Distributed inference allows you to handle larger language models by distributing the computational workload among multiple workers. This is particularly useful when individual workers do not have sufficient resources, such as VRAM, to run the entire model independently.</p>"},{"location":"tutorials/performing-distributed-inference-across-workers-llama-box/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following:</p> <ul> <li>A multi-node GPUStack cluster is installed and running.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul> <p>In this tutorial, we\u2019ll assume a cluster with two nodes, each equipped with an NVIDIA P40 GPU (22GB VRAM), as shown in the following image:</p> <p></p> <p>We aim to run a large language model that requires more VRAM than a single worker can provide. For this tutorial, we\u2019ll use the <code>Qwen/Qwen2.5-72B-Instruct</code> model with the <code>q2_k</code> quantization format. The required resources for running this model can be estimated using the gguf-parser tool:</p> <pre><code>gguf-parser --hf-repo Qwen/Qwen2.5-72B-Instruct-GGUF --hf-file qwen2.5-72b-instruct-q2_k-00001-of-00007.gguf --ctx-size=8192 --in-short --skip-architecture --skip-metadata --skip-tokenizer\n</code></pre> <pre><code>+--------------------------------------------------------------------------------------+\n| ESTIMATE                                                                             |\n+----------------------------------------------+---------------------------------------+\n|                      RAM                     |                 VRAM 0                |\n+--------------------+------------+------------+----------------+----------+-----------+\n| LAYERS (I + T + O) |     UMA    |   NONUMA   | LAYERS (T + O) |    UMA   |   NONUMA  |\n+--------------------+------------+------------+----------------+----------+-----------+\n|      1 + 0 + 0     | 259.89 MiB | 409.89 MiB |     80 + 1     | 2.50 GiB | 28.89 GiB |\n+--------------------+------------+------------+----------------+----------+-----------+\n</code></pre> <p>From the output, we can see that the estimated VRAM requirement for this model exceeds the 22GB VRAM available on each worker node. Thus, we need to distribute the inference across multiple workers to successfully run the model.</p>"},{"location":"tutorials/performing-distributed-inference-across-workers-llama-box/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":"<p>Follow these steps to deploy the model from Hugging Face, enabling distributed inference:</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Enable the <code>GGUF</code> checkbox to filter models by GGUF format.</li> <li>Use the search bar in the top left to search for the model name <code>Qwen/Qwen2.5-72B-Instruct-GGUF</code>.</li> <li>In the <code>Available Files</code> section, select the <code>q2_k</code> quantization format.</li> <li>Expand the <code>Advanced</code> section and scroll down. Verify that the <code>Allow Distributed Inference Across Workers</code> option is enabled (this is enabled by default). GPUStack will evaluate the available resources in the cluster and run the model in a distributed manner if required.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p></p>"},{"location":"tutorials/performing-distributed-inference-across-workers-llama-box/#step-2-verify-the-model-deployment","title":"Step 2: Verify the Model Deployment","text":"<p>Once the model is deployed, verify the deployment on the <code>Models</code> page, where you can view details about how the model is running across multiple workers.</p> <p></p> <p>You can also check worker and GPU resource usage by navigating to the <code>Resources</code> page.</p> <p></p> <p>Finally, go to the <code>Playground</code> page to interact with the model and verify that everything is functioning correctly.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/","title":"Running DeepSeek R1 671B with Distributed vLLM","text":"<p>This tutorial guides you through the process of configuring and running the unquantized DeepSeek R1 671B using Distributed vLLM on a GPUStack cluster. Due to the extremely large size of the model, distributed inference across multiple workers is usually required.</p> <p>GPUStack enables easy setup and orchestration of distributed inference using vLLM, making it possible to run massive models like DeepSeek R1 with minimal manual configuration.</p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure the following requirements are met:</p> <ul> <li>You have access to a sufficient number of Linux nodes, each equipped with the required GPUs. For example:</li> </ul> GPU Number of Nodes H100/H800:8 2 A100/A800-80GB:8 4 A100/A800:8 8 <ul> <li>High-speed interconnects such as NVLink or InfiniBand are recommended for optimal performance.</li> <li>Model files should be downloaded to the same path on each node. While GPUStack supports on-the-fly model downloading, pre-downloading is recommended as it can be time consuming depending on the network speed.</li> </ul> <p>Note</p> <ul> <li>In this tutorial, we assume a setup of 4 nodes, each equipped with 8 A800-80GB GPUs and connected via 200G InfiniBand.</li> <li>A100/A800 GPUs do not support the FP8 precision originally used by DeepSeek R1. Hence, we use the BF16 version from Unsloth.</li> </ul>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-1-install-gpustack-server","title":"Step 1: Install GPUStack Server","text":"<p>In this tutorial, we will use Docker to install GPUStack. You can also use other installation methods if you prefer.</p> <p>Use the following command to start the GPUStack server:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    -v /path/to/your/model:/path/to/your/model \\\n    -e NCCL_SOCKET_IFNAME=eth2 \\\n    -e GLOO_SOCKET_IFNAME=eth2 \\\n    gpustack/gpustack --enable-ray\n</code></pre> <p>Note</p> <ul> <li>Replace <code>/path/to/your/model</code> with the actual path.</li> <li>Set <code>NCCL_SOCKET_IFNAME</code> and <code>GLOO_SOCKET_IFNAME</code> to the network interface used for inter-node communication. We use eth2 as an example.</li> <li>The <code>--enable-ray</code> flag enables Ray for distributed inference, which is required by vLLM.</li> </ul> <p>After GPUStack server is up and running, run the following commands to get the initial admin password and the token for worker registration:</p> <pre><code>docker exec gpustack cat /var/lib/gpustack/initial_admin_password\ndocker exec gpustack cat /var/lib/gpustack/token\n</code></pre>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-2-install-gpustack-workers","title":"Step 2: Install GPUStack Workers","text":"<p>On each worker node, run the following command to start a GPUStack worker:</p> <pre><code>docker run -d --name gpustack \\\n    --restart=unless-stopped \\\n    --gpus all \\\n    --network=host \\\n    --ipc=host \\\n    -v gpustack-data:/var/lib/gpustack \\\n    -v /path/to/your/model:/path/to/your/model \\\n    -e NCCL_SOCKET_IFNAME=eth2 \\\n    -e GLOO_SOCKET_IFNAME=eth2 \\\n    gpustack/gpustack \\\n    --server-url http://your_gpustack_server_ip_or_hostname \\\n    --token your_gpustack_token \\\n    --enable-ray\n</code></pre> <p>Note</p> <ul> <li>Replace the placeholder paths, IP address/hostname, and token accordingly.</li> <li>Ensure the model path matches that of the server and is valid on all worker nodes.</li> </ul>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-3-access-gpustack-ui","title":"Step 3: Access GPUStack UI","text":"<p>Once the server and all workers are running, access the GPUStack UI via your browser:</p> <pre><code>http://your_gpustack_server_ip_or_hostname\n</code></pre> <p>Log in using the <code>admin</code> username and the password obtained in Step 1. Navigate to the <code>Resources</code> page to verify that all workers are in the Ready state and their GPUs are listed.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-4-deploy-the-deepseek-r1-model","title":"Step 4: Deploy the DeepSeek R1 Model","text":"<ol> <li>Go to the <code>Models</code> page.</li> <li>Click <code>Deploy Model</code>.</li> <li>Select <code>Local Path</code> as your source.</li> <li>Enter a name (e.g., <code>DeepSeek-R1</code>) in the <code>Name</code> field.</li> <li>Specify the <code>Model Path</code> as the directory that contains the DeepSeek R1 model files on each worker node.</li> <li>Ensure the <code>Backend</code> is set to <code>vLLM</code>.</li> <li>After passing the compatibility check, click <code>Save</code> to deploy.</li> </ol>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-5-monitor-deployment","title":"Step 5: Monitor Deployment","text":"<p>You can monitor the deployment status on the <code>Models</code> page. Hover over <code>distributed across workers</code> to view GPU and worker usage. Click <code>View Logs</code> to see real-time logs showing model loading progress. It may take a few minutes to load the model.</p> <p></p> <p>After the model is running, revisit the Resources tab to check GPU utilization. By default, vLLM uses 90% of GPU memory. You may adjust this in the model configuration settings.</p> <p></p>"},{"location":"tutorials/running-deepseek-r1-671b-with-distributed-vllm/#step-6-run-inference-via-playground","title":"Step 6: Run Inference via Playground","text":"<p>Once the model is deployed and running, you can test it using the GPUStack Playground.</p> <ol> <li>Navigate to the <code>Playground</code> -&gt; <code>Chat</code>.</li> <li>If only one model is deployed, it will be selected by default. Otherwise, use the dropdown menu to choose <code>DeepSeek-R1</code>.</li> <li>Enter prompts and interact with the model.</li> </ol> <p></p> <p>You can also use the <code>Compare</code> tab to test conccurrent inference scenarios.</p> <p></p> <p>You have now successfully deployed and run DeepSeek R1 671B using Distributed vLLM on a GPUStack cluster. Explore the model\u2019s performance and capabilities in your own applications.</p> <p>For further assistance, feel free to reach out to the GPUStack community or support team.</p>"},{"location":"tutorials/running-on-copilot-plus-pcs-with-snapdragon-x/","title":"Running Inference on Copilot+ PCs with Snapdragon X","text":"<p>GPUStack supports running on ARM64 Windows, enabling use on Snapdragon X-based Copilot+ PCs.</p> <p>Note</p> <p>Only CPU-based inference is supported on Snapdragon X devices. GPUStack does not currently support GPU or NPU acceleration on this platform.</p>"},{"location":"tutorials/running-on-copilot-plus-pcs-with-snapdragon-x/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Copilot+ PC with Snapdragon X. In this tutorial, we use the Dell XPS 13 9345.</li> <li>Install AMD64 Python (version 3.10 to 3.12). See details</li> </ul>"},{"location":"tutorials/running-on-copilot-plus-pcs-with-snapdragon-x/#installing-gpustack","title":"Installing GPUStack","text":"<p>Run PowerShell as administrator (avoid using PowerShell ISE), then run the following command to install GPUStack:</p> <pre><code>Invoke-Expression (Invoke-WebRequest -Uri \"https://get.gpustack.ai\" -UseBasicParsing).Content\n</code></pre> <p>After installation, follow the on-screen instructions to obtain credentials and log in to the GPUStack UI.</p>"},{"location":"tutorials/running-on-copilot-plus-pcs-with-snapdragon-x/#deploying-a-model","title":"Deploying a Model","text":"<ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button and select <code>Ollama Library</code> from the dropdown.</li> <li>Enter <code>llama3.2</code> in the <code>Name</code> field.</li> <li>Select <code>llama3.2</code> from the <code>Ollama Model</code> dropdown.</li> <li>Click <code>Save</code> to deploy the model.</li> </ol> <p>Once deployed, you can monitor the model's status on the <code>Models</code> page.</p> <p></p>"},{"location":"tutorials/running-on-copilot-plus-pcs-with-snapdragon-x/#running-inference","title":"Running Inference","text":"<p>Navigate to the <code>Playground</code> page in the GPUStack UI, where you can interact with the deployed model.</p> <p></p>"},{"location":"user-guide/api-key-management/","title":"API Key Management","text":"<p>GPUStack supports authentication using API keys. Each GPUStack user can generate and manage their own API keys.</p>"},{"location":"user-guide/api-key-management/#create-api-key","title":"Create API Key","text":"<ol> <li>Navigate to the <code>API Keys</code> page.</li> <li>Click the <code>New API Key</code> button.</li> <li>Fill in the <code>Name</code>, <code>Description</code>, and select the <code>Expiration</code> of the API key.</li> <li>Click the <code>Save</code> button.</li> <li>Copy and store the key somewhere safe, then click the <code>Done</code> button.</li> </ol> <p>Note</p> <p>Please note that you can only see the generated API key once upon creation.</p>"},{"location":"user-guide/api-key-management/#delete-api-key","title":"Delete API Key","text":"<ol> <li>Navigate to the <code>API Keys</code> page.</li> <li>Find the API key you want to delete.</li> <li>Click the <code>Delete</code> button in the <code>Operations</code> column.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/api-key-management/#use-api-key","title":"Use API Key","text":"<p>GPUStack supports using the API key as a bearer token. The following is an example using curl:</p> <pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"user-guide/compatibility-check/","title":"Compatibility Check","text":"<p>GPUStack performs a compatibility check prior to model deployment. This check provides detailed information about the model\u2019s compatibility with the current GPUStack environment. The following compatibility checks are performed:</p>"},{"location":"user-guide/compatibility-check/#inference-backend-compatibility","title":"Inference Backend Compatibility","text":"<p>Checks whether the selected inference backend is compatible with the current environment, including operating system, GPU, and architecture.</p>"},{"location":"user-guide/compatibility-check/#model-compatibility","title":"Model Compatibility","text":"<p>Determines whether the model is supported by the selected inference backend. This includes checking for supported model formats and architectures (e.g., <code>LlamaForCausalLM</code>, <code>Qwen3ForCausalLM</code>, etc.). This check is based on built-in inference backends and their supported models. If a custom backend version is used, this check will be skipped.</p>"},{"location":"user-guide/compatibility-check/#schedulability-check","title":"Schedulability Check","text":"<p>Evaluates whether the model can be scheduled in the current environment. This includes verifying available resources such as RAM and VRAM, as well as configured scheduling rules.</p>"},{"location":"user-guide/compatibility-check/#scheduling-rules","title":"Scheduling Rules","text":"<p>Scheduling rules (including worker selectors, GPU selectors, and scheduling policies) are used to determine whether a model can be scheduled in the current environment.</p>"},{"location":"user-guide/compatibility-check/#resource-check","title":"Resource Check","text":"<p>The resource check ensures that sufficient system resources are available to deploy the model. GPUStack estimates the required resources and compares them with available resources in the environment. Estimations are performed using the following methods:</p> <ol> <li>For GGUF models: GPUStack uses the GGUF parser to estimate the model's resource requirements.</li> <li>For other models: GPUStack estimates VRAM usage using the following formula:</li> </ol> \\[ \\text{VRAM} = \\text{WEIGHT\\_SIZE} \\times 1.2 + \\text{FRAMEWORK\\_FOOTPRINT} \\] <ul> <li><code>WEIGHT_SIZE</code> refers to the size of the model weights in bytes.</li> <li><code>FRAMEWORK_FOOTPRINT</code> is a constant representing the framework\u2019s memory overhead. For example, vLLM may use several gigabytes of VRAM for CUDA graphs.</li> <li>The 1.2x multiplier is an empirical estimate. For more details, refer to this explanation.</li> </ul> <p>This formula provides a rough estimate and may not be accurate for all models. Typically, it reflects a lower-bound estimate of the required VRAM. If the estimation is insufficient, users can perform fine-grained scheduling by manually selecting workers and GPUs, or by adjusting advanced backend parameters. For instance, with vLLM, users can specify <code>--tensor-parallel-size</code> and <code>--pipeline-parallel-size</code> to control GPU allocation for the model.</p>"},{"location":"user-guide/image-generation-apis/","title":"Image Generation APIs","text":"<p>GPUStack provides APIs for generating images given a prompt and/or an input image when running diffusion models.</p> <p>Note</p> <p>The image generation APIs are only available when using the llama-box inference backend.</p>"},{"location":"user-guide/image-generation-apis/#supported-models","title":"Supported Models","text":"<p>The following models are available for image generation:</p> <p>Tip</p> <p>Please use the converted GGUF models provided by GPUStack. Check the model link for more details.</p> <ul> <li>stabilityai/stable-diffusion-3.5-large-turbo [Hugging Face], [ModelScope]</li> <li>stabilityai/stable-diffusion-3.5-large [Hugging Face], [ModelScope]</li> <li>stabilityai/stable-diffusion-3.5-medium [Hugging Face], [ModelScope]</li> <li>stabilityai/stable-diffusion-3-medium [Hugging Face], [ModelScope]</li> <li>TencentARC/FLUX.1-mini [Hugging Face], [ModelScope]</li> <li>Freepik/FLUX.1-lite [Hugging Face], [ModelScope]</li> <li>black-forest-labs/FLUX.1-dev [Hugging Face], [ModelScope]</li> <li>black-forest-labs/FLUX.1-schnell [Hugging Face], [ModelScope]</li> <li>stabilityai/sdxl-turbo [Hugging Face], [ModelScope]</li> <li>stabilityai/stable-diffusion-xl-refiner-1.0 [Hugging Face], [ModelScope]</li> <li>stabilityai/stable-diffusion-xl-base-1.0 [Hugging Face], [ModelScope]</li> <li>stabilityai/sd-turbo [Hugging Face], [ModelScope]</li> <li>stabilityai/stable-diffusion-2-1 [Hugging Face], [ModelScope]</li> <li>stable-diffusion-v1-5/stable-diffusion-v1-5 [Hugging Face], [ModelScope]</li> <li>CompVis/stable-diffusion-v1-4 [Hugging Face], [ModelScope]</li> </ul>"},{"location":"user-guide/image-generation-apis/#api-details","title":"API Details","text":"<p>The image generation APIs adhere to OpenAI API specification. While OpenAI APIs for image generation are simple and opinionated, GPUStack extends these capabilities with additional features.</p>"},{"location":"user-guide/image-generation-apis/#create-image","title":"Create Image","text":""},{"location":"user-guide/image-generation-apis/#streaming","title":"Streaming","text":"<p>This image generation API supports streaming responses to return the progressing of the generation. To enable streaming, set the <code>stream</code> parameter to <code>true</code> in the request body. Example:</p> <pre><code>REQUEST : (application/json)\n{\n  \"n\": 1,\n  \"response_format\": \"b64_json\",\n  \"size\": \"512x512\",\n  \"prompt\": \"A lovely cat\",\n  \"quality\": \"standard\",\n  \"stream\": true,\n  \"stream_options\": {\n    \"include_usage\": true, // return usage information\n  }\n}\n\nRESPONSE : (text/event-stream)\ndata: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\ndata: [DONE]\n</code></pre>"},{"location":"user-guide/image-generation-apis/#advanced-options","title":"Advanced Options","text":"<p>This image generation API supports additional options to control the generation process. The following options are available:</p> <pre><code>REQUEST : (application/json)\n{\n  \"n\": 1,\n  \"response_format\": \"b64_json\",\n  \"size\": \"512x512\",\n  \"prompt\": \"A lovely cat\",\n  \"sampler\": \"euler\",      // required, select from euler_a;euler;heun;dpm2;dpm++2s_a;dpm++2m;dpm++2mv2;ipndm;ipndm_v;lcm\n  \"schedule\": \"default\",   // optional, select from default;discrete;karras;exponential;ays;gits\n  \"seed\": null,            // optional, random seed\n  \"cfg_scale\": 4.5,        // optional, for sampler, the scale of classifier-free guidance in the output phase\n  \"sample_steps\": 20,      // optional, number of sample steps\n  \"negative_prompt\": \"\",   // optional, negative prompt\n  \"stream\": true,\n  \"stream_options\": {\n    \"include_usage\": true, // return usage information\n  }\n}\n\nRESPONSE : (text/event-stream)\ndata: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n...\ndata: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\ndata: [DONE]\n</code></pre>"},{"location":"user-guide/image-generation-apis/#create-image-edit","title":"Create Image Edit","text":""},{"location":"user-guide/image-generation-apis/#streaming_1","title":"Streaming","text":"<p>This image generation API supports streaming responses to return the progressing of the generation. To enable streaming, set the <code>stream</code> parameter to <code>true</code> in the request body. Example:</p> <pre><code>REQUEST: (multipart/form-data)\nn=1\nresponse_format=b64_json\nsize=512x512\nprompt=\"A lovely cat\"\nquality=standard\nimage=...                         // required\nmask=...                          // optional\nstream=true\nstream_options_include_usage=true // return usage information\n\nRESPONSE : (text/event-stream)\nCASE 1: correct input image\n  data: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\n  data: [DONE]\nCASE 2: illegal input image\n  error: {\"code\": 400, \"message\": \"Invalid image\", \"type\": \"invalid_request_error\"}\n</code></pre>"},{"location":"user-guide/image-generation-apis/#advanced-options_1","title":"Advanced Options","text":"<p>This image generation API supports additional options to control the generation process. The following options are available:</p> <pre><code>REQUEST: (multipart/form-data)\nn=1\nresponse_format=b64_json\nsize=512x512\nprompt=\"A lovely cat\"\nimage=...                         // required\nmask=...                          // optional\nsampler=euler                     // required, select from euler_a;euler;heun;dpm2;dpm++2s_a;dpm++2m;dpm++2mv2;ipndm;ipndm_v;lcm\nschedule=default                  // optional, select from default;discrete;karras;exponential;ays;gits\nseed=null                         // optional, random seed\ncfg_scale=4.5                     // optional, for sampler, the scale of classifier-free guidance in the output phase\nsample_steps=20                   // optional, number of sample steps\nnegative_prompt=\"\"                // optional, negative prompt\nstream=true\nstream_options_include_usage=true // return usage information\n\nRESPONSE : (text/event-stream)\nCASE 1: correct input image\n  data: {\"created\":1731916353,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":10.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":50.0}], ...}\n  ...\n  data: {\"created\":1731916371,\"data\":[{\"index\":0,\"object\":\"image.chunk\",\"progress\":100.0,\"b64_json\":\"...\"}], \"usage\":{\"generation_per_second\":...,\"time_per_generation_ms\":...,\"time_to_process_ms\":...}, ...}\n  data: [DONE]\nCASE 2: illegal input image\n  error: {\"code\": 400, \"message\": \"Invalid image\", \"type\": \"invalid_request_error\"}\n</code></pre>"},{"location":"user-guide/image-generation-apis/#usage","title":"Usage","text":"<p>The followings are examples using the image generation APIs:</p>"},{"location":"user-guide/image-generation-apis/#curl-create-image","title":"curl (Create Image)","text":"<pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1-openai/image/generate \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"n\": 1,\n        \"response_format\": \"b64_json\",\n        \"size\": \"512x512\",\n        \"prompt\": \"A lovely cat\",\n        \"quality\": \"standard\",\n        \"stream\": true,\n        \"stream_options\": {\n        \"include_usage\": true\n        }\n    }'\n</code></pre>"},{"location":"user-guide/image-generation-apis/#curl-create-image-edit","title":"curl (Create Image Edit)","text":"<pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1-openai/image/edit \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -F image=\"@otter.png\" \\\n    -F mask=\"@mask.png\" \\\n    -F prompt=\"A lovely cat\" \\\n    -F n=1 \\\n    -F size=\"512x512\"\n</code></pre>"},{"location":"user-guide/inference-backends/","title":"Inference Backends","text":"<p>GPUStack supports the following inference backends:</p> <ul> <li>llama-box</li> <li>vLLM</li> <li>vox-box</li> <li>Ascend MindIE</li> </ul> <p>When users deploy a model, the backend is selected automatically based on the following criteria:</p> <ul> <li>If the model is a GGUF model, <code>llama-box</code> is used.</li> <li>If the model is a known <code>Text-to-Speech</code> or <code>Speech-to-Text</code> model, <code>vox-box</code> is used.</li> <li>Otherwise, <code>vLLM</code> is used.</li> </ul>"},{"location":"user-guide/inference-backends/#llama-box","title":"llama-box","text":"<p>llama-box is a LM inference server based on llama.cpp and stable-diffusion.cpp.</p>"},{"location":"user-guide/inference-backends/#supported-platforms","title":"Supported Platforms","text":"<p>The llama-box backend supports Linux, macOS and Windows (with CPU offloading only on Windows ARM architecture) platforms.</p>"},{"location":"user-guide/inference-backends/#supported-models","title":"Supported Models","text":"<ul> <li>LLMs: For supported LLMs, refer to the llama.cpp README.</li> <li>Diffussion Models: Supported models are listed in this Hugging Face collection or this ModelScope collection.</li> <li>Reranker Models: Supported models can be found in this Hugging Face collection or this ModelScope collection.</li> </ul>"},{"location":"user-guide/inference-backends/#supported-features","title":"Supported Features","text":""},{"location":"user-guide/inference-backends/#allow-cpu-offloading","title":"Allow CPU Offloading","text":"<p>After enabling CPU offloading, GPUStack prioritizes loading as many layers as possible onto the GPU to optimize performance. If GPU resources are limited, some layers will be offloaded to the CPU, with full CPU inference used only when no GPU is available.</p>"},{"location":"user-guide/inference-backends/#allow-distributed-inference-across-workers","title":"Allow Distributed Inference Across Workers","text":"<p>Enable distributed inference across multiple workers. The primary Model Instance will communicate with backend instances on one or more others workers, offloading computation tasks to them.</p>"},{"location":"user-guide/inference-backends/#multimodal-language-models","title":"Multimodal Language Models","text":"<p>Llama-box supports the following multimodal language models. When using a vision language model, image inputs are supported in the chat completion API.</p> <ul> <li>Qwen2-VL</li> </ul> <p>Note</p> <p>When deploying a vision language model, GPUStack downloads and uses the multimodal projector file with the pattern <code>*mmproj*.gguf</code> by default. If multiple files match the pattern, GPUStack selects the file with higher precision (e.g., <code>f32</code> over <code>f16</code>). If the default pattern does not match the projector file or you want to use a specific one, you can customize the multimodal projector file by setting the <code>--mmproj</code> parameter in the model configuration. You can specify the relative path to the projector file in the model source. This syntax acts as shorthand, and GPUStack will download the file from the source and normalize the path when using it.</p>"},{"location":"user-guide/inference-backends/#parameters-reference","title":"Parameters Reference","text":"<p>See the full list of supported parameters for llama-box here.</p>"},{"location":"user-guide/inference-backends/#vllm","title":"vLLM","text":"<p>vLLM is a high-throughput and memory-efficient LLMs inference engine. It is a popular choice for running LLMs in production. vLLM seamlessly supports most state-of-the-art open-source models, including: Transformer-like LLMs (e.g., Llama), Mixture-of-Expert LLMs (e.g., Mixtral), Embedding Models (e.g. E5-Mistral), Multi-modal LLMs (e.g., LLaVA)</p> <p>By default, GPUStack estimates the VRAM requirement for the model instance based on the model's metadata. You can customize the parameters to fit your needs. The following vLLM parameters might be useful:</p> <ul> <li><code>--gpu-memory-utilization</code> (default: 0.9): The fraction of GPU memory to use for the model instance.</li> <li><code>--max-model-len</code>: Model context length. For large-context models, GPUStack automatically sets this parameter to <code>8192</code> to simplify model deployment, especially in resource constrained environments. You can customize this parameter to fit your needs.</li> <li><code>--tensor-parallel-size</code>: Number of tensor parallel replicas. By default, GPUStack sets this parameter given the GPU resources available and the estimation of the model's memory requirement. You can customize this parameter to fit your needs.</li> </ul> <p>For more details, please refer to vLLM documentation.</p>"},{"location":"user-guide/inference-backends/#supported-platforms_1","title":"Supported Platforms","text":"<p>The vLLM backend works on AMD64 Linux.</p> <p>Note</p> <ol> <li>When users install GPUStack on amd64 Linux using the installation script, vLLM is automatically installed.</li> <li>When users deploy a model using the vLLM backend, GPUStack sets worker label selectors to <code>{\"os\": \"linux\", \"arch\": \"amd64\"}</code> by default to ensure the model instance is scheduled to proper workers. You can customize the worker label selectors in the model configuration.</li> </ol>"},{"location":"user-guide/inference-backends/#supported-models_1","title":"Supported Models","text":"<p>Please refer to the vLLM documentation for supported models.</p>"},{"location":"user-guide/inference-backends/#supported-features_1","title":"Supported Features","text":""},{"location":"user-guide/inference-backends/#multimodal-language-models_1","title":"Multimodal Language Models","text":"<p>vLLM supports multimodal language models listed here. When users deploy a vision language model using the vLLM backend, image inputs are supported in the chat completion API.</p>"},{"location":"user-guide/inference-backends/#distributed-inference-across-workers-experimental","title":"Distributed Inference Across Workers (Experimental)","text":"<p>vLLM supports distributed inference across multiple workers using Ray. You can enable a Ray cluster in GPUStack by using the <code>--enable-ray</code> start parameter, allowing vLLM to run distributed inference across multiple workers.</p> <p>Known Limitations</p> <ol> <li>The GPUStack server and all participating workers must run on Linux and use the same version of Python, which is a requirement of Ray.</li> <li>Model files must be accessible at the same path on all participating workers. You must either use a shared file system or download the model files to the same path on all participating workers.</li> <li>Each worker can only be assigned to one distributed vLLM model instance at a time.</li> </ol> <p>Auto-scheduling is supported with the following conditions:</p> <ul> <li>Participating workers have the same number of GPUs.</li> <li>All GPUs in the worker satisfy the gpu_memory_utilization(defaults to 0.9) requirement.</li> <li>The total number of GPUs can be divided by the number of attention heads.</li> <li>The total VRAM claim is greater than the estimated VRAM claim.</li> </ul> <p>If the above conditions are not met, the model instance will not be scheduled automatically. However, you can manually schedule it by selecting the desired workers/GPUs in the model configuration.</p>"},{"location":"user-guide/inference-backends/#parameters-reference_1","title":"Parameters Reference","text":"<p>See the full list of supported parameters for vLLM here.</p>"},{"location":"user-guide/inference-backends/#vox-box","title":"vox-box","text":"<p>vox-box is an inference engine designed for deploying text-to-speech and speech-to-text models. It also provides an API that is fully compatible with the OpenAI audio API.</p>"},{"location":"user-guide/inference-backends/#supported-platforms_2","title":"Supported Platforms","text":"<p>The vox-box backend supports Linux, macOS and Windows platforms.</p> <p>Note</p> <ol> <li>To use Nvidia GPUs, ensure the following NVIDIA libraries are installed on workers:<ul> <li>cuBLAS for CUDA 12</li> <li>cuDNN 9 for CUDA 12</li> </ul> </li> <li>When users install GPUStack on Linux, macOS and Windows using the installation script, vox-box is automatically installed.</li> <li>CosyVoice models are natively supported on Linux AMD architecture and macOS. However, these models are not supported on Linux ARM or Windows architectures.</li> </ol>"},{"location":"user-guide/inference-backends/#supported-models_2","title":"Supported Models","text":"Model Type Link Supported Platforms Faster-whisper-large-v3 speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-large-v2 speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-large-v1 speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-medium speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-medium.en speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-small speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-small.en speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-distil-whisper-large-v3 speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-distil-whisper-large-v2 speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-distil-whisper-medium.en speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-tiny speech-to-text Hugging Face, ModelScope Linux, macOS, Windows Faster-whisper-tiny.en speech-to-text Hugging Face, ModelScope Linux, macOS, Windows CosyVoice-300M-Instruct text-to-speech Hugging Face, ModelScope Linux(ARM not supported), macOS, Windows(Not supported) CosyVoice-300M-SFT text-to-speech Hugging Face, ModelScope Linux(ARM not supported), macOS, Windows(Not supported) CosyVoice-300M text-to-speech Hugging Face, ModelScope Linux(ARM not supported), macOS, Windows(Not supported) CosyVoice-300M-25Hz text-to-speech ModelScope Linux(ARM not supported), macOS, Windows(Not supported) CosyVoice2-0.5B text-to-speech Hugging Face, ModelScope Linux(ARM not supported), macOS, Windows(Not supported)"},{"location":"user-guide/inference-backends/#supported-features_2","title":"Supported Features","text":""},{"location":"user-guide/inference-backends/#allow-gpucpu-offloading","title":"Allow GPU/CPU Offloading","text":"<p>vox-box supports deploying models to NVIDIA GPUs. If GPU resources are insufficient, it will automatically deploy the models to the CPU.</p>"},{"location":"user-guide/inference-backends/#ascend-mindie-experimental","title":"Ascend MindIE (Experimental)","text":"<p>Ascend MindIE is a high-performance inference service on Ascend hardware.</p>"},{"location":"user-guide/inference-backends/#supported-platforms_3","title":"Supported Platforms","text":"<p>The Ascend MindIE backend works on Linux platforms only, including ARM64 and x86_64 architectures.</p>"},{"location":"user-guide/inference-backends/#supported-models_3","title":"Supported Models","text":"<p>Ascend MindIE supports various models listed here.</p> <p>Within GPUStack, support large language models (LLMs) and multimodal language models (VLMs) . However, embedding models and multimodal generation models are not supported yet.</p>"},{"location":"user-guide/inference-backends/#supported-features_3","title":"Supported Features","text":"<p>Ascend MindIE owns a variety of features outlined here.</p> <p>At present, GPUStack supports a subset of these capabilities, including Quantization, Mixture of Experts(MoE), Prefix Caching, Function Calling, Multimodal Understanding, Multi-head Latent Attention(MLA).</p> <p>Note</p> <ol> <li>Quantization needs specific weight, and must adjust the model's <code>config.json</code>,    please follow the reference(guide) to prepare the correct weight.</li> <li>For Multimodal Understanding feature, some versions of Ascend MindIE's API are incompatible with OpenAI,    please track this issue for more support.</li> <li>Extending Context Size feature is WIP,    please track this issue for more details.</li> <li>Some features are mutually exclusive, so be careful when using them.</li> </ol>"},{"location":"user-guide/inference-backends/#parameters-reference_2","title":"Parameters Reference","text":"Parameter Default Description <code>--trust-remote-code</code> Trust remote code (for model loading). <code>--npu-memory-fraction</code> 0.9 Fraction of NPU memory to be used for the model executor (0 to 1). Example: 0.5 means 50% memory utilization. <code>--max-link-num</code> 1000 Maximum number of parallel requests. <code>--max-seq-len</code> 8192 Model context length. If unspecified, it will be derived from the model config. <code>--max-input-token-len</code> Maximum input token length. If unspecified, it will be derived from <code>--max-seq-len</code>. <code>--truncation</code> Truncate the input token length when it exceeds the minimum of <code>--max-input-token-len</code> and <code>--max-seq-len - 1</code>. <code>--cpu-mem-size</code> 5 CPU swap space size in GiB. If unspecified, the default value will be used. <code>--cache-block-size</code> 128 KV cache block size. Must be a power of 2. <code>--max-batch-size</code> 200 Maximum number of requests batched during decode stage. <code>--max-prefill-batch-size</code> 50 Maximum number of requests batched during prefill stage. Must be less than <code>--max-batch-size</code>. <code>--max-preempt-count</code> 0 Maximum number of preempted requests allowed during decoding. Must be less than <code>--max-batch-size</code>. <code>--max-queue-delay-microseconds</code> 5000 Maximum queue wait time in microseconds. <code>--prefill-time-ms-per-req</code> 150 Estimated prefill time per request (ms). Used to decide between prefill and decode stage. <code>--prefill-policy-type</code> 0 Prefill stage strategy:  <code>0</code>: FCFS (First Come First Serve)  <code>1</code>: STATE (same as FCFS)  <code>2</code>: PRIORITY (priority queue)  <code>3</code>: MLFQ (Multi-Level Feedback Queue) <code>--decode-time-ms-per-req</code> 50 Estimated decode time per request (ms). Used with <code>--prefill-time-ms-per-req</code> for batch selection. <code>--decode-policy-type</code> 0 Decode stage strategy:  <code>0</code>: FCFS  <code>1</code>: STATE (prioritize preempted or swapped requests)  <code>2</code>: PRIORITY  <code>3</code>: MLFQ <code>--support-select-batch</code> Enable batch selection. Determines execution priority based on <code>--prefill-time-ms-per-req</code> and <code>--decode-time-ms-per-req</code>. <code>--enable-prefix-caching</code> Enable prefix caching. Use <code>--no-enable-prefix-caching</code> to disable explicitly. <code>--enforce-eager</code> Emit operators in eager mode. <code>--metrics</code> Expose metrics at <code>/metrics</code> endpoint. <code>--log-level</code> Info Log level for MindIE. Options: <code>Verbose</code>, <code>Info</code>, <code>Warning</code>, <code>Warn</code>, <code>Error</code>, <code>Debug</code>."},{"location":"user-guide/model-catalog/","title":"Model Catalog","text":"<p>The Model Catalog is an index of popular models to help you quickly find and deploy models.</p>"},{"location":"user-guide/model-catalog/#browse-models","title":"Browse Models","text":"<p>You can browse the Model Catalog by navigating to the <code>Catalog</code> page. You can filter models by name and category. The following screenshot shows the Model Catalog page:</p> <p></p>"},{"location":"user-guide/model-catalog/#deploy-a-model-from-the-catalog","title":"Deploy a Model from the Catalog","text":"<p>You can deploy a model from the Model Catalog by clicking the model card. A model deployment configuration page will appear. You can review and customize the deployment configuration and click the <code>Save</code> button to deploy the model.</p>"},{"location":"user-guide/model-catalog/#customize-model-catalog","title":"Customize Model Catalog","text":"<p>You can customize the Model Catalog by providing a YAML file via GPUStack server configuration using the <code>--model-catalog-file</code> flag. It accepts either a local file path or a URL. You can refer to the built-in model catalog file here for the schema. It contains a list of model sets, each with model metadata and templates for deployment configurations.</p> <p>The following is an example model set in the model catalog file:</p> <pre><code>- name: Llama3.2\n  description: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n  home: https://www.llama.com/\n  icon: /static/catalog_icons/meta.png\n  categories:\n    - llm\n  capabilities:\n    - context/128k\n    - tools\n  sizes:\n    - 1\n    - 3\n  licenses:\n    - llama3.2\n  release_date: \"2024-09-25\"\n  order: 2\n  templates:\n    - quantizations:\n        - Q3_K_L\n        - Q4_K_M\n        - Q5_K_M\n        - Q6_K_L\n        - Q8_0\n        - f16\n      source: huggingface\n      huggingface_repo_id: bartowski/Llama-3.2-{size}B-Instruct-GGUF\n      huggingface_filename: \"*-{quantization}*.gguf\"\n      replicas: 1\n      backend: llama-box\n      cpu_offloading: true\n      distributed_inference_across_workers: true\n    - quantizations: [\"BF16\"]\n      source: huggingface\n      huggingface_repo_id: unsloth/Llama-3.2-{size}B-Instruct\n      replicas: 1\n      backend: vllm\n      backend_parameters:\n        - --enable-auto-tool-choice\n        - --tool-call-parser=llama3_json\n        - --chat-template={data_dir}/chat_templates/tool_chat_template_llama3.2_json.jinja\n</code></pre>"},{"location":"user-guide/model-catalog/#using-model-catalog-in-air-gapped-environments","title":"Using Model Catalog in Air-Gapped Environments","text":"<p>The built-in model catalog sources models from either Hugging Face or ModelScope. If you are using GPUStack in an air-gapped environment without internet access, you can customize the model catalog to use a local-path model source. Here is an example:</p> <pre><code>- name: Llama3.2\n  description: The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n  home: https://www.llama.com/\n  icon: /static/catalog_icons/meta.png\n  categories:\n    - llm\n  capabilities:\n    - context/128k\n    - tools\n  sizes:\n    - 1\n    - 3\n  licenses:\n    - llama3.2\n  release_date: \"2024-09-25\"\n  order: 2\n  templates:\n    - quantizations:\n        - Q3_K_L\n        - Q4_K_M\n        - Q5_K_M\n        - Q6_K_L\n        - Q8_0\n        - f16\n      source: local_path\n      # assuming you have all the GGUF model files in /path/to/the/model/directory\n      local_path: /path/to/the/model/directory/Llama-3.2-{size}B-Instruct-{quantization}.gguf\n      replicas: 1\n      backend: llama-box\n      cpu_offloading: true\n      distributed_inference_across_workers: true\n    - quantizations: [\"BF16\"]\n      source: local_path\n      # assuming you have both /path/to/Llama-3.2-1B-Instruct and /path/to/Llama-3.2-3B-Instruct directories\n      local_path: /path/to/Llama-3.2-{size}B-Instruct\n      replicas: 1\n      backend: vllm\n      backend_parameters:\n        - --enable-auto-tool-choice\n        - --tool-call-parser=llama3_json\n        - --chat-template={data_dir}/chat_templates/tool_chat_template_llama3.2_json.jinja\n</code></pre>"},{"location":"user-guide/model-catalog/#template-variables","title":"Template Variables","text":"<p>The following template variables are available for the deployment configuration:</p> <ul> <li><code>{size}</code>: Model size in billion parameters.</li> <li><code>{quantization}</code>: Quantization method of the model.</li> <li><code>{data_dir}</code>: GPUStack data directory path.</li> </ul>"},{"location":"user-guide/model-file-management/","title":"Model File Management","text":"<p>GPUStack allows admins to download and manage model files.</p>"},{"location":"user-guide/model-file-management/#add-model-file","title":"Add Model File","text":"<p>GPUStack currently supports models from Hugging Face, ModelScope, Ollama, and local paths. To add model files, navigate to the <code>Resources</code> page and click the <code>Model Files</code> tab.</p>"},{"location":"user-guide/model-file-management/#add-a-hugging-face-model","title":"Add a Hugging Face Model","text":"<ol> <li>Click the <code>Add Model File</code> button and select <code>Hugging Face</code> from the dropdown.</li> <li> <p>Use the search bar in the top left to find a model by name, e.g., <code>Qwen/Qwen2.5-0.5B-Instruct</code>. To search only for GGUF models, check the <code>GGUF</code> checkbox.</p> </li> <li> <p>(Optional) For GGUF models, select the desired quantization format from <code>Available Files</code>.</p> </li> <li>Select the target worker to download the model file.</li> <li>(Optional) Specify a <code>Local Directory</code> to download the model to a custom path instead of the GPUStack cache directory.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#add-a-modelscope-model","title":"Add a ModelScope Model","text":"<ol> <li>Click the <code>Add Model File</code> button and select <code>ModelScope</code> from the dropdown.</li> <li>Use the search bar in the top left to find a model by name, e.g., <code>Qwen/Qwen2.5-0.5B-Instruct</code>. To search only for GGUF models, check the <code>GGUF</code> checkbox.</li> <li>(Optional) For GGUF models, select the desired quantization format from <code>Available Files</code>.</li> <li>Select the target worker to download the model file.</li> <li>(Optional) Specify a <code>Local Directory</code> to download the model to a custom path instead of the GPUStack cache directory.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#add-an-ollama-model","title":"Add an Ollama Model","text":"<ol> <li>Click the <code>Add Model File</code> button and select <code>Ollama Library</code> from the dropdown.</li> <li>Select a model from the dropdown list or input a custom Ollama model, e.g., <code>llama3</code>, <code>llama3:70b</code>, or <code>youraccount/llama3:70b</code>.</li> <li>Select the target worker to download the model file.</li> <li>(Optional) Specify a <code>Local Directory</code> to download the model to a custom path instead of the GPUStack cache directory.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#add-a-local-path-model","title":"Add a Local Path Model","text":"<p>You can add models from a local path. The path can be a directory (e.g., a Hugging Face model folder) or a file (e.g., a GGUF model) located on the worker.</p> <ol> <li>Click the <code>Add Model File</code> button and select <code>Local Path</code> from the dropdown.</li> <li>Enter the <code>Model Path</code>.</li> <li>Select the target worker.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#retry-download","title":"Retry Download","text":"<p>If a model file download fails, you can retry it:</p> <ol> <li>Navigate to the <code>Resources</code> page and click the <code>Model Files</code> tab.</li> <li>Locate the model file with an error status.</li> <li>Click the ellipsis button in the <code>Operations</code> column and select <code>Retry Download</code>.</li> <li>GPUStack will attempt to download the model file again from the specified source.</li> </ol>"},{"location":"user-guide/model-file-management/#deploy-model","title":"Deploy Model","text":"<p>Models can be deployed from model files. Since the model is stored on a specific worker, GPUStack will add a worker selector using the <code>worker-name</code> key to ensure proper scheduling.</p> <ol> <li>Navigate to the <code>Resources</code> page and click the <code>Model Files</code> tab.</li> <li>Find the model file you want to deploy.</li> <li>Click the <code>Deploy</code> button in the <code>Operations</code> column.</li> <li>Review or adjust the <code>Name</code>, <code>Replicas</code>, and other deployment parameters.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/model-file-management/#delete-model-file","title":"Delete Model File","text":"<ol> <li>Navigate to the <code>Resources</code> page and click the <code>Model Files</code> tab.</li> <li>Find the model file you want to delete.</li> <li>Click the ellipsis button in the <code>Operations</code> column and select <code>Delete</code>.</li> <li>(Optional) Check the <code>Also delete the file from disk</code> option.</li> <li>Click the <code>Delete</code> button to confirm.</li> </ol>"},{"location":"user-guide/model-management/","title":"Model Management","text":"<p>You can manage large language models in GPUStack by navigating to the <code>Models</code> page. A model in GPUStack contains one or multiple replicas of model instances. On deployment, GPUStack automatically computes resource requirements for the model instances from model metadata and schedules them to available workers accordingly.</p>"},{"location":"user-guide/model-management/#deploy-model","title":"Deploy Model","text":"<p>Currently, models from Hugging Face, ModelScope, Ollama and local paths are supported.</p>"},{"location":"user-guide/model-management/#deploying-a-hugging-face-model","title":"Deploying a Hugging Face Model","text":"<ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</p> </li> <li> <p>Search the model by name from Hugging Face using the search bar in the top left. For example, <code>microsoft/Phi-3-mini-4k-instruct-gguf</code>. If you only want to search for GGUF models, check the \"GGUF\" checkbox.</p> </li> <li> <p>Select a file with the desired quantization format from <code>Available Files</code>.</p> </li> <li> <p>Adjust the <code>Name</code> and <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Model Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-management/#deploying-a-modelscope-model","title":"Deploying a ModelScope Model","text":"<ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>ModelScope</code> in the dropdown.</p> </li> <li> <p>Search the model by name from ModelScope using the search bar in the top left. For example, <code>Qwen/Qwen2-0.5B-Instruct</code>. If you only want to search for GGUF models, check the \"GGUF\" checkbox.</p> </li> <li> <p>Select a file with the desired quantization format from <code>Available Files</code>.</p> </li> <li> <p>Adjust the <code>Name</code> and <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Model Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-management/#deploying-an-ollama-model","title":"Deploying an Ollama Model","text":"<ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>Ollama Library</code> in the dropdown.</p> </li> <li> <p>Fill in the <code>Name</code> of the model.</p> </li> <li> <p>Select an <code>Ollama Model</code> from the dropdown list, or input any Ollama model you need. For example, <code>llama3</code>, <code>llama3:70b</code> or <code>youraccount/llama3:70b</code>.</p> </li> <li> <p>Adjust the <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Model Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-management/#deploying-a-local-path-model","title":"Deploying a Local Path Model","text":"<p>You can deploy a model from a local path. The model path can be a directory (e.g., a downloaded Hugging Face model directory) or a file (e.g., a GGUF model file) located on workers. This is useful when running in an air-gapped environment.</p> <p>Note</p> <ol> <li>GPUStack does not check the validity of the model path for scheduling, which may lead to deployment failure if the model path is inaccessible. It is recommended to ensure the model path is accessible on all workers(e.g., using NFS, rsync, etc.). You can also use the worker selector configuration to deploy the model to specific workers.</li> <li>GPUStack cannot evaluate the model's resource requirements unless the server has access to the same model path. Consequently, you may observe empty VRAM/RAM allocations for a deployed model. To mitigate this, it is recommended to make the model files available on the same path on the server. Alternatively, you can customize backend parameters, such as <code>tensor-split</code>, to configure how the model is distributed across the GPUs.</li> </ol> <p>To deploy a local path model:</p> <ol> <li> <p>Click the <code>Deploy Model</code> button, then select <code>Local Path</code> in the dropdown.</p> </li> <li> <p>Fill in the <code>Name</code> of the model.</p> </li> <li> <p>Fill in the <code>Model Path</code>.</p> </li> <li> <p>Adjust the <code>Replicas</code> as needed.</p> </li> <li> <p>Expand the <code>Advanced</code> section for advanced configurations if needed. Please refer to the Advanced Model Configuration section for more details.</p> </li> <li> <p>Click the <code>Save</code> button.</p> </li> </ol>"},{"location":"user-guide/model-management/#edit-model","title":"Edit Model","text":"<ol> <li>Find the model you want to edit on the model list page.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>Update the attributes as needed. For example, change the <code>Replicas</code> to scale up or down.</li> <li>Click the <code>Save</code> button.</li> </ol> <p>Note</p> <p>After editing the model, the configuration will not be applied to existing model instances. You need to delete the existing model instances. GPUStack will recreate new instances based on the updated model configuration.</p>"},{"location":"user-guide/model-management/#stop-model","title":"Stop Model","text":"<p>Stopping a model will delete all model instances and release the resources. It is equivalent to scaling down the model to zero replicas.</p> <ol> <li>Find the model you want to stop on the model list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Stop</code>.</li> <li>Confirm the operation.</li> </ol>"},{"location":"user-guide/model-management/#start-model","title":"Start Model","text":"<p>Starting a model is equivalent to scaling up the model to one replica.</p> <ol> <li>Find the model you want to start on the model list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Start</code>.</li> </ol>"},{"location":"user-guide/model-management/#delete-model","title":"Delete Model","text":"<ol> <li>Find the model you want to delete on the model list page.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/model-management/#view-model-instance","title":"View Model Instance","text":"<ol> <li>Find the model you want to check on the model list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the model.</li> </ol>"},{"location":"user-guide/model-management/#delete-model-instance","title":"Delete Model Instance","text":"<ol> <li>Find the model you want to check on the model list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the model.</li> <li>Find the model instance you want to delete.</li> <li>Click the ellipsis button for the model instance in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol> <p>Note</p> <p>After a model instance is deleted, GPUStack will recreate a new instance to satisfy the expected replicas of the model if necessary.</p>"},{"location":"user-guide/model-management/#view-model-instance-logs","title":"View Model Instance Logs","text":"<ol> <li>Find the model you want to check on the model list page.</li> <li>Click the <code>&gt;</code> symbol to view the instance list of the model.</li> <li>Find the model instance you want to check.</li> <li>Click the <code>View Logs</code> button for the model instance in the <code>Operations</code> column.</li> </ol>"},{"location":"user-guide/model-management/#use-self-hosted-ollama-models","title":"Use Self-hosted Ollama Models","text":"<p>You can deploy self-hosted Ollama models by configuring the <code>--ollama-library-base-url</code> option in the GPUStack server. The <code>Ollama Library</code> URL should point to the base URL of the Ollama model registry. For example, <code>https://registry.mycompany.com</code>.</p> <p>Here is an example workflow to set up a registry, publish a model, and use it in GPUStack:</p> <pre><code># Run a self-hosted OCI registry\ndocker run -d -p 5001:5000 --name registry registry:2\n\n# Push a model to the registry using Ollama\nollama pull llama3\nollama cp llama3 localhost:5001/library/llama3\nollama push localhost:5001/library/llama3 --insecure\n\n# Start GPUStack server with the custom Ollama library URL\ncurl -sfL https://get.gpustack.ai | sh -s - --ollama-library-base-url http://localhost:5001\n</code></pre> <p>That's it! You can now deploy the model <code>llama3</code> from <code>Ollama Library</code> source in GPUStack as usual, but the model will now be fetched from the self-hosted registry.</p>"},{"location":"user-guide/model-management/#advanced-model-configuration","title":"Advanced Model Configuration","text":"<p>GPUStack supports tailored configurations for model deployment.</p>"},{"location":"user-guide/model-management/#model-category","title":"Model Category","text":"<p>The model category helps you organize and filter models. By default, GPUStack automatically detects the model category based on the model's metadata. You can also customize the category by selecting it from the dropdown list.</p>"},{"location":"user-guide/model-management/#schedule-type","title":"Schedule Type","text":""},{"location":"user-guide/model-management/#auto","title":"Auto","text":"<p>GPUStack automatically schedules model instances to appropriate GPUs/Workers based on current resource availability.</p> <ul> <li> <p>Placement Strategy</p> </li> <li> <p>Spread: Make the resources of the entire cluster relatively evenly distributed among all workers. It may produce more resource fragmentation on a single worker.</p> </li> <li> <p>Binpack: Prioritize the overall utilization of cluster resources, reducing resource fragmentation on Workers/GPUs.</p> </li> <li> <p>Worker Selector</p> </li> </ul> <p>When configured, the scheduler will deploy the model instance to the worker containing specified labels.</p> <ol> <li> <p>Navigate to the <code>Resources</code> page and edit the desired worker. Assign custom labels to the worker by adding them in the labels section.</p> </li> <li> <p>Go to the <code>Models</code> page and click on the <code>Deploy Model</code> button. Expand the <code>Advanced</code> section and input the previously assigned worker labels in the <code>Worker Selector</code> configuration. During deployment, the Model Instance will be allocated to the corresponding worker based on these labels.</p> </li> </ol>"},{"location":"user-guide/model-management/#manual","title":"Manual","text":"<p>This schedule type allows users to specify which GPU to deploy the model instance on.</p> <ul> <li>GPU Selector</li> </ul> <p>Select one or more GPUs from the list. The model instance will attempt to deploy to the selected GPU if resources permit.</p>"},{"location":"user-guide/model-management/#backend","title":"Backend","text":"<p>The inference backend. Currently, GPUStack supports three backends: llama-box, vLLM and vox-box. GPUStack automatically selects the backend based on the model's configuration.</p> <p>For more details, please refer to the Inference Backends section.</p>"},{"location":"user-guide/model-management/#backend-version","title":"Backend Version","text":"<p>Specify a backend version, such as <code>v1.0.0</code>. The version format and availability depend on the selected backend. This option is useful for ensuring compatibility or taking advantage of features introduced in specific backend versions. Refer to the Pinned Backend Versions section for more information.</p>"},{"location":"user-guide/model-management/#backend-parameters","title":"Backend Parameters","text":"<p>Input the parameters for the backend you want to customize when running the model. The parameter should be in the format <code>--parameter=value</code>, <code>--bool-parameter</code> or as separate fields for <code>--parameter</code> and <code>value</code>. For example, use <code>--ctx-size=8192</code> for llama-box.</p> <p>For full list of supported parameters, please refer to the Inference Backends section.</p>"},{"location":"user-guide/model-management/#environment-variables","title":"Environment Variables","text":"<p>Environment variables used when running the model. These variables are passed to the backend process at startup.</p>"},{"location":"user-guide/model-management/#allow-cpu-offloading","title":"Allow CPU Offloading","text":"<p>Note</p> <p>Available for llama-box backend only.</p> <p>After enabling CPU offloading, GPUStack prioritizes loading as many layers as possible onto the GPU to optimize performance. If GPU resources are limited, some layers will be offloaded to the CPU, with full CPU inference used only when no GPU is available.</p>"},{"location":"user-guide/model-management/#allow-distributed-inference-across-workers","title":"Allow Distributed Inference Across Workers","text":"<p>Note</p> <p>Available for llama-box and vLLM backends.</p> <p>Enable distributed inference across multiple workers. The primary Model Instance will communicate with backend instances on one or more other workers, offloading computation tasks to them.</p>"},{"location":"user-guide/model-management/#auto-restrat-on-error","title":"Auto-Restrat on Error","text":"<p>Enable automatic restart of the model instance if it encounters an error. This feature ensures high availability and reliability of the model instance. If an error occurs, GPUStack will automatically attempt to restart the model instance using an exponential backoff strategy. The delay between restart attempts increases exponentially, up to a maximum interval of 5 minutes. This approach prevents the system from being overwhelmed by frequent restarts in the case of persistent errors.</p>"},{"location":"user-guide/openai-compatible-apis/","title":"OpenAI Compatible APIs","text":"<p>GPUStack serves OpenAI-compatible APIs using the <code>/v1-openai</code> path. Most of the APIs also work under the <code>/v1</code> path as an alias, except for the <code>models</code> endpoint, which is reserved for GPUStack management APIs.</p>"},{"location":"user-guide/openai-compatible-apis/#supported-endpoints","title":"Supported Endpoints","text":"<p>The following API endpoints are supported:</p> <ul> <li> List Models</li> <li> Create Completion</li> <li> Create Chat Completion</li> <li> Create Embeddings</li> <li> Create Image</li> <li> Create Image Edit</li> <li> Create Speech</li> <li> Create Transcription</li> </ul>"},{"location":"user-guide/openai-compatible-apis/#usage","title":"Usage","text":"<p>The following are examples using the APIs in different languages:</p>"},{"location":"user-guide/openai-compatible-apis/#curl","title":"curl","text":"<pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1-openai/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -d '{\n    \"model\": \"llama3\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"user-guide/openai-compatible-apis/#openai-python-api-library","title":"OpenAI Python API library","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"http://your_gpustack_server_url/v1\", api_key=\"your_api_key\")\n\ncompletion = client.chat.completions.create(\n  model=\"llama3\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n</code></pre>"},{"location":"user-guide/openai-compatible-apis/#openai-node-api-library","title":"OpenAI Node API library","text":"<pre><code>const OpenAI = require(\"openai\");\n\nconst openai = new OpenAI({\n  apiKey: \"your_api_key\",\n  baseURL: \"http://your_gpustack_server_url/v1\",\n});\n\nasync function main() {\n  const params = {\n    model: \"llama3\",\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant.\",\n      },\n      {\n        role: \"user\",\n        content: \"Hello!\",\n      },\n    ],\n  };\n  const chatCompletion = await openai.chat.completions.create(params);\n  console.log(chatCompletion.choices[0].message);\n}\nmain();\n</code></pre>"},{"location":"user-guide/pinned-backend-versions/","title":"Pinned Backend Versions","text":"<p>Inference engines in the generative AI domain are evolving rapidly to enhance performance and unlock new capabilities. This constant evolution provides exciting opportunities but also presents challenges for maintaining model compatibility and deployment stability.</p> <p>GPUStack allows you to pin inference backend versions to specific releases, offering a balance between staying up-to-date with the latest advancements and ensuring a reliable runtime environment. This feature is particularly beneficial in the following scenarios:</p> <ul> <li>Leveraging the newest backend features without waiting for a GPUStack update.</li> <li>Locking in a specific backend version to maintain compatibility with existing models.</li> <li>Assigning different backend versions to models with varying requirements.</li> </ul> <p>By pinning backend versions, you gain full control over your inference environment, enabling both flexibility and predictability in deployment.</p>"},{"location":"user-guide/pinned-backend-versions/#automatic-installation-of-pinned-backend-versions","title":"Automatic Installation of Pinned Backend Versions","text":"<p>To simplify deployment, GPUStack supports the automatic installation of pinned backend versions when feasible. The process depends on the type of backend:</p> <ol> <li>Prebuilt Binaries    For backends like <code>llama-box</code>, GPUStack downloads the specified version using the same mechanism as in GPUStack bootstrapping.</li> </ol> <p>Tip</p> <p>You can customize the download source using the <code>--tools-download-base-url</code> configuration option.</p> <ol> <li>Python-based Backends    For backends like <code>vLLM</code> and <code>vox-box</code>, GPUStack uses <code>pipx</code> to install the specified version in an isolated Python environment.</li> </ol> <p>Tip</p> <ul> <li>Ensure that <code>pipx</code> is installed on the worker nodes.</li> <li>If <code>pipx</code> is not in the system PATH, specify its location with the <code>--pipx-path</code> configuration option.</li> </ul> <p>This automation reduces manual intervention, allowing you to focus on deploying and using your models.</p>"},{"location":"user-guide/pinned-backend-versions/#manual-installation-of-pinned-backend-versions","title":"Manual Installation of Pinned Backend Versions","text":"<p>When automatic installation is not feasible or preferred, GPUStack provides a straightforward way to manually install specific versions of inference backends. Follow these steps:</p> <ol> <li>Prepare the Executable    Install the backend executable or link it under the GPUStack bin directory. The default locations are:</li> </ol> <ul> <li>Linux/macOS: <code>/var/lib/gpustack/bin</code></li> <li>Windows: <code>$env:AppData\\gpustack\\bin</code></li> </ul> <p>Tip</p> <p>You can customize the bin directory using the <code>--bin-dir</code> configuration option.</p> <ol> <li>Name the Executable    Ensure the executable is named in the following format:</li> </ol> <ul> <li>Linux/macOS: <code>&lt;backend&gt;_&lt;version&gt;</code></li> <li>Windows: <code>&lt;backend&gt;_&lt;version&gt;.exe</code></li> </ul> <p>For example, the vLLM executable for version v0.7.3 should be named <code>vllm_v0.7.3</code> on Linux.</p> <p>By following these steps, you can maintain full control over the backend installation process, ensuring that the correct version is used for your deployment.</p>"},{"location":"user-guide/rerank-api/","title":"Rerank API","text":"<p>In the context of Retrieval-Augmented Generation (RAG), reranking refers to the process of selecting the most relevant information from retrieved documents or knowledge sources before presenting them to the user or utilizing them for answer generation.</p> <p>GPUStack serves Jina compatible Rerank API using the <code>/v1/rerank</code> path.</p>"},{"location":"user-guide/rerank-api/#supported-models","title":"Supported Models","text":"<p>The following models are available for reranking:</p> <ul> <li>bce-reranker-base_v1</li> <li>jina-reranker-v1-turbo-en</li> <li>jina-reranker-v1-tiny-en</li> <li>bge-reranker-v2-m3</li> <li>gte-multilingual-reranker-base \ud83e\uddea</li> <li>jina-reranker-v2-base-multilingual \ud83e\uddea</li> </ul>"},{"location":"user-guide/rerank-api/#usage","title":"Usage","text":"<p>The following is an example using the Rerank API:</p> <pre><code>export GPUSTACK_API_KEY=your_api_key\ncurl http://your_gpustack_server_url/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"model\": \"bge-reranker-v2-m3\",\n        \"query\": \"What is a panda?\",\n        \"top_n\": 3,\n        \"documents\": [\n            \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n        ]\n    }' | jq\n</code></pre> <p>Example output:</p> <pre><code>{\n  \"model\": \"bge-reranker-v2-m3\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"document\": {\n        \"text\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n      },\n      \"index\": 2,\n      \"relevance_score\": 1.951932668685913\n    },\n    {\n      \"document\": {\n        \"text\": \"it is a bear\"\n      },\n      \"index\": 1,\n      \"relevance_score\": -3.7347371578216553\n    },\n    {\n      \"document\": {\n        \"text\": \"hi\"\n      },\n      \"index\": 0,\n      \"relevance_score\": -6.157620906829834\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 69,\n    \"total_tokens\": 69\n  }\n}\n</code></pre>"},{"location":"user-guide/user-management/","title":"User Management","text":"<p>GPUStack supports users of two roles: <code>Admin</code> and <code>User</code>. Admins can monitor system status, manage models, users, and system settings. Users can manage their own API keys and use the completion API.</p>"},{"location":"user-guide/user-management/#default-admin","title":"Default Admin","text":"<p>On bootstrap, GPUStack creates a default admin user. The initial password for the default admin is stored in <code>&lt;data-dir&gt;/initial_admin_password</code>. In the default setup, it should be <code>/var/lib/gpustack/initial_admin_password</code>. You can customize the default admin password by setting the <code>--bootstrap-password</code> parameter when starting <code>gpustack</code>.</p>"},{"location":"user-guide/user-management/#create-user","title":"Create User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Click the <code>Create User</code> button.</li> <li>Fill in <code>Name</code>, <code>Full Name</code>, <code>Password</code>, and select <code>Role</code> for the user.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/user-management/#update-user","title":"Update User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to edit.</li> <li>Click the <code>Edit</code> button in the <code>Operations</code> column.</li> <li>Update the attributes as needed.</li> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"user-guide/user-management/#delete-user","title":"Delete User","text":"<ol> <li>Navigate to the <code>Users</code> page.</li> <li>Find the user you want to delete.</li> <li>Click the ellipsis button in the <code>Operations</code> column, then select <code>Delete</code>.</li> <li>Confirm the deletion.</li> </ol>"},{"location":"user-guide/playground/","title":"Playground","text":"<p>GPUStack offers a playground UI where users can test and experiment with the APIs. Refer to each subpage for detailed instructions and information.</p>"},{"location":"user-guide/playground/audio/","title":"Audio Playground","text":"<p>The Audio Playground is a dedicated space for testing and experimenting with GPUStack\u2019s Text-to-Speech (TTS) and Speech-to-Text (STT) APIs. It allows users to interactively convert text to audio and audio to text, customize parameters, and review code examples for seamless API integration.</p>"},{"location":"user-guide/playground/audio/#text-to-speech","title":"Text to Speech","text":"<p>Switch to the \"Text to Speech\" tab to test TTS models.</p>"},{"location":"user-guide/playground/audio/#text-input","title":"Text Input","text":"<p>Enter the text you want to convert, then click the <code>Submit</code> button to generate the corresponding speech.</p> <p></p>"},{"location":"user-guide/playground/audio/#clear-text","title":"Clear Text","text":"<p>Click the <code>Clear</code> button to reset the text input and remove the generated speech.</p>"},{"location":"user-guide/playground/audio/#select-model","title":"Select Model","text":"<p>Select an available TTS model in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/audio/#customize-parameters","title":"Customize Parameters","text":"<p>Customize the voice and format of the audio output.</p> <p>Tip</p> <p>Supported voices may vary between models.</p>"},{"location":"user-guide/playground/audio/#view-code","title":"View Code","text":"<p>After experimenting with input text and parameters, click the <code>View Code</code> button to see how to call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/audio/#speech-to-text","title":"Speech to Text","text":"<p>Switch to the \"Speech to Text\" tab to test STT models.</p>"},{"location":"user-guide/playground/audio/#provide-audio-file","title":"Provide Audio File","text":"<p>You can provide audio for transcription in two ways:</p> <ol> <li>Upload an audio file.</li> <li>Record audio online.</li> </ol> <p>Note</p> <p>If the online recording is not available, it could be due to one of the following reasons:</p> <ol> <li>For HTTPS or <code>http://localhost</code> access, microphone permissions must be enabled in your browser.</li> <li> <p>For access via <code>http://{host IP}</code>, the URL must be added to your browser's trusted list.</p> <p>Example:   In Chrome, navigate to <code>chrome://flags/</code>, add the GPUStack URL to \"Insecure origins treated as secure,\" and enable this option.</p> </li> </ol> <p></p> <p></p>"},{"location":"user-guide/playground/audio/#select-model_1","title":"Select Model","text":"<p>Select an available STT model in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/audio/#copy-text","title":"Copy Text","text":"<p>Copy the transcription results generated by the model.</p>"},{"location":"user-guide/playground/audio/#customize-parameters_1","title":"Customize Parameters","text":"<p>Select the appropriate language for your audio file to optimize transcription accuracy.</p>"},{"location":"user-guide/playground/audio/#view-code_1","title":"View Code","text":"<p>After experimenting with audio files and parameters, click the <code>View Code</code> button to see how to call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/chat/","title":"Chat Playground","text":"<p>Interact with the chat completions API. The following is an example screenshot:</p> <p></p>"},{"location":"user-guide/playground/chat/#prompts","title":"Prompts","text":"<p>You can adjust the prompt messages on the left side of the playground. There are three role types of prompt messages: system, user, and assistant.</p> <ul> <li>System: Typically a predefined instruction or guidance that sets the context, defines the behavior, or imposes specific constraints on how the model should generate its responses.</li> <li>User: The input or query provided by the user (the person interacting with the LLM).</li> <li>Assistant: The response generated by the LLM.</li> </ul>"},{"location":"user-guide/playground/chat/#edit-system-message","title":"Edit System Message","text":"<p>You can add and edit the system message at the top of the playground.</p>"},{"location":"user-guide/playground/chat/#edit-user-and-assistant-messages","title":"Edit User and Assistant Messages","text":"<p>To add a user or assistant message, click the <code>New Message</code> button.</p> <p>To remove a user or assistant message, click the minus button at the right corner of the message.</p> <p>To change the role of a message, click the <code>User</code> or <code>Assistant</code> text at the beginning of the message.</p>"},{"location":"user-guide/playground/chat/#upload-image","title":"Upload Image","text":"<p>You can add images to the prompt by clicking the <code>Upload Image</code> button.</p>"},{"location":"user-guide/playground/chat/#clear-prompts","title":"Clear Prompts","text":"<p>Click the <code>Clear</code> button to clear all the prompts.</p>"},{"location":"user-guide/playground/chat/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground. Please refer to Model Management to learn about how to manage models.</p>"},{"location":"user-guide/playground/chat/#customize-parameters","title":"Customize Parameters","text":"<p>You can customize completion parameters in the <code>Parameters</code> section.</p>"},{"location":"user-guide/playground/chat/#do-completion","title":"Do Completion","text":"<p>You can do a completion by clicking the <code>Submit</code> button.</p>"},{"location":"user-guide/playground/chat/#view-code","title":"View Code","text":"<p>Once you've done experimenting with the prompts and parameters, you can click the <code>View Code</code> button to check how you can call the API with the same input by code. Code examples in <code>curl</code>, <code>Python</code>, and <code>Node.js</code> are provided.</p>"},{"location":"user-guide/playground/chat/#compare-playground","title":"Compare Playground","text":"<p>You can compare multiple models in the playground. The following is an example screenshot:</p> <p></p>"},{"location":"user-guide/playground/chat/#comparision-mode","title":"Comparision Mode","text":"<p>You can choose the number of models to compare by clicking the comparison view buttons, including 2, 3, 4 and 6-model comparison.</p>"},{"location":"user-guide/playground/chat/#prompts_1","title":"Prompts","text":"<p>You can adjust the prompt messages similar to the chat playground.</p>"},{"location":"user-guide/playground/chat/#upload-image_1","title":"Upload Image","text":"<p>You can add images to the prompt by clicking the <code>Upload Image</code> button.</p>"},{"location":"user-guide/playground/chat/#clear-prompts_1","title":"Clear Prompts","text":"<p>Click the <code>Clear</code> button to clear all the prompts.</p>"},{"location":"user-guide/playground/chat/#select-model_1","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-left corner of each model panel.</p>"},{"location":"user-guide/playground/chat/#customize-parameters_1","title":"Customize Parameters","text":"<p>You can customize completion parameters by clicking the settings button of each model.</p>"},{"location":"user-guide/playground/embedding/","title":"Embedding Playground","text":"<p>The Embedding Playground lets you test the model\u2019s ability to convert text into embeddings. It allows you to experiment with multiple text inputs, visualize embeddings, and review code examples for API integration.</p>"},{"location":"user-guide/playground/embedding/#add-text","title":"Add Text","text":"<p>Add at least two text entries and click the <code>Submit</code> button to generate embeddings.</p>"},{"location":"user-guide/playground/embedding/#batch-input-text","title":"Batch Input Text","text":"<p>Enable <code>Batch Input Mode</code> to automatically split multi-line text into separate entries based on line breaks. This is useful for processing multiple text snippets in a single operation.</p>"},{"location":"user-guide/playground/embedding/#visualization","title":"Visualization","text":"<p>Visualize the embedding results using PCA (Principal Component Analysis) to reduce dimensions and display them on a 2D plot. Results can be viewed in two formats:</p> <ol> <li>Chart - Display PCA results visually.</li> <li>JSON - View raw embeddings in JSON format.</li> </ol> <p>In the chart, the distance between points represents the similarity between corresponding texts. Closer points indicate higher similarity.</p> <p></p>"},{"location":"user-guide/playground/embedding/#clear","title":"Clear","text":"<p>Click the <code>Clear</code> button to reset text entries and clear the output.</p>"},{"location":"user-guide/playground/embedding/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/embedding/#view-code","title":"View Code","text":"<p>After experimenting with the text inputs, click the <code>View Code</code> button to see how you can call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/image/","title":"Image Playground","text":"<p>The Image Playground is a dedicated space for testing and experimenting with GPUStack\u2019s image generation APIs. It allows users to interactively explore the capabilities of different models, customize parameters, and review code examples for seamless API integration.</p>"},{"location":"user-guide/playground/image/#generate-image","title":"Generate Image","text":""},{"location":"user-guide/playground/image/#prompt","title":"Prompt","text":"<p>You can input or randomly generate a prompt, then click the Submit button to generate an image.</p> <p></p>"},{"location":"user-guide/playground/image/#clear-prompt","title":"Clear Prompt","text":"<p>Click the <code>Clear</code> button to reset the prompt and remove the generated image.</p>"},{"location":"user-guide/playground/image/#preview","title":"Preview","text":"<p>Select the <code>Faster</code> option for the <code>Preview</code> parameter to see the image generation progress in real-time.</p> <p></p>"},{"location":"user-guide/playground/image/#edit-image","title":"Edit Image","text":"<p>Upload an image and highlight the areas you want to modify by painting over them. Then, enter a prompt and <code>submit</code>. If no areas are painted, the entire image will be modified.</p> <p></p>"},{"location":"user-guide/playground/image/#save-mask","title":"Save Mask","text":"<p>Click <code>Save Mask</code> to save the painted areas as a separate image.</p>"},{"location":"user-guide/playground/image/#download-image","title":"Download Image","text":"<p>Click <code>Download Image</code> to save the edited image.</p>"},{"location":"user-guide/playground/image/#preview_1","title":"Preview","text":"<p>You can enable <code>Preview</code> while editing images to see the changes in real-time.</p>"},{"location":"user-guide/playground/image/#select-model","title":"Select Model","text":"<p>You can select available models in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/image/#customize-parameters","title":"Customize Parameters","text":"<p>You can customize the image generation parameters by switching between two API styles:</p> <ol> <li>OpenAI-compatible mode.</li> <li>Advanced mode.</li> </ol> <p></p>"},{"location":"user-guide/playground/image/#advanced-parameters","title":"Advanced Parameters","text":"Parameter Default Description <code>Counts</code> <code>1</code> Number of images to generate. <code>Size</code> <code>512x512</code> The size of the generated image in 'widthxheight' format. <code>Sample Method</code> <code>euler_a</code> The sampler algorithm for image generation. Options include 'euler_a', 'euler', 'heun', 'dpm2', 'dpm++2s_a', 'dpm++2m', 'dpm++2mv2', 'ipndm', 'ipndm_v', and 'lcm'. <code>Schedule Method</code> <code>discrete</code> The noise scheduling method. <code>Sampling Steps</code> <code>10</code> The number of sampling steps to perform. Higher values may improve image quality at the cost of longer processing time. <code>Guidance</code> <code>3.5</code> The scale for classifier-free guidance. A higher value increases adherence to the prompt. <code>CFG Scale</code> <code>4.5</code> The scale for classifier-free guidance. A higher value increases adherence to the prompt. <code>Negative Prompt</code> (empty) A negative prompt to specify what the image should avoid. <code>Preview</code> <code>Faster</code> Controls how the image generation process is displayed. Options include 'Faster', 'Normal', 'None'. <code>Seed</code> (empty) Random seed. <p>Note</p> <p>The maximum image size is restricted by the model's deployment settings. See the diagram below:</p> <p></p>"},{"location":"user-guide/playground/image/#view-code","title":"View Code","text":"<p>After experimenting with prompts and parameters, click the <code>View Code</code> button to see how to call the API with the same inputs. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"user-guide/playground/rerank/","title":"Rerank Playground","text":"<p>The Rerank Playground allows you to test reranker models that reorder multiple texts based on their relevance to a query. Experiment with various input texts, customize parameters, and review code examples for API integration.</p>"},{"location":"user-guide/playground/rerank/#add-text","title":"Add Text","text":"<p>Add multiple text entries to the document for reranking.</p>"},{"location":"user-guide/playground/rerank/#bach-input-text","title":"Bach Input Text","text":"<p>Enable <code>Batch Input Mode</code> to split multi-line text into separate entries based on line breaks. This is useful for processing multiple text snippets efficiently.</p>"},{"location":"user-guide/playground/rerank/#clear","title":"Clear","text":"<p>Click the <code>Clear</code> button to reset the document and query results.</p>"},{"location":"user-guide/playground/rerank/#query","title":"Query","text":"<p>Input a query and click the <code>Submit</code> button to get a ranked list of texts based on their relevance to the query.</p> <p></p>"},{"location":"user-guide/playground/rerank/#select-model","title":"Select Model","text":"<p>Select an available reranker model in GPUStack by clicking the model dropdown at the top-right corner of the playground UI.</p>"},{"location":"user-guide/playground/rerank/#customize-parameters","title":"Customize Parameters","text":"<p>In the parameter section, set <code>Top N</code> to specify the number of matching texts to retrieve.</p>"},{"location":"user-guide/playground/rerank/#view-code","title":"View Code","text":"<p>After experimenting with the input text and query, click the <code>View Code</code> button to see how to call the API with the same input. Code examples are provided in <code>curl</code>, <code>Python</code>, and <code>Node.js</code>.</p>"},{"location":"using-models/editing-images/","title":"Editing Images","text":"<p>You can use image models to edit images by selecting an area of the image to edit and describing the desired changes. The model will generate the edited image based on your description.</p> <p>This guide demonstrates how to edit images in GPUStack.</p>"},{"location":"using-models/editing-images/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU with at least 24 GB of VRAM.</li> <li>Access to Hugging Face for downloading the model files.</li> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> </ul> <p>In this guide, we will use the <code>FLUX.1-Fill-dev</code> model in <code>Q8_0</code> quantization to edit images.</p>"},{"location":"using-models/editing-images/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Catalog</code> page in the GPUStack UI.</li> <li>Search and select the <code>FLUX.1 Fill Dev</code> model.</li> <li>In the popup, select the <code>Q8_0</code> quantization.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p>"},{"location":"using-models/editing-images/#step-2-use-the-model-to-edit-images","title":"Step 2: Use the Model to Edit Images","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Image</code> page in the GPUStack UI.</li> <li>Click the <code>Edit</code> tab on the top</li> <li>Verify that the deployed model is selected from the top-right <code>Model</code> dropdown.</li> <li>Upload the example image by clicking the center upload area.</li> <li>Draw a mask over the hair area of the example image.</li> <li>Input the following text prompt in the <code>Text Prompt</code> field:    <pre><code>Pink short hair bang, natural\n</code></pre></li> <li>Click the <code>Submit</code> button to generate the edited image.</li> </ol> <p>The generated image will be displayed in the UI. Your image may look different given the seed and randomness involved in the generation process.</p> <p></p> <p>If you want to reproduce the result shown above, you can use the following parameters:</p> <pre><code>Size: 768x1024(3:4)\nSample Method: euler\nSchedule Method: discrete\nSampling Steps: 50\nGuidance: 30.0\nCFG Scale: 1.0\nStrength: 1.0\nSeed: 656821733471329\nText Prompt: Pink short hair bang, natural\n</code></pre>"},{"location":"using-models/editing-images/#step-3-using-the-edit-images-api","title":"Step 3: Using the Edit Images API","text":"<p>Click the <code>View Code</code> button to see example code snippets for using the edit images API programmatically.</p> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/","title":"Recommended Parameters for Image Generation Models","text":"<p>GPUStack currently does not support GGUF image generation models that are not all-in-one (base model, text encoder, and VAE combined). Please refer to the supported model list for details:</p> <ul> <li> <p>Hugging Face Collection</p> </li> <li> <p>ModelScope Collection</p> </li> </ul> <p>The core parameters of image generation models are key to achieving desired outputs. These include <code>Prompt</code>, <code>Seed</code>, <code>Resolution</code>, <code>Sampler</code>, <code>Scheduler</code>, <code>Sampling Steps</code> and <code>CFG scale</code>. Different models may have variations in parameter settings. In order to quickly get started and generate satisfying images, the following section is going to provide some reference values for parameter configurations.</p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#flux1-dev","title":"FLUX.1-dev","text":"<p>For FLUX models, it is recommended to disable CFG (CFG=1) for better results.</p> <p>Reference settings:</p> Parameter Value Size 1024x1024 Sampler euler Scheduler discrete Steps 20 CFG 1.0 <p>Recommended samplers: euler, heun, ipndm, ipndm_v</p> <p>Recommended scheduler: discrete</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: A kangaroo holding a beer,wearing ski goggles and passionately singing silly songs.\nSize: 1024x1024\nSampler: euler\nScheduler: discrete\nSteps: 20\nCFG: 1.0\nSeed: 838887451\n</code></pre> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#using-lora","title":"Using LoRA","text":"<p>Configuration: Edit model -&gt; Advanced -&gt; Backend Parameters -&gt; Add <code>--lora=&lt;path/to/your/lora_file&gt;</code></p> <p></p> <p>The top row shows the original images, while the bottom row displays the corresponding images generated using LoRA.</p> <p></p> <p>Note</p> <p>LoRA is currently an experimental feature. Not all models or LoRA files are compatible.</p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#flux1-schnell","title":"FLUX.1-schnell","text":"<p>For FLUX models, it is recommended to disable CFG (CFG=1) for better results.</p> <p>Reference settings:</p> Parameter Value Size 1024x1024 Sampler euler Scheduler discrete Steps 2-4 CFG 1.0 <p>Recommended samplers: euler, dpm++2mv2, ipndm_v</p> <p>Recommended scheduler: discrete</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: A mischievous ferret with a playful grin squeezes itself into a large glass jar, surrounded by colorful candy. The jar sits on a wooden table in a cozy kitchen, and warm sunlight filters through a nearby window\nSize: 1024x1024\nSampler: euler\nScheduler: discrete\nSteps: 3\nCFG: 1.0\nSeed: 1565801500\n</code></pre> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#stable-diffusion-v3-5-large","title":"Stable-Diffusion-v3-5-Large","text":"<p>Reference settings:</p> Parameter Value Size 1024x1024 Sampler euler Scheduler discrete Steps 25 CFG 4.5 <p>Recommended samplers: dpm++2m, ipndm, ipndm_v, dpm++2mv2, eluer, heun, dpm2</p> <p>Recommended scheduler: discrete</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: Lucky flower pop art style with pink color scheme,happy cute girl character wearing oversized headphones and smiling while listening to music in the air with her eyes closed,vibrant colorful Japanese anime cartoon illustration with bold outlines and bright colors,colorful text \"GPUStack\" on top of background,high resolution,detailed,\nSize: 1024x1024\nSampler: dpm++2m\nScheduler: discrete\nSteps: 25\nCFG: 5\nSeed: 3520225659\n</code></pre> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#stable-diffusion-v3-5-large-turbo","title":"Stable-Diffusion-v3-5-Large-Turbo","text":"<p>For turbo models, it is recommended to disable CFG (CFG=1) for better results.</p> <p>Reference settings:</p> Parameter Value Size 1024x1024 Sampler euler/dpm++2m Scheduler discrete/exponential Steps 5/15-20 CFG 1.0 <p>Recommended samplers: euler, ipndm, ipndm_v, dpm++2mv2, heun, dpm2, dpm++2m</p> <p>Recommended scheduler: discrete, karras, exponential</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: This dreamlike digital art captures a vibrant, kaleidoscopic bird in a lush rainforest\nSize: 768x1024\nSampler: heun\nScheduler: karras\nSteps: 15\nCFG: 1.0\nSeed: 2536656539\n</code></pre> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#stable-diffusion-v3-5-medium","title":"Stable-Diffusion-v3-5-Medium","text":"<p>Reference settings:</p> Parameter Value Size 768x1024 Sampler euler Scheduler discrete Steps 28 CFG 4.5 <p>Recommended samplers: euler, ipndm, ipndm_v, dpm++2mv2, heun, dpm2, dpm++2m</p> <p>Recommended scheduler: discrete</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: Plush toy, a box of French fries, pink bag, long French fries, smiling expression, round eyes, smiling mouth, bright colors, simple composition, clean background, jellycat style,\nNegative Prompt: ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2)\nSize: 768x1024\nSampler: euler\nScheduler: discrete\nSteps: 28\nCFG: 4.5\nSeed: 3353126565\n</code></pre> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#stable-diffusion-v3-medium","title":"Stable-Diffusion-v3-Medium","text":"<p>Reference settings:</p> Parameter Value Size 1024x1024 Sampler euler Scheduler discrete Steps 25 CFG 4.0 <p>Recommended samplers: euler, ipndm, ipndm_v, dpm++2mv2, heun, dpm2, dpm++2m</p> <p>Recommended scheduler: discrete</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: A guitar crafted from a watermelon, realistic, close-up, ultra-HD, digital art, with smoke and ice cubes, soft lighting, dramatic stage effects of light and shadow, pastel aesthetic filter, time-lapse photography, macro photography, ultra-high resolution, perfect design composition, surrealism, hyper-imaginative, ultra-realistic, ultra-HD quality\nSize: 768x1280\nSampler: euler\nScheduler: discrete\nSteps: 30\nCFG: 5.0\nSeed: 1937760054\n</code></pre> <p>Tip</p> <p>The default maximum image height is 1024. To increase it, edit the model and add the backend parameter --image-max-height=1280 in the advanced settings.</p> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#sdxl-base-v10","title":"SDXL-base-v1.0","text":"<p>Reference settings:</p> Parameter Value Size 1024x1024 Sampler dpm++2m Scheduler karras Steps 25 CFG 5.0 <p>Recommended samplers: euler, ipndm, ipndm_v, dpm++2mv2, heun, dpm2, dpm++2m</p> <p>Recommended scheduler: discrete, karras, exponential</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: Weeds blowing in the wind,By the seaside,Ultra-realistic,Majestic epic scenery,excessively splendid ancient rituals,vibrant,beautiful Eastern fantasy,bright sunshine,pink peach blossoms,daytime perspective.\nNegative Prompt: ng_deepnegative_v1_75t,(badhandv4:1.2),EasyNegative,(worst quality:2),\nSize: 768x1280\nSampler: dpm++2m\nScheduler: exponential\nSteps: 30\nCFG: 5.0\nSeed: 3754742591\n</code></pre> <p></p>"},{"location":"using-models/recommended-parameters-for-image-generation-models/#stable-diffusion-v2-1-turbo","title":"Stable-Diffusion-v2-1-Turbo","text":"<p>For turbo models, it is recommended to disable CFG (CFG=1) for better results.</p> <p>Reference settings:</p> Parameter Value Size 512x512 Sampler euler_a Scheduler discrete Steps 6 CFG 1.0 <p>Recommended samplers: eluer_a, dmp++2s, lcm</p> <p>Recommended scheduler: discrete, karras, exponential, ays, gits</p> <p>\u270f\ufe0fTry it out!</p> <pre><code>Prompt: A burger patty, with the bottom bun and lettuce and tomatoes.\nSize: 512x512\nSampler: euler_a\nScheduler: discrete\nSteps: 6\nCFG: 1.0\nSeed: 1375548153\n</code></pre> <p></p> <p>Note</p> <p>The parameters above are for reference only. The ideal settings may vary depending on the specific situation and should be adjusted accordingly.</p>"},{"location":"using-models/using-audio-models/","title":"Using Audio Models","text":"<p>GPUStack supports running both Speech-to-Text and Text-to-Speech models. Speech-to-Text models convert audio inputs in various languages into written text, while Text-to-Speech models transform written text into natural and expressive speech.</p> <p>In this guide, we will walk you through deploying and using Speech-to-Text and Text-to-Speech models in GPUStack.</p>"},{"location":"using-models/using-audio-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A Linux system with AMD64 architecture or macOS.</li> <li>Access to Hugging Face for downloading the model files.</li> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> </ul>"},{"location":"using-models/using-audio-models/#running-speech-to-text-model","title":"Running Speech-to-Text Model","text":""},{"location":"using-models/using-audio-models/#step-1-deploy-speech-to-text-model","title":"Step 1: Deploy Speech-to-Text Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar in the top left to search for the model name <code>Systran/faster-whisper-medium</code>.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p> <p></p>"},{"location":"using-models/using-audio-models/#step-2-interact-with-speech-to-text-model","title":"Step 2: Interact with Speech-to-Text Model","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Audio</code> page in the GPUStack UI.</li> <li>Select the <code>Speech to Text</code> Tab.</li> <li>Select the deployed model from the top-right dropdown.</li> <li>Click the <code>Upload</code> button to upload audio file or click the <code>Microphone</code> button to record audio.</li> <li>Click the <code>Generate Text Content</code> button to generate the text.</li> </ol>"},{"location":"using-models/using-audio-models/#running-text-to-speech-model","title":"Running Text-to-Speech Model","text":""},{"location":"using-models/using-audio-models/#step-1-deploy-text-to-speech-model","title":"Step 1: Deploy Text-to-Speech Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar in the top left to search for the model name <code>FunAudioLLM/CosyVoice-300M</code>.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p> <p></p>"},{"location":"using-models/using-audio-models/#step-2-interact-with-text-to-speech-model","title":"Step 2: Interact with Text to Speech Model","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Audio</code> page in the GPUStack UI.</li> <li>Select the <code>Text to Speech</code> Tab.</li> <li>Choose the deployed model from the dropdown menu in the top-right corner. Then, configure the voice and output audio format.</li> <li>Input the text to generate.</li> <li>Click the <code>Submit</code> button to generate the text.</li> </ol>"},{"location":"using-models/using-embedding-models/","title":"Using Embedding Models","text":"<p>Text embeddings are numerical representations of text that capture semantic meaning, enabling machines to understand relationships and similarities between different pieces of text. In essence, they transform text into vectors in a continuous space, where texts with similar meanings are positioned closer together. Text embeddings are widely used in applications such as natural language processing, information retrieval, and recommendation systems.</p> <p>In this guide, we will demonstrate how to deploy embedding models in GPUStack and generate text embeddings using the deployed models.</p>"},{"location":"using-models/using-embedding-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul>"},{"location":"using-models/using-embedding-models/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Enable the <code>GGUF</code> checkbox to filter models by GGUF format.</li> <li>Use the search bar in the top left to search for the model name <code>CompendiumLabs/bge-small-en-v1.5-gguf</code>.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p> <p></p>"},{"location":"using-models/using-embedding-models/#step-2-generate-an-api-key","title":"Step 2: Generate an API Key","text":"<p>We will use the GPUStack API to generate text embeddings, and an API key is required:</p> <ol> <li>Navigate to the <code>API Keys</code> page in the GPUStack UI.</li> <li>Click the <code>New API Key</code> button.</li> <li>Enter a name for the API key and click the <code>Save</code> button.</li> <li>Copy the generated API key. You can only view the API key once, so make sure to save it securely.</li> </ol>"},{"location":"using-models/using-embedding-models/#step-3-generate-text-embeddings","title":"Step 3: Generate Text Embeddings","text":"<p>With the model deployed and an API key, you can generate text embeddings via the GPUStack API. Here is an example script using <code>curl</code>:</p> <pre><code>export SERVER_URL=&lt;your-server-url&gt;\nexport GPUSTACK_API_KEY=&lt;your-api-key&gt;\ncurl $SERVER_URL/v1-openai/embeddings \\\n  -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"input\": \"The food was delicious and the waiter...\",\n    \"model\": \"bge-small-en-v1.5\",\n    \"encoding_format\": \"float\"\n  }'\n</code></pre> <p>Replace <code>&lt;your-server-url&gt;</code> with the URL of your GPUStack server and <code>&lt;your-api-key&gt;</code> with the API key you generated in the previous step.</p> <p>Example response:</p> <pre><code>{\n  \"data\": [\n    {\n      \"embedding\": [\n        -0.012189436703920364, 0.016934078186750412, 0.003965042531490326,\n        -0.03453584015369415, -0.07623119652271271, -0.007116147316992283,\n        0.11278388649225235, 0.019714849069714546, 0.010370955802500248,\n        -0.04219457507133484, -0.029902394860982895, 0.01122555136680603,\n        0.022912170737981796, 0.031186765059828758, 0.006303929258137941,\n        # ... additional values\n      ],\n      \"index\": 0,\n      \"object\": \"embedding\"\n    }\n  ],\n  \"model\": \"bge-small-en-v1.5\",\n  \"object\": \"list\",\n  \"usage\": { \"prompt_tokens\": 12, \"total_tokens\": 12 }\n}\n</code></pre>"},{"location":"using-models/using-image-generation-models/","title":"Using Image Generation Models","text":"<p>GPUStack supports deploying and running state-of-the-art Image Generation Models. These models allow you to generate stunning images from textual descriptions, enabling applications in design, content creation, and more.</p> <p>In this guide, we will walk you through deploying and using image generation models in GPUStack.</p>"},{"location":"using-models/using-image-generation-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU that has at least 12 GB of VRAM.</li> <li>Access to Hugging Face for downloading the model files.</li> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> </ul>"},{"location":"using-models/using-image-generation-models/#step-1-deploy-the-stable-diffusion-model","title":"Step 1: Deploy the Stable Diffusion Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Use the search bar in the top left to search for the model name <code>gpustack/stable-diffusion-v3-5-medium-GGUF</code>.</li> <li>In the <code>Available Files</code> section, select the <code>stable-diffusion-v3-5-medium-Q4_0.gguf</code> file.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p> <p></p>"},{"location":"using-models/using-image-generation-models/#step-2-use-the-model-for-image-generation","title":"Step 2: Use the Model for Image Generation","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Image</code> page in the GPUStack UI.</li> <li>Verify that the deployed model is selected from the top-right <code>Model</code> dropdown.</li> <li>Enter a prompt describing the image you want to generate. For example:</li> </ol> <pre><code>a female character with long, flowing hair that appears to be made of ethereal, swirling patterns resembling the Northern Lights or Aurora Borealis. The background is dominated by deep blues and purples, creating a mysterious and dramatic atmosphere. The character's face is serene, with pale skin and striking features. She wears a dark-colored outfit with subtle patterns. The overall style of the artwork is reminiscent of fantasy or supernatural genres.\n</code></pre> <ol> <li>Select <code>euler</code> in the <code>Sampler</code> dropdown.</li> <li>Set the <code>Sample Steps</code> to <code>20</code>.</li> <li>Click the <code>Submit</code> button to create the image.</li> </ol> <p>The generated image will be displayed in the UI. Your image may look different given the seed and randomness involved in the generation process.</p> <p></p>"},{"location":"using-models/using-image-generation-models/#conclusion","title":"Conclusion","text":"<p>With this setup, you can generate unique and visually compelling images from textual prompts. Experiment with different prompts and settings to push the boundaries of what\u2019s possible. For more details, refer to Recommended Parameters for Image Generation Models</p>"},{"location":"using-models/using-large-language-models/","title":"Using Large Language Models","text":"<p>Large Language Models (LLMs) are powerful AI models capable of understanding and generating human-like text, making them essential for applications such as chatbots, content generation, code completion, and more.</p> <p>In this guide, you will learn how to deploy and interact with LLMs in GPUStack.</p>"},{"location":"using-models/using-large-language-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A Linux machine with one or more GPUs that has at least 30 GB of VRAM in total. We will use the vLLM backend which only supports Linux.</li> <li>Access to Hugging Face or ModelScope for downloading the model files.</li> <li>GPUStack installed and running. If not, refer to the Quickstart Guide.</li> </ul>"},{"location":"using-models/using-large-language-models/#step-1-deploy-large-language-models","title":"Step 1: Deploy Large Language Models","text":""},{"location":"using-models/using-large-language-models/#deoloy-from-catalog","title":"Deoloy from Catalog","text":"<p>Large language models in the catalog are marked with the <code>LLM</code> category. When you select a large language model from the catalog, the default configurations should work as long as you have enough GPU resources and the backend is compatible with your setup (e.g., vLLM backend requires an amd64 Linux worker).</p> <p>Here, we take the deployment of <code>DeepSeek R1</code> as an example.</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Catalog</code> as the source for your model.</li> <li>In the catalog list page, use the search bar in the top left to search for the model keyword <code>DeepSeek</code>.</li> <li>Review the model description, maximum context length and supported sizes.</li> </ol> <p></p>"},{"location":"using-models/using-large-language-models/#deployment-using-llama-box","title":"Deployment Using llama-box","text":"<ol> <li>Select the <code>Deepseek R1</code> from the catalog.</li> <li>Select <code>7B</code> in Size.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p>After deployment, you can monitor the model's status on the <code>Models</code> page and wait for it to start running.</p>"},{"location":"using-models/using-large-language-models/#deployment-using-vllm","title":"Deployment Using vLLM","text":"<ol> <li>Select the <code>Deepseek R1</code> from the catalog.</li> <li>Since the model name is the access ID and cannot be the same as a previously created one, change the default model name to <code>deepseek-r1-vllm</code>.</li> <li>Select the <code>vLLM</code> backend.</li> <li>Select <code>7B</code> in Size.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p>After deployment, you can monitor the model's status on the <code>Models</code> page and wait for it to start running.</p>"},{"location":"using-models/using-large-language-models/#step-2-use-the-llms-for-text-generation","title":"Step 2: Use the LLMs for Text Generation","text":"<ol> <li>Navigate to the <code>Playground</code> &gt; <code>Chat</code> page in the GPUStack UI.</li> <li>Verify that the deployed model is selected from the top-right <code>Model</code> dropdown.</li> <li>Provide a prompt for the text generation. For example:</li> </ol> <pre><code>2, 4, 6, 8, &gt; What is the next number?\n</code></pre> <ol> <li>Adjust the <code>Parameters</code> on the right based on your needs.</li> <li>Click the <code>Submit</code> button to generate the text.</li> </ol> <p>The generated chain of thought and result will be displayed in the UI.</p> <p></p> <p>By following these steps, you can leverage LLMs for AI-powered text generation and natural language tasks in GPUStack. Experiment with different prompts and settings to explore the full capabilities of LLMs!</p>"},{"location":"using-models/using-reranker-models/","title":"Using Reranker Models","text":"<p>Reranker Models are specialized models designed to improve the ranking of a list of items based on relevance to a given query. They are commonly used in information retrieval and search systems to refine initial search results, prioritizing items that are more likely to meet the user\u2019s intent. Reranker models take the initial document list and reorder items to enhance precision in applications such as search engines, recommendation systems, and question-answering tasks.</p> <p>In this guide, we will demonstrate how to deploy and use reranker models in GPUStack.</p>"},{"location":"using-models/using-reranker-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>GPUStack is installed and running. If not, refer to the Quickstart Guide.</li> <li>Access to Hugging Face for downloading the model files.</li> </ul>"},{"location":"using-models/using-reranker-models/#step-1-deploy-the-model","title":"Step 1: Deploy the Model","text":"<p>Follow these steps to deploy the model from Hugging Face:</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click the <code>Deploy Model</code> button.</li> <li>In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Enable the <code>GGUF</code> checkbox to filter models by GGUF format.</li> <li>Use the search bar in the top left to search for the model name <code>gpustack/bge-reranker-v2-m3-GGUF</code>.</li> <li>Leave everything as default and click the <code>Save</code> button to deploy the model.</li> </ol> <p></p> <p>After deployment, you can monitor the model's status on the <code>Models</code> page.</p> <p></p>"},{"location":"using-models/using-reranker-models/#step-2-generate-an-api-key","title":"Step 2: Generate an API Key","text":"<p>We will use the GPUStack API to interact with the model. To do this, you need to generate an API key:</p> <ol> <li>Navigate to the <code>API Keys</code> page in the GPUStack UI.</li> <li>Click the <code>New API Key</code> button.</li> <li>Enter a name for the API key and click the <code>Save</code> button.</li> <li>Copy the generated API key. You can only view the API key once, so make sure to save it securely.</li> </ol>"},{"location":"using-models/using-reranker-models/#step-3-reranking","title":"Step 3: Reranking","text":"<p>With the model deployed and an API key, you can rerank a list of documents via the GPUStack API. Here is an example script using <code>curl</code>:</p> <pre><code>export SERVER_URL=&lt;your-server-url&gt;\nexport GPUSTACK_API_KEY=&lt;your-api-key&gt;\ncurl $SERVER_URL/v1/rerank \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $GPUSTACK_API_KEY\" \\\n    -d '{\n        \"model\": \"bge-reranker-v2-m3\",\n        \"query\": \"What is a panda?\",\n        \"top_n\": 3,\n        \"documents\": [\n            \"hi\",\n            \"it is a bear\",\n            \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n        ]\n    }' | jq\n</code></pre> <p>Replace <code>&lt;your-server-url&gt;</code> with the URL of your GPUStack server and <code>&lt;your-api-key&gt;</code> with the API key you generated in the previous step.</p> <p>Example response:</p> <pre><code>{\n  \"model\": \"bge-reranker-v2-m3\",\n  \"object\": \"list\",\n  \"results\": [\n    {\n      \"document\": {\n        \"text\": \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.\"\n      },\n      \"index\": 2,\n      \"relevance_score\": 1.951932668685913\n    },\n    {\n      \"document\": {\n        \"text\": \"it is a bear\"\n      },\n      \"index\": 1,\n      \"relevance_score\": -3.7347371578216553\n    },\n    {\n      \"document\": {\n        \"text\": \"hi\"\n      },\n      \"index\": 0,\n      \"relevance_score\": -6.157620906829834\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 69,\n    \"total_tokens\": 69\n  }\n}\n</code></pre>"},{"location":"using-models/using-vision-language-models/","title":"Using Vision Language Models","text":"<p>Vision Language Models can process both visual (image) and language (text) data simultaneously, making them versatile tools for various applications, such as image captioning, visual question answering, and more. In this guide, you will learn how to deploy and interact with Vision Language Models (VLMs) in GPUStack.</p> <p>The procedure for deploying and interacting with these models in GPUStack is similar. The main difference is the parameters you need to set when deploying the models. For more information on the parameters you can set, please refer to Backend Parameters .</p> <p>In this guide, we will cover the deployment of the following models:</p> <ul> <li>Llama3.2-Vision</li> <li>Qwen2-VL</li> <li>Pixtral</li> <li>Phi3.5-Vision</li> </ul>"},{"location":"using-models/using-vision-language-models/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A Linux machine with one or more GPUs that has at least 30 GB of VRAM in total. We will use the vLLM backend which only supports Linux.</li> <li>Access to Hugging Face and a Hugging Face API key for downloading the model files.</li> <li>You have been granted access to the above models on Hugging Face. Llama3.2-VL and Pixtral are gated models, and you need to request access to them.</li> </ul> <p>Note</p> <p>An Ubuntu node equipped with one H100 (80GB) GPU is used throughout this guide.</p>"},{"location":"using-models/using-vision-language-models/#step-1-install-gpustack","title":"Step 1: Install GPUStack","text":"<p>Run the following command to install GPUStack:</p> <pre><code>curl -sfL https://get.gpustack.ai | sh -s - --huggingface-token &lt;Hugging Face API Key&gt;\n</code></pre> <p>Replace <code>&lt;Hugging Face API Key&gt;</code> with your Hugging Face API key. GPUStack will use this key to download the model files.</p>"},{"location":"using-models/using-vision-language-models/#step-2-log-in-to-gpustack-ui","title":"Step 2: Log in to GPUStack UI","text":"<p>Run the following command to get the default password:</p> <pre><code>cat /var/lib/gpustack/initial_admin_password\n</code></pre> <p>Open your browser and navigate to <code>http://&lt;your-server-ip&gt;</code>. Replace <code>&lt;your-server-ip&gt;</code> with the IP address of your server. Log in using the username <code>admin</code> and the password you obtained in the previous step.</p>"},{"location":"using-models/using-vision-language-models/#step-3-deploy-vision-language-models","title":"Step 3: Deploy Vision Language Models","text":""},{"location":"using-models/using-vision-language-models/#deoloy-from-catalog","title":"Deoloy from Catalog","text":"<p>Vision language models in the catalog are marked with the <code>vision</code> capability. When you select a vision language model from the catalog, the default configurations should work as long as you have enough GPU resources and the backend is compatible with your setup(e.g., vLLM backend requires an amd64 Linux worker).</p> <p></p>"},{"location":"using-models/using-vision-language-models/#example-of-custom-deployment-using-llama-box","title":"Example of Custom Deployment Using llama-box","text":"<p>When deploying GGUF VLM models with llama-box, GPUStack automatically handles the multi-modal projector file and it should work out of the box.</p> <ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI and click the <code>Deploy Model</code> button. In the dropdown, select <code>Hugging Face</code> as the source for your model.</li> <li>Enable the <code>GGUF</code> checkbox to filter models by GGUF format.</li> <li>Use the search bar to find the <code>bartowski/Qwen2-VL-2B-Instruct-GGUF</code> model.</li> <li>Use the GGUF <code>Q4_K_M</code> quantization format.</li> <li>Click the <code>Save</code> button to deploy the model.</li> </ol> <p></p>"},{"location":"using-models/using-vision-language-models/#example-of-custom-deployment-using-vllm","title":"Example of Custom Deployment Using vLLM","text":""},{"location":"using-models/using-vision-language-models/#deploy-llama32-vision","title":"Deploy Llama3.2-Vision","text":"<ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>meta-llama/Llama-3.2-11B-Vision-Instruct</code> in the search bar.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button multiple times and add the following parameters:</li> </ol> <ul> <li><code>--enforce-eager</code></li> <li><code>--max-num-seqs=16</code></li> <li><code>--max-model-len=8192</code></li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"using-models/using-vision-language-models/#deploy-qwen2-vl","title":"Deploy Qwen2-VL","text":"<ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>Qwen/Qwen2-VL-7B-Instruct</code> in the search bar.</li> <li>Click the <code>Save</code> button. The default configurations should work as long as you have enough GPU resources.</li> </ol>"},{"location":"using-models/using-vision-language-models/#deploy-pixtral","title":"Deploy Pixtral","text":"<ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>mistralai/Pixtral-12B-2409</code> in the search bar.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button multiple times and add the following parameters:</li> </ol> <ul> <li><code>--tokenizer-mode=mistral</code></li> <li><code>--limit-mm-per-prompt=image=4</code></li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"using-models/using-vision-language-models/#deploy-phi35-vision","title":"Deploy Phi3.5-Vision","text":"<ol> <li>Navigate to the <code>Models</code> page in the GPUStack UI.</li> <li>Click on the <code>Deploy Model</code> button, then select <code>Hugging Face</code> in the dropdown.</li> <li>Search for <code>microsoft/Phi-3.5-vision-instruct</code> in the search bar.</li> <li>Expand the <code>Advanced</code> section in configurations and scroll down to the <code>Backend Parameters</code> section.</li> <li>Click on the <code>Add Parameter</code> button and add the following parameter:</li> </ol> <ul> <li><code>--trust-remote-code</code></li> </ul> <ol> <li>Click the <code>Save</code> button.</li> </ol>"},{"location":"using-models/using-vision-language-models/#step-4-interact-with-vision-language-models","title":"Step 4: Interact with Vision Language Models","text":"<ol> <li>Navigate to the <code>Playground</code> page in the GPUStack UI.</li> <li>Select the deployed model from the top-right dropdown.</li> <li>Click on the <code>Upload Image</code> button above the input text area and upload an image.</li> <li>Enter a prompt in the input text area. For example, \"Describe the image.\"</li> <li>Click the <code>Submit</code> button to generate the output.</li> </ol>"}]}