
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://docs.gpustack.ai/0.6/faq/">
      
      
        <link rel="prev" href="../troubleshooting/">
      
      
        <link rel="next" href="../api-reference/">
      
      
      <link rel="icon" href="../assets/logo.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.30">
    
    
      
        <title>FAQ - GPUStack</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="orange">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#faq" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
          <aside class="md-banner md-banner--warning">
            <div class="md-banner__inner md-grid md-typeset">
              
  You're not viewing the latest version.
  <a href="../..">
    <strong>Click here to go to latest.</strong>
  </a>

            </div>
            <script>var el=document.querySelector("[data-md-component=outdated]"),outdated=__md_get("__outdated",sessionStorage);!0===outdated&&el&&(el.hidden=!1)</script>
          </aside>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="GPUStack" class="md-header__button md-logo" aria-label="GPUStack" data-md-component="logo">
      
  <img src="../assets/logo-white.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            GPUStack
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              FAQ
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="orange"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 0-7 7c0 2.38 1.19 4.47 3 5.74V17a1 1 0 0 0 1 1h6a1 1 0 0 0 1-1v-2.26c1.81-1.27 3-3.36 3-5.74a7 7 0 0 0-7-7M9 21a1 1 0 0 0 1 1h4a1 1 0 0 0 1-1v-1H9v1Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="light-blue" data-md-color-accent="orange"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2a7 7 0 0 1 7 7c0 2.38-1.19 4.47-3 5.74V17a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1v-2.26C6.19 13.47 5 11.38 5 9a7 7 0 0 1 7-7M9 21v-1h6v1a1 1 0 0 1-1 1h-4a1 1 0 0 1-1-1m3-17a5 5 0 0 0-5 5c0 2.05 1.23 3.81 3 4.58V16h4v-2.42c1.77-.77 3-2.53 3-4.58a5 5 0 0 0-5-5Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/gpustack/gpustack" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="GPUStack" class="md-nav__button md-logo" aria-label="GPUStack" data-md-component="logo">
      
  <img src="../assets/logo-white.png" alt="logo">

    </a>
    GPUStack
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/gpustack/gpustack" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../quickstart/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quickstart
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Installation
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Installation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/installation-requirements/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation Requirements
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    NVIDIA CUDA
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            NVIDIA CUDA
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/nvidia-cuda/online-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/nvidia-cuda/air-gapped-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Air-Gapped Installation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    AMD ROCm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            AMD ROCm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/amd-rocm/online-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/amd-rocm/air-gapped-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Air-Gapped Installation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/apple-metal-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Apple Metal
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Ascend CANN
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Ascend CANN
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/ascend-cann/online-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/ascend-cann/air-gapped-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Air-Gapped Installation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" >
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Hygon DTK
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Hygon DTK
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/hygon-dtk/online-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/hygon-dtk/air-gapped-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Air-Gapped Installation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_7" >
        
          
          <label class="md-nav__link" for="__nav_3_7" id="__nav_3_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Moore Threads MUSA
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_7">
            <span class="md-nav__icon md-icon"></span>
            Moore Threads MUSA
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/moorethreads-musa/online-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/moorethreads-musa/air-gapped-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Air-Gapped Installation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CPU
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            CPU
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/cpu/online-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Online Installation
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/cpu/air-gapped-installation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Air-Gapped Installation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/installation-script/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installation Script
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../installation/uninstallation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Uninstallation
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../upgrade/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Upgrade
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    User Guide
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            User Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../user-guide/playground/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Playground
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            Playground
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/playground/chat/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chat
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/playground/image/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/playground/audio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Audio
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/playground/embedding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Embedding
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/playground/rerank/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rerank
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/model-management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/model-catalog/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Catalog
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/model-file-management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model File management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/api-key-management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Key Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/user-management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    User Management
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/inference-backends/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference Backends
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/pinned-backend-versions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Pinned Backend Versions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/compatibility-check/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Compatibility Check
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/openai-compatible-apis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI Compatible APIs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/image-generation-apis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Image Generation APIs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../user-guide/rerank-api/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rerank API
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Using Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Using Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/using-large-language-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Large Language Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/using-vision-language-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Vision Language Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/using-embedding-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Embedding Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/using-reranker-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Reranker Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/using-image-generation-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Image Generation Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/recommended-parameters-for-image-generation-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Recommended Parameters for Image Generation Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/editing-images/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Editing Images
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../using-models/using-audio-models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Using Audio Models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/running-deepseek-r1-671b-with-distributed-vllm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running DeepSeek R1 671B with Distributed vLLM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/performing-distributed-inference-across-workers-llama-box/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Performing Distributed Inference Across Workers (llama-box)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/inference-on-cpus/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference On CPUs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/inference-with-tool-calling/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference with Tool Calling
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/running-on-copilot-plus-pcs-with-snapdragon-x/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running on Copilot+ PCs with Snapdragon X
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Integrations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integrations/openai-compatible-apis/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    OpenAI Compatible APIs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integrations/interate-with-dify/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Integrate with Dify
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../integrations/interate-with-ragflow/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Integrate with RAGFlow
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Architecture
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../scheduler/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Scheduler
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../troubleshooting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Troubleshooting
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#support-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Support Matrix
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Support Matrix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybird-cluster-support" class="md-nav__link">
    <span class="md-ellipsis">
      Hybird Cluster Support
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributed-inference-support" class="md-nav__link">
    <span class="md-ellipsis">
      Distributed Inference Support
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Installation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-change-the-default-gpustack-port" class="md-nav__link">
    <span class="md-ellipsis">
      How can I change the default GPUStack port?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-change-the-registered-worker-name" class="md-nav__link">
    <span class="md-ellipsis">
      How can I change the registered worker name?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-change-the-registered-worker-ip" class="md-nav__link">
    <span class="md-ellipsis">
      How can I change the registered worker IP?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-are-gpustacks-data-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are GPUStack's data stored?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-are-model-files-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are model files stored?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-parameters-can-i-set-when-starting-gpustack" class="md-nav__link">
    <span class="md-ellipsis">
      What parameters can I set when starting GPUStack?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#upgrade" class="md-nav__link">
    <span class="md-ellipsis">
      Upgrade
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Upgrade">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-upgrade-the-built-in-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade the built-in vLLM?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-upgrade-the-built-in-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade the built-in Transformers?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-upgrade-the-built-in-llama-box" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade the built-in llama-box?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#view-logs" class="md-nav__link">
    <span class="md-ellipsis">
      View Logs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="View Logs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-view-the-gpustack-logs" class="md-nav__link">
    <span class="md-ellipsis">
      How can I view the GPUStack logs?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-enable-debug-mode-in-gpustack" class="md-nav__link">
    <span class="md-ellipsis">
      How can I enable debug mode in GPUStack?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-view-the-rpc-server-logs" class="md-nav__link">
    <span class="md-ellipsis">
      How can I view the RPC server logs?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-are-the-model-logs-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are the model logs stored?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-enable-the-backend-debug-mode" class="md-nav__link">
    <span class="md-ellipsis">
      How can I enable the backend debug mode?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-workers" class="md-nav__link">
    <span class="md-ellipsis">
      Managing Workers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Managing Workers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-worker-is-stuck-in-unreachable-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the worker is stuck in Unreachable state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-worker-is-stuck-in-notready-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the worker is stuck in NotReady state?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detect-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      Detect GPUs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Detect GPUs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-did-it-fail-to-detect-the-ascend-npus" class="md-nav__link">
    <span class="md-ellipsis">
      Why did it fail to detect the Ascend NPUs?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-models" class="md-nav__link">
    <span class="md-ellipsis">
      Managing Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Managing Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy the model?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How can I deploy the model?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-the-model-from-hugging-face" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy the model from Hugging Face?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-the-model-from-local-path" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy the model from Local Path?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-a-locally-downloaded-ollama-model" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy a locally downloaded Ollama model?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-model-is-stuck-in-pending-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the model is stuck in Pending state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-model-is-stuck-in-scheduled-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the model is stuck in Scheduled state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-model-is-stuck-in-error-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the model is stuck in Error state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-resolve-the-error-so-cannot-open-shared-object-file-no-such-file-or-directory" class="md-nav__link">
    <span class="md-ellipsis">
      How can I resolve the error *.so: cannot open shared object file: No such file or directory?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-did-it-fail-to-load-the-model-when-using-the-local-path" class="md-nav__link">
    <span class="md-ellipsis">
      Why did it fail to load the model when using the local path?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-doesnt-deleting-a-model-free-up-disk-space" class="md-nav__link">
    <span class="md-ellipsis">
      Why doesnâ€™t deleting a model free up disk space?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-each-gpu-have-a-llama-box-process-by-default" class="md-nav__link">
    <span class="md-ellipsis">
      Why does each GPU have a llama-box process by default?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backend-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Backend Parameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Backend Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-know-the-purpose-of-the-backend-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      How can I know the purpose of the backend parameters?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-set-the-models-context-length" class="md-nav__link">
    <span class="md-ellipsis">
      How can I set the modelâ€™s context length?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-models" class="md-nav__link">
    <span class="md-ellipsis">
      Using Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      Using Vision Language Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Vision Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-resolve-the-error-at-most-1-images-may-be-provided-in-one-request" class="md-nav__link">
    <span class="md-ellipsis">
      How can I resolve the error At most 1 image(s) may be provided in one request?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-gpustack" class="md-nav__link">
    <span class="md-ellipsis">
      Managing GPUStack
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Managing GPUStack">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-manage-the-gpustack-service" class="md-nav__link">
    <span class="md-ellipsis">
      How can I manage the GPUStack service?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-do-i-use-gpustack-behind-a-proxy" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use GPUStack behind a proxy?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../api-reference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_14" >
        
          
          <label class="md-nav__link" for="__nav_14" id="__nav_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CLI Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            CLI Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cli-reference/start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Start
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cli-reference/chat/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Chat
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cli-reference/draw/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Draw
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cli-reference/download-tools/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Download Tools
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#support-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Support Matrix
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Support Matrix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hybird-cluster-support" class="md-nav__link">
    <span class="md-ellipsis">
      Hybird Cluster Support
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributed-inference-support" class="md-nav__link">
    <span class="md-ellipsis">
      Distributed Inference Support
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Installation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-change-the-default-gpustack-port" class="md-nav__link">
    <span class="md-ellipsis">
      How can I change the default GPUStack port?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-change-the-registered-worker-name" class="md-nav__link">
    <span class="md-ellipsis">
      How can I change the registered worker name?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-change-the-registered-worker-ip" class="md-nav__link">
    <span class="md-ellipsis">
      How can I change the registered worker IP?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-are-gpustacks-data-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are GPUStack's data stored?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-are-model-files-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are model files stored?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-parameters-can-i-set-when-starting-gpustack" class="md-nav__link">
    <span class="md-ellipsis">
      What parameters can I set when starting GPUStack?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#upgrade" class="md-nav__link">
    <span class="md-ellipsis">
      Upgrade
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Upgrade">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-upgrade-the-built-in-vllm" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade the built-in vLLM?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-upgrade-the-built-in-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade the built-in Transformers?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-upgrade-the-built-in-llama-box" class="md-nav__link">
    <span class="md-ellipsis">
      How can I upgrade the built-in llama-box?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#view-logs" class="md-nav__link">
    <span class="md-ellipsis">
      View Logs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="View Logs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-view-the-gpustack-logs" class="md-nav__link">
    <span class="md-ellipsis">
      How can I view the GPUStack logs?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-enable-debug-mode-in-gpustack" class="md-nav__link">
    <span class="md-ellipsis">
      How can I enable debug mode in GPUStack?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-view-the-rpc-server-logs" class="md-nav__link">
    <span class="md-ellipsis">
      How can I view the RPC server logs?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#where-are-the-model-logs-stored" class="md-nav__link">
    <span class="md-ellipsis">
      Where are the model logs stored?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-enable-the-backend-debug-mode" class="md-nav__link">
    <span class="md-ellipsis">
      How can I enable the backend debug mode?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-workers" class="md-nav__link">
    <span class="md-ellipsis">
      Managing Workers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Managing Workers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-worker-is-stuck-in-unreachable-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the worker is stuck in Unreachable state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-worker-is-stuck-in-notready-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the worker is stuck in NotReady state?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#detect-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      Detect GPUs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Detect GPUs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-did-it-fail-to-detect-the-ascend-npus" class="md-nav__link">
    <span class="md-ellipsis">
      Why did it fail to detect the Ascend NPUs?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-models" class="md-nav__link">
    <span class="md-ellipsis">
      Managing Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Managing Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-the-model" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy the model?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="How can I deploy the model?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-the-model-from-hugging-face" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy the model from Hugging Face?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-the-model-from-local-path" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy the model from Local Path?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-deploy-a-locally-downloaded-ollama-model" class="md-nav__link">
    <span class="md-ellipsis">
      How can I deploy a locally downloaded Ollama model?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-model-is-stuck-in-pending-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the model is stuck in Pending state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-model-is-stuck-in-scheduled-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the model is stuck in Scheduled state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-should-i-do-if-the-model-is-stuck-in-error-state" class="md-nav__link">
    <span class="md-ellipsis">
      What should I do if the model is stuck in Error state?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-resolve-the-error-so-cannot-open-shared-object-file-no-such-file-or-directory" class="md-nav__link">
    <span class="md-ellipsis">
      How can I resolve the error *.so: cannot open shared object file: No such file or directory?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-did-it-fail-to-load-the-model-when-using-the-local-path" class="md-nav__link">
    <span class="md-ellipsis">
      Why did it fail to load the model when using the local path?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-doesnt-deleting-a-model-free-up-disk-space" class="md-nav__link">
    <span class="md-ellipsis">
      Why doesnâ€™t deleting a model free up disk space?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-does-each-gpu-have-a-llama-box-process-by-default" class="md-nav__link">
    <span class="md-ellipsis">
      Why does each GPU have a llama-box process by default?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backend-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      Backend Parameters
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Backend Parameters">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-know-the-purpose-of-the-backend-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      How can I know the purpose of the backend parameters?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-can-i-set-the-models-context-length" class="md-nav__link">
    <span class="md-ellipsis">
      How can I set the modelâ€™s context length?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-models" class="md-nav__link">
    <span class="md-ellipsis">
      Using Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      Using Vision Language Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Using Vision Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-resolve-the-error-at-most-1-images-may-be-provided-in-one-request" class="md-nav__link">
    <span class="md-ellipsis">
      How can I resolve the error At most 1 image(s) may be provided in one request?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#managing-gpustack" class="md-nav__link">
    <span class="md-ellipsis">
      Managing GPUStack
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Managing GPUStack">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-can-i-manage-the-gpustack-service" class="md-nav__link">
    <span class="md-ellipsis">
      How can I manage the GPUStack service?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-do-i-use-gpustack-behind-a-proxy" class="md-nav__link">
    <span class="md-ellipsis">
      How do I use GPUStack behind a proxy?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/gpustack/gpustack/edit/main/docs/faq.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/gpustack/gpustack/raw/main/docs/faq.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  


<h1 id="faq">FAQ</h1>
<h2 id="support-matrix">Support Matrix</h2>
<h3 id="hybird-cluster-support">Hybird Cluster Support</h3>
<p>It supports a mix of Linux, Windows, and macOS nodes, as well as x86_64 and arm64 architectures. Additionally, It also supports various GPUs, including NVIDIA, Apple Metal, AMD, Ascend, Hygon and Moore Threads.</p>
<h3 id="distributed-inference-support">Distributed Inference Support</h3>
<p><strong>Single-Node Multi-GPU</strong></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> llama-box (Image Generation models are not supported)</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> vLLM</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> MindIE</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> vox-box</li>
</ul>
<p><strong>Multi-Node Multi-GPU</strong></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> llama-box</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> vLLM</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> MindIE</li>
</ul>
<p><strong>Heterogeneous-Node Multi-GPU</strong></p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled checked/><span class="task-list-indicator"></span></label> llama-box</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Related documentations:</p>
<p><strong>vLLM</strong>ï¼š<a href="https://docs.vllm.ai/en/latest/serving/distributed_serving.html">Distributed Inference and Serving</a></p>
<p><strong>llama-box</strong>ï¼š<a href="https://github.com/ggml-org/llama.cpp/tree/master/examples/rpc">Distributed LLM inference with llama.cpp</a></p>
</div>
<h2 id="installation">Installation</h2>
<h3 id="how-can-i-change-the-default-gpustack-port">How can I change the default GPUStack port?</h3>
<p>By default, the GPUStack server uses port 80. You can change it using the following method:</p>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>vim<span class="w"> </span>/etc/systemd/system/gpustack.service
</code></pre></div>
<p>Add the <code>--port</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="nv">ExecStart</span><span class="o">=</span>/root/.local/bin/gpustack<span class="w"> </span>start<span class="w"> </span>--port<span class="w"> </span><span class="m">9090</span>
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>daemon-reload<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>vim<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<p>Add the <code>--port</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span>&lt;array&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;start&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;--port&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;9090&lt;/string&gt;
<span class="w">  </span>&lt;/array&gt;
</code></pre></div>
<p>Save and start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">nssm</span> <span class="n">edit</span> <span class="n">GPUStack</span>
</code></pre></div>
<p>Add parameter after <code>start</code>:</p>
<div class="highlight"><pre><span></span><code>start --port 9090
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Add the <code>--port</code> parameter at the end of the <code>docker run</code> command, as shown below:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>gpustack-data:/var/lib/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">9090</span>
</code></pre></div>
<p><strong>pip Installation</strong></p>
<p>Add the <code>--port</code> parameter at the end of the <code>gpustack start</code>:</p>
<div class="highlight"><pre><span></span><code>gpustack<span class="w"> </span>start<span class="w"> </span>--port<span class="w"> </span><span class="m">9090</span>
</code></pre></div>
<h3 id="how-can-i-change-the-registered-worker-name">How can I change the registered worker name?</h3>
<p>You can set it to a custom name using the <code>--worker-name</code> parameter when running GPUStack:</p>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>vim<span class="w"> </span>/etc/systemd/system/gpustack.service
</code></pre></div>
<p>Add the <code>--worker-name</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="nv">ExecStart</span><span class="o">=</span>/root/.local/bin/gpustack<span class="w"> </span>start<span class="w"> </span>--worker-name<span class="w"> </span>New-Name
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>daemon-reload<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>vim<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<p>Add the <code>--worker-name</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span>&lt;array&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;start&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;--worker-name&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;New-Name&lt;/string&gt;
<span class="w">  </span>&lt;/array&gt;
</code></pre></div>
<p>Save and start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">nssm</span> <span class="n">edit</span> <span class="n">GPUStack</span>
</code></pre></div>
<p>Add parameter after <code>start</code>:</p>
<div class="highlight"><pre><span></span><code>start --worker-name New-Name
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Add the <code>--worker-name</code> parameter at the end of the <code>docker run</code> command, as shown below:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>gpustack-data:/var/lib/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--worker-name<span class="w"> </span>New-Name
</code></pre></div>
<p><strong>pip Installation</strong></p>
<p>Add the <code>--worker-name</code> parameter at the end of the <code>gpustack start</code>:</p>
<div class="highlight"><pre><span></span><code>gpustack<span class="w"> </span>start<span class="w"> </span>--worker-name<span class="w"> </span>New-Name
</code></pre></div>
<h3 id="how-can-i-change-the-registered-worker-ip">How can I change the registered worker IP?</h3>
<p>You can set it to a custom IP using the <code>--worker-ip</code> parameter when running GPUStack:</p>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>vim<span class="w"> </span>/etc/systemd/system/gpustack.service
</code></pre></div>
<p>Add the <code>--worker-ip</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="nv">ExecStart</span><span class="o">=</span>/root/.local/bin/gpustack<span class="w"> </span>start<span class="w"> </span>--worker-ip<span class="w"> </span>xx.xx.xx.xx
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>daemon-reload<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>vim<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<p>Add the <code>--worker-ip</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span>&lt;array&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;start&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;--worker-ip&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;xx.xx.xx.xx&lt;/string&gt;
<span class="w">  </span>&lt;/array&gt;
</code></pre></div>
<p>Save and start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">nssm</span> <span class="n">edit</span> <span class="n">GPUStack</span>
</code></pre></div>
<p>Add parameter after <code>start</code>:</p>
<div class="highlight"><pre><span></span><code>start --worker-ip xx.xx.xx.xx
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Add the <code>--worker-ip</code> parameter at the end of the <code>docker run</code> command, as shown below:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>gpustack-data:/var/lib/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--worker-ip<span class="w"> </span>xx.xx.xx.xx
</code></pre></div>
<p><strong>pip Installation</strong></p>
<p>Add the <code>--worker-ip</code> parameter at the end of the <code>gpustack start</code>:</p>
<div class="highlight"><pre><span></span><code>gpustack<span class="w"> </span>start<span class="w"> </span>--worker-ip<span class="w"> </span>xx.xx.xx.xx
</code></pre></div>
<h3 id="where-are-gpustacks-data-stored">Where are GPUStack's data stored?</h3>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<p>The default path is as follows:</p>
<div class="highlight"><pre><span></span><code>/var/lib/gpustack
</code></pre></div>
<p>You can set it to a custom path using the <code>--data-dir</code> parameter when running GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>vim<span class="w"> </span>/etc/systemd/system/gpustack.service
</code></pre></div>
<p>Add the <code>--data-dir</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="nv">ExecStart</span><span class="o">=</span>/root/.local/bin/gpustack<span class="w"> </span>start<span class="w"> </span>--data-dir<span class="w"> </span>/data/gpustack-data
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>daemon-reload<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<p>The default path is as follows:</p>
<div class="highlight"><pre><span></span><code>/var/lib/gpustack
</code></pre></div>
<p>You can set it to a custom path using the <code>--data-dir</code> parameter when running GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>vim<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="w">  </span>&lt;array&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;start&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;--data-dir&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/data/gpustack-data&lt;/string&gt;
<span class="w">  </span>&lt;/array&gt;
</code></pre></div>
<p>Save and start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<p>The default path is as follows:</p>
<div class="highlight"><pre><span></span><code><span class="s2">&quot;$env:APPDATA\gpustack&quot;</span>
</code></pre></div>
<p>You can set it to a custom path using the <code>--data-dir</code> parameter when running GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="n">nssm</span> <span class="n">edit</span> <span class="n">GPUStack</span>
</code></pre></div>
<p>Add parameter after <code>start</code>:</p>
<div class="highlight"><pre><span></span><code>start --data-dir D:\gpustack-data
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>When running the GPUStack container, the Docker volume is mounted using <code>-v</code> parameter. The default data path is under the Docker data directory, specifically in the volumes subdirectory, and the default path is:</p>
<div class="highlight"><pre><span></span><code>/var/lib/docker/volumes/gpustack-data/_data
</code></pre></div>
<p>You can check it by the following method:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>volume<span class="w"> </span>ls
docker<span class="w"> </span>volume<span class="w"> </span>inspect<span class="w"> </span>gpustack-data
</code></pre></div>
<p>If you need to change it to a custom path, modify the mount configuration when running container. For example, to mount the host directory <code>/data/gpustack</code>:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/data/gpustack:/var/lib/gpustack<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack
</code></pre></div>
<p><strong>pip Installation</strong></p>
<p>Add the <code>--data-dir</code> parameter at the end of the <code>gpustack start</code>:</p>
<div class="highlight"><pre><span></span><code>gpustack<span class="w"> </span>start<span class="w"> </span>--data-dir<span class="w"> </span>/data/gpustack-data
</code></pre></div>
<h3 id="where-are-model-files-stored">Where are model files stored?</h3>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<p>The default path is as follows:</p>
<div class="highlight"><pre><span></span><code>/var/lib/gpustack/cache
</code></pre></div>
<p>You can set it to a custom path using the <code>--cache-dir</code> parameter when running GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>vim<span class="w"> </span>/etc/systemd/system/gpustack.service
</code></pre></div>
<p>Add the <code>--cache-dir</code> parameter:</p>
<div class="highlight"><pre><span></span><code><span class="nv">ExecStart</span><span class="o">=</span>/root/.local/bin/gpustack<span class="w"> </span>start<span class="w"> </span>--cache-dir<span class="w"> </span>/data/model-cache
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>daemon-reload<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<p>The default path is as follows:</p>
<div class="highlight"><pre><span></span><code>/var/lib/gpustack/cache
</code></pre></div>
<p>You can set it to a custom path using the <code>--cache-dir</code> parameter when running GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>vim<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="w">  </span>&lt;array&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;start&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;--cache-dir&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/data/model-cache&lt;/string&gt;
<span class="w">  </span>&lt;/array&gt;
</code></pre></div>
<p>Save and start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<p>The default path is as follows:</p>
<div class="highlight"><pre><span></span><code><span class="s2">&quot;$env:APPDATA\gpustack\cache&quot;</span>
</code></pre></div>
<p>You can set it to a custom path using the <code>--cache-dir</code> parameter when running GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="n">nssm</span> <span class="n">edit</span> <span class="n">GPUStack</span>
</code></pre></div>
<p>Add parameter after <code>start</code>:</p>
<div class="highlight"><pre><span></span><code>start --cache-dir D:\model-cache
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>When running the GPUStack container, the Docker volume is mounted using <code>-v</code> parameter. The default cache path is under the Docker data directory, specifically in the volumes subdirectory, and the default path is:</p>
<div class="highlight"><pre><span></span><code>/var/lib/docker/volumes/gpustack-data/_data/cache
</code></pre></div>
<p>You can check it by the following method:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>volume<span class="w"> </span>ls
docker<span class="w"> </span>volume<span class="w"> </span>inspect<span class="w"> </span>gpustack-data
</code></pre></div>
<p>If you need to change it to a custom path, modify the mount configuration when running container.</p>
<blockquote>
<p><strong>Note</strong>: If the data directory is already mounted, the cache directory should not be mounted inside the data directory. You need to specify a different path using the <code>--cache-dir</code> parameter.</p>
</blockquote>
<p>For example, to mount the host directory <code>/data/model-cache</code>:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/data/gpustack:/var/lib/gpustack<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/data/model-cache:/data/model-cache<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cache-dir<span class="w"> </span>/data/model-cache
</code></pre></div>
<p><strong>pip Installation</strong></p>
<p>Add the <code>--cache-dir</code> parameter at the end of the <code>gpustack start</code>:</p>
<div class="highlight"><pre><span></span><code>gpustack<span class="w"> </span>start<span class="w"> </span>--cache-dir<span class="w"> </span>/data/model-cache
</code></pre></div>
<h3 id="what-parameters-can-i-set-when-starting-gpustack">What parameters can I set when starting GPUStack?</h3>
<p>Please refer to: <a href="../cli-reference/start/">gpustack start</a></p>
<h2 id="upgrade">Upgrade</h2>
<h3 id="how-can-i-upgrade-the-built-in-vllm">How can I upgrade the built-in vLLM?</h3>
<p>GPUStack supports multiple versions of inference backends. When deploying a model, you can specify the backend version in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Version</code> to use a <a href="https://github.com/vllm-project/vllm/releases">newly released vLLM version</a>. GPUStack will automatically create a virtual environment using pipx to install it:</p>
<p><a class="glightbox" href="../assets/faq/pin-vllm-backend-version.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="pin-vllm-backend-version" src="../assets/faq/pin-vllm-backend-version.png" /></a></p>
<p>Or you can manually install the custom version, including using a custom PyPI mirror, and then link the executable to <code>/var/lib/gpustack/bin/</code>. After that, configure and use it as described above.</p>
<div class="highlight"><pre><span></span><code>python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span><span class="k">$(</span>pipx<span class="w"> </span>environment<span class="w"> </span>--value<span class="w"> </span>PIPX_LOCAL_VENVS<span class="k">)</span>/vllm-v0-8-5-post1
<span class="k">$(</span>pipx<span class="w"> </span>environment<span class="w"> </span>--value<span class="w"> </span>PIPX_LOCAL_VENVS<span class="k">)</span>/vllm-v0-8-5-post1/bin/python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">vllm</span><span class="o">==</span>v0.8.5.post1<span class="w"> </span>-i<span class="w"> </span>https://mirrors.aliyun.com/pypi/simple
sudo<span class="w"> </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/var/lib/gpustack/bin
sudo<span class="w"> </span>ln<span class="w"> </span>-s<span class="w"> </span><span class="k">$(</span>pipx<span class="w"> </span>environment<span class="w"> </span>--value<span class="w"> </span>PIPX_LOCAL_VENVS<span class="k">)</span>/vllm-v0-8-5-post1/bin/vllm<span class="w"> </span>/var/lib/gpustack/bin/vllm_v0.8.5.post1
</code></pre></div>
<p>If you still need to upgrade the built-in vLLM, you can upgrade vLLM on all worker nodes using the following method:</p>
<p><strong>Script Installation</strong></p>
<div class="highlight"><pre><span></span><code>pipx<span class="w"> </span>runpip<span class="w"> </span>gpustack<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>vllm
pipx<span class="w"> </span>runpip<span class="w"> </span>gpustack<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>vllm
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>gpustack<span class="w"> </span>bash
pip<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>vllm
pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>vllm
</code></pre></div>
<p><strong>pip Installation</strong></p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>vllm
pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>vllm
</code></pre></div>
<h3 id="how-can-i-upgrade-the-built-in-transformers">How can I upgrade the built-in Transformers?</h3>
<p><strong>Script Installation</strong></p>
<div class="highlight"><pre><span></span><code>pipx<span class="w"> </span>runpip<span class="w"> </span>gpustack<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>transformers
pipx<span class="w"> </span>runpip<span class="w"> </span>gpustack<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>transformers
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>gpustack<span class="w"> </span>bash
pip<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>transformers
pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>transformers
</code></pre></div>
<p><strong>pip Installation</strong></p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>list<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>transformers
pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>transformers
</code></pre></div>
<h3 id="how-can-i-upgrade-the-built-in-llama-box">How can I upgrade the built-in llama-box?</h3>
<p>GPUStack supports multiple versions of inference backends. When deploying a model, you can specify the backend version in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Version</code> to use a <a href="https://github.com/gpustack/llama-box/releases">newly released llama-box version</a>. GPUStack will automatically download and configure it:</p>
<p><a class="glightbox" href="../assets/faq/pin-llama-box-backend-version.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="pin-llama-box-backend-version" src="../assets/faq/pin-llama-box-backend-version.png" /></a></p>
<p>If you are using distributed inference, you should upgrade llama-box on all worker nodes using the following method:</p>
<p>Download a newly released llama-box binary from <a href="https://github.com/gpustack/llama-box/releases">llama-box releases</a>.</p>
<p>And you need to stop the GPUStack first, then replace the binary, and finally restart the GPUStack. You can check the file location through some directories, for example:</p>
<p><strong>Script &amp; pip Installation</strong></p>
<div class="highlight"><pre><span></span><code>ps<span class="w"> </span>-ef<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>llama-box
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>gpustack<span class="w"> </span>bash
ps<span class="w"> </span>-ef<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>llama-box
</code></pre></div>
<h2 id="view-logs">View Logs</h2>
<h3 id="how-can-i-view-the-gpustack-logs">How can I view the GPUStack logs?</h3>
<p>The GPUStack logs provide information on the startup status, calculated model resource requirements, and more. Refer to the <a href="../troubleshooting/#view-gpustack-logs">Troubleshooting</a> for viewing the GPUStack logs.</p>
<h3 id="how-can-i-enable-debug-mode-in-gpustack">How can I enable debug mode in GPUStack?</h3>
<p>You can temporarily enable debug mode without interrupting the GPUStack service. Refer to the <a href="../troubleshooting/#configure-log-level">Troubleshooting</a> for guidance.</p>
<p>If you want to enable debug mode persistently, both server and worker can add the <code>--debug</code> parameter when running GPUStack:</p>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>vim<span class="w"> </span>/etc/systemd/system/gpustack.service
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nv">ExecStart</span><span class="o">=</span>/root/.local/bin/gpustack<span class="w"> </span>start<span class="w"> </span>--debug
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>daemon-reload<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>vim<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="w">  </span>&lt;array&gt;
<span class="w">    </span>&lt;string&gt;/Users/gpustack/.local/bin/gpustack&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;start&lt;/string&gt;
<span class="w">    </span>&lt;string&gt;--debug&lt;/string&gt;
<span class="w">  </span>&lt;/array&gt;
</code></pre></div>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">nssm</span> <span class="n">edit</span> <span class="n">GPUStack</span>
</code></pre></div>
<p>Add parameter after <code>start</code>:</p>
<div class="highlight"><pre><span></span><code>start --debug
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Add the <code>--debug</code> parameter at the end of the <code>docker run</code> command, as shown below:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>gpustack-data:/var/lib/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--debug
</code></pre></div>
<p><strong>pip Installation</strong></p>
<p>Add the <code>--debug</code> parameter at the end of the <code>gpustack start</code>:</p>
<div class="highlight"><pre><span></span><code>gpustack<span class="w"> </span>start<span class="w"> </span>--debug
</code></pre></div>
<h3 id="how-can-i-view-the-rpc-server-logs">How can I view the RPC server logs?</h3>
<p>RPC Server is used for distributed inference of GGUF models. If the model starts abnormally or if there are issues with distributed inference, you can check the RPC Server logs on the corresponding node:</p>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux &amp; macOS</li>
</ul>
<p>The default path is as follows. If the <code>--data-dir</code> or <code>--log-dir</code> parameters are set, please modify it to the actual path you have configured:</p>
<div class="highlight"><pre><span></span><code>tail<span class="w"> </span>-200f<span class="w"> </span>/var/lib/gpustack/log/rpc_server/gpu-0.log
</code></pre></div>
<p>Each GPU corresponds to an RPC Server. For other GPU indices, modify it to the actual index:</p>
<div class="highlight"><pre><span></span><code>tail<span class="w"> </span>-200f<span class="w"> </span>/var/lib/gpustack/log/rpc_server/gpu-n.log
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<p>The default path is as follows. If the <code>--data-dir</code> or <code>--log-dir</code> parameters are set, please modify it to the actual path you have configured:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Get-Content</span> <span class="s2">&quot;$env:APPDATA\gpustack\log\rpc_server\gpu-0.log&quot;</span> <span class="n">-Tail</span> <span class="n">200</span> <span class="n">-Wait</span>
</code></pre></div>
<p>Each GPU corresponds to an RPC Server. For other GPU indices, modify it to the actual index:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Get-Content</span> <span class="s2">&quot;$env:APPDATA\gpustack\log\rpc_server\gpu-n.log&quot;</span> <span class="n">-Tail</span> <span class="n">200</span> <span class="n">-Wait</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>The default path is as follows. If the <code>--data-dir</code> or <code>--log-dir</code> parameters are set, please modify it to the actual path you have configured:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>gpustack<span class="w"> </span>tail<span class="w"> </span>-200f<span class="w"> </span>/var/lib/gpustack/log/rpc_server/gpu-0.log
</code></pre></div>
<p>Each GPU corresponds to an RPC Server. For other GPU indices, modify it to the actual index:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>gpustack<span class="w"> </span>tail<span class="w"> </span>-200f<span class="w"> </span>/var/lib/gpustack/log/rpc_server/gpu-n.log
</code></pre></div>
<h3 id="where-are-the-model-logs-stored">Where are the model logs stored?</h3>
<p>The model instance logs are stored in the <code>/var/lib/gpustack/log/serve/</code> directory of the corresponding worker node or worker container, with the log file named <code>id.log</code>, where id is the model instance ID. If the <code>--data-dir</code> or <code>--log-dir</code> parameter is set, the logs will be stored in the actual path specified by the parameter.</p>
<h3 id="how-can-i-enable-the-backend-debug-mode">How can I enable the backend debug mode?</h3>
<p><strong>llama-box backend (GGUF models)</strong></p>
<p>Add the <code>--verbose</code> parameter in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Parameters</code> and recreate the model instance:</p>
<p><a class="glightbox" href="../assets/faq/enable-llama-box-debug-mode.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="enable-llama-box-debug-mode" src="../assets/faq/enable-llama-box-debug-mode.png" /></a></p>
<p><strong>vLLM backends (Safetensors models)</strong></p>
<p>Add the <code>VLLM_LOGGING_LEVEL=DEBUG</code> environment variable in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Environment Variables</code> and recreate the model instance:</p>
<p><a class="glightbox" href="../assets/faq/enable-vllm-debug-mode.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="enable-vllm-debug-mode" src="../assets/faq/enable-vllm-debug-mode.png" /></a></p>
<h2 id="managing-workers">Managing Workers</h2>
<h3 id="what-should-i-do-if-the-worker-is-stuck-in-unreachable-state">What should I do if the worker is stuck in <code>Unreachable</code> state?</h3>
<p>Try accessing the URL shown in the error from the server. If the server is running in container, you need to enter the server container to execute the command:</p>
<div class="highlight"><pre><span></span><code>curl http://10.10.10.1:10150/healthz
</code></pre></div>
<h3 id="what-should-i-do-if-the-worker-is-stuck-in-notready-state">What should I do if the worker is stuck in <code>NotReady</code> state?</h3>
<p>Check the GPUStack logs on the corresponding worker <a href="../troubleshooting/#view-gpustack-logs">here</a>. If there are no abnormalities in the logs, verify that the time zones and system clocks are consistent across all nodes.</p>
<h2 id="detect-gpus">Detect GPUs</h2>
<h3 id="why-did-it-fail-to-detect-the-ascend-npus">Why did it fail to detect the Ascend NPUs?</h3>
<p>Check if <code>npu-smi</code> can be executed in the container:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>-it<span class="w"> </span>gpustack<span class="w"> </span>bash
npu-smi<span class="w"> </span>info
</code></pre></div>
<p>When the following error occurs, it indicates that other containers are also mounting the NPU device, and sharing is not supported:</p>
<div class="highlight"><pre><span></span><code>dcmi<span class="w"> </span>model<span class="w"> </span>initialized<span class="w"> </span>failed,<span class="w"> </span>because<span class="w"> </span>the<span class="w"> </span>device<span class="w"> </span>is<span class="w"> </span>used.<span class="w"> </span>ret<span class="w"> </span>is<span class="w"> </span>-8020
</code></pre></div>
<p>Check if any containers on the host have mounted NPU devices:</p>
<div class="highlight"><pre><span></span><code><span class="k">if</span><span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="k">$(</span>docker<span class="w"> </span>ps<span class="w"> </span><span class="p">|</span><span class="w"> </span>wc<span class="w"> </span>-l<span class="k">)</span><span class="w"> </span>-gt<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">]</span><span class="p">;</span><span class="w"> </span><span class="k">then</span><span class="w"> </span>docker<span class="w"> </span>ps<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-v<span class="w"> </span>CONT<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $1}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>xargs<span class="w"> </span>docker<span class="w"> </span>inspect<span class="w"> </span>--format<span class="o">=</span><span class="s1">&#39;{{printf &quot;%.5s&quot; .ID}} {{range .HostConfig.Devices}}{{.PathOnHost}} {{end}}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span>-k2<span class="p">;</span><span class="w"> </span><span class="k">fi</span><span class="p">;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span>ok
</code></pre></div>
<p>Only mount NPUs that are not mounted by other containers, specify them using the <code>--device</code>:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/davinci4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/davinci5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/davinci6<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/davinci7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/davinci_manager<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/devmm_svm<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/hisi_hdc<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/usr/local/dcmi:/usr/local/dcmi<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/usr/local/bin/npu-smi:/usr/local/bin/npu-smi<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/etc/ascend_install.info:/etc/ascend_install.info<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>gpustack-data:/var/lib/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack:latest-npu
</code></pre></div>
<h2 id="managing-models">Managing Models</h2>
<h3 id="how-can-i-deploy-the-model">How can I deploy the model?</h3>
<h4 id="how-can-i-deploy-the-model-from-hugging-face">How can I deploy the model from Hugging Face?</h4>
<p>To deploy models from Hugging Face, the server node and the worker nodes where the model instances are scheduled must have access to Hugging Face, or you can use a mirror.</p>
<p>For example, configure the <code>hf-mirror.com</code> mirror:</p>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<p>Create or edit <code>/etc/default/gpustack</code> on <strong>all nodes</strong> , add the <code>HF_ENDPOINT</code> environment variable to use <code>https://hf-mirror.com</code> as the Hugging Face mirror:</p>
<div class="highlight"><pre><span></span><code>vim<span class="w"> </span>/etc/default/gpustack
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nv">HF_ENDPOINT</span><span class="o">=</span>https://hf-mirror.com
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Add the <code>HF_ENDPOINT</code> environment variable when running container, as shown below:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-d<span class="w"> </span>--name<span class="w"> </span>gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--restart<span class="o">=</span>unless-stopped<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span>all<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-e<span class="w"> </span><span class="nv">HF_ENDPOINT</span><span class="o">=</span>https://hf-mirror.com<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>gpustack-data:/var/lib/gpustack<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>gpustack/gpustack
</code></pre></div>
<p><strong>pip Installation</strong></p>
<div class="highlight"><pre><span></span><code>HF_ENDPOINT=https://hf-mirror.com gpustack start
</code></pre></div>
<h4 id="how-can-i-deploy-the-model-from-local-path">How can I deploy the model from Local Path?</h4>
<p>When deploying models from Local Path, it is recommended to <strong>upload the model files to each node</strong> and <strong>maintain the same absolute path</strong>. Alternatively, the model instance should be manually scheduled to nodes that have the model files via manual scheduling or label selection. Another option is to mount a shared storage across multiple nodes.</p>
<p>When deploying GGUF models from Local Path, the path <strong>must point to the absolute path of the <code>.gguf</code> file</strong>. For sharded model files, use the absolute path of the first <code>.gguf</code> file (00001). If using container installation, the model files must be mounted into the container, and the path <strong>should point to the containerâ€™s path</strong>, not the hostâ€™s path.</p>
<p>When deploying Safetensors models from Local Path, the path <strong>must point to the absolute path of the model directory which contain <code>*.safetensors</code>, <code>config.json</code>, and other files</strong>. If using container installation, the model files must be mounted into the container, and the path <strong>should point to the containerâ€™s path</strong>, not the hostâ€™s path.</p>
<p><a class="glightbox" href="../assets/faq/deploy-model-from-local-path.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="deploy-model-from-local-path" src="../assets/faq/deploy-model-from-local-path.png" /></a></p>
<h4 id="how-can-i-deploy-a-locally-downloaded-ollama-model">How can I deploy a locally downloaded Ollama model?</h4>
<p>Use the following command to find the full path of the model file and deploy it via the Local Path. The example below uses <code>deepseek-r1:14b-qwen-distill-q4_K_M</code>, be sure to replace it with your actual Ollama model name:</p>
<div class="highlight"><pre><span></span><code>ollama<span class="w"> </span>show<span class="w"> </span>deepseek-r1:14b-qwen-distill-q4_K_M<span class="w"> </span>--modelfile<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>FROM<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>blobs<span class="w"> </span><span class="p">|</span><span class="w"> </span>sed<span class="w"> </span><span class="s1">&#39;s/^FROM[[:space:]]*//&#39;</span>
</code></pre></div>
<p><a class="glightbox" href="../assets/faq/deploy-downloaded-ollama-model.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="deploy-downloaded-ollama-model" src="../assets/faq/deploy-downloaded-ollama-model.png" /></a></p>
<h3 id="what-should-i-do-if-the-model-is-stuck-in-pending-state">What should I do if the model is stuck in <code>Pending</code> state?</h3>
<p><code>Pending</code> means that there are currently no workers meeting the modelâ€™s requirements, move the mouse over the <code>Pending</code> status to view the reason.</p>
<p>First, check the <code>Resources</code>-<code>Workers</code> section to ensure that the worker status is Ready.</p>
<p>Then, for different backends:</p>
<ul>
<li>llama-box</li>
</ul>
<p>llama-box uses the <a href="https://github.com/gpustack/gguf-parser-go">GGUF Parser</a> to calculate the modelâ€™s memory requirements. You need to ensure that the allocatable memory is greater than the calculated memory requirements of the model. Note that even if other models are in an <code>Error</code> or <code>Downloading</code> state, the GPU memory has already been allocated. If you are unsure how much GPU memory the model requires, you can use the <a href="https://github.com/gpustack/gguf-parser-go">GGUF Parser</a> to calculate it.</p>
<p>The context size for the model also affects the required GPU memory. You can adjust the <code>--ctx-size</code> parameter to set a smaller context. In GPUStack, if this parameter is not set, its default value is <code>8192</code>. If it is specified in the backend parameters, the actual setting will take effect.</p>
<p>You can adjust it to a smaller context in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Parameters</code> as needed, for example, <code>--ctx-size=2048</code>. However, keep in mind that the max tokens for each inference request is influenced by both the <code>--ctx-size</code> and <code>--parallel</code> parameters:
<code>max tokens = context size / parallel</code></p>
<p>The default value of <code>--parallel</code> is <code>4</code>, so in this case, the max tokens would be <code>512</code>. If the token count exceeds the max tokens, the inference output will be truncated.</p>
<p>On the other hand, the <code>--parallel</code> parameter represents the number of parallel sequences to decode, which can roughly be considered as a setting for the modelâ€™s concurrent request handling.</p>
<p>Therefore, it is important to appropriately set the <code>--ctx-size</code> and <code>--parallel</code> parameters, ensuring that the max tokens for a single request is within the limits and that the available GPU memory can support the specified context size.</p>
<p>If you need to align with Ollamaâ€™s configuration, you can refer to the following examples:</p>
<p>Set the following parameters in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Parameters</code>:</p>
<div class="highlight"><pre><span></span><code>--ctx-size=8192
--parallel=4
</code></pre></div>
<p>If your GPU memory is insufficient, try launching with a lower configuration:</p>
<div class="highlight"><pre><span></span><code>--ctx-size=2048
--parallel=1
</code></pre></div>
<ul>
<li>vLLM</li>
</ul>
<p>vLLM requires that all GPUs have more than 90% of their memory available by default (controlled by the <code>--gpu-memory-utilization</code> parameter). Ensure that there is enough allocatable GPU memory exceeding 90%. Note that even if other models are in an <code>Error</code> or <code>Downloading</code> state, the GPU memory has already been allocated.</p>
<p>If all GPUs have more than 90% available memory but still show <code>Pending</code>, it indicates insufficient memory. For <code>safetensors</code> models in BF16 format, the required GPU memory (GB) can be estimated as:</p>
<div class="highlight"><pre><span></span><code>GPU Memory (GB) = Number of Parameters (B) * 2 * 1.2 + 2
</code></pre></div>
<p>If the allocatable GPU memory is less than 90%, but you are sure the model can run with a lower allocation, you can adjust the <code>--gpu-memory-utilization</code> parameter. For example, add <code>--gpu-memory-utilization=0.5</code> in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Parameters</code> to allocate 50% of the GPU memory.</p>
<p><strong>Note</strong>: If the model encounters an error after running and the logs show <code>CUDA: out of memory</code>, it means the allocated GPU memory is insufficient. You will need to further adjust <code>--gpu-memory-utilization</code>, add more resources, or deploy a smaller model.</p>
<p>The context size for the model also affects the required GPU memory. You can adjust the <code>--max-model-len</code> parameter to set a smaller context. In GPUStack, if this parameter is not set, its default value is 8192. If it is specified in the backend parameters, the actual setting will take effect.</p>
<p>You can adjust it to a smaller context as needed, for example, <code>--max-model-len=2048</code>. However, keep in mind that the max tokens for each inference request cannot exceed the value of <code>--max-model-len</code>. Therefore, setting a very small context may cause inference truncation.</p>
<p>The <code>--enforce-eager</code> parameter also helps reduce GPU memory usage. However, this parameter in vLLM forces the model to execute in eager execution mode, meaning that operations are executed immediately as they are called, rather than being deferred for optimization in graph-based execution (like in lazy execution). This can make the execution slower but easier to debug. However, it can also reduce performance due to the lack of optimizations provided by graph execution.</p>
<h3 id="what-should-i-do-if-the-model-is-stuck-in-scheduled-state">What should I do if the model is stuck in <code>Scheduled</code> state?</h3>
<p>Try <a href="./#how-can-i-manage-the-gpustack-service">restarting the GPUStack service</a> where the model is scheduled. If the issue persists, check the worker logs <a href="../troubleshooting/#view-gpustack-logs">here</a> to analyze the cause.</p>
<h3 id="what-should-i-do-if-the-model-is-stuck-in-error-state">What should I do if the model is stuck in <code>Error</code> state?</h3>
<p>Move the mouse over the <code>Error</code> status to view the reason. If there is a <code>View More</code> button, click it to check the error messages in the model logs and analyze the cause of the error.</p>
<h3 id="how-can-i-resolve-the-error-so-cannot-open-shared-object-file-no-such-file-or-directory">How can I resolve the error *.so: cannot open shared object file: No such file or directory?</h3>
<p>If the error occurs during model startup indicating that any <code>.so</code> file cannot be opened, for example:</p>
<div class="highlight"><pre><span></span><code>llama-box:<span class="w"> </span>error<span class="w"> </span><span class="k">while</span><span class="w"> </span>loading<span class="w"> </span>shared<span class="w"> </span>libraries:<span class="w"> </span>libcudart.so.12:<span class="w"> </span>cannot<span class="w"> </span>open<span class="w"> </span>shared<span class="w"> </span>object<span class="w"> </span>file:<span class="w"> </span>No<span class="w"> </span>such<span class="w"> </span>file<span class="w"> </span>or<span class="w"> </span>directory
</code></pre></div>
<p>The cause is that GPUStack doesnâ€™t recognize the <code>LD_LIBRARY_PATH</code> environment variable, which may be due to a missing environment variable or unconfigured toolkits (such as CUDA, CANN, etc.) during GPUStack installation.</p>
<p>To check if the environment variable is set:</p>
<div class="highlight"><pre><span></span><code><span class="nb">echo</span><span class="w"> </span><span class="nv">$LD_LIBRARY_PATH</span>
</code></pre></div>
<p>If not configured, hereâ€™s an example configuration for CUDA.</p>
<p>Ensure that the <code>nvidia-smi</code> is executable and the NVIDIA driver version is <code>550</code> or later:</p>
<div class="highlight"><pre><span></span><code>nvidia-smi
</code></pre></div>
<p>Configure the CUDA environment variables. If not installed, install <code>CUDA 12.4</code> or later:</p>
<div class="highlight"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/cuda/targets/x86_64-linux/lib
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/usr/local/cuda/bin
<span class="nb">echo</span><span class="w"> </span><span class="nv">$LD_LIBRARY_PATH</span>
<span class="nb">echo</span><span class="w"> </span><span class="nv">$PATH</span>
</code></pre></div>
<p>Create or edit <code>/etc/default/gpustack</code> , add the <code>PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables:</p>
<div class="highlight"><pre><span></span><code>vim<span class="w"> </span>/etc/default/gpustack
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>......
<span class="nv">PATH</span><span class="o">=</span>......
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<h3 id="why-did-it-fail-to-load-the-model-when-using-the-local-path">Why did it fail to load the model when using the local path?</h3>
<p>When deploying a model using Local Path and encountering a <code>failed to load model</code> error, you need to check whether the model files exist on the node that the model instance is scheduled to, and if the absolute path is correct.</p>
<p>For GGUF models, you need to specify the absolute path to the <code>.gguf</code> file. For sharded models, use the absolute path to the first <code>.gguf</code> file (typically 00001).</p>
<p>If using Docker installation, the model files must be mounted into the container. Make sure the path you provide is the one inside the container, not the host path.</p>
<p><a class="glightbox" href="../assets/faq/deploy-model-from-local-path.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="deploy-model-from-local-path" src="../assets/faq/deploy-model-from-local-path.png" /></a></p>
<h3 id="why-doesnt-deleting-a-model-free-up-disk-space">Why doesnâ€™t deleting a model free up disk space?</h3>
<p>This is to avoid re-downloading the model when redeploying. You need to clean it up in <code>Resources</code> â†’ <code>Model Files</code> manually.</p>
<h3 id="why-does-each-gpu-have-a-llama-box-process-by-default">Why does each GPU have a llama-box process by default?</h3>
<p>This process is the RPC server used for llama-boxâ€™s distributed inference. If you are sure that you do not need distributed inference with llama-box, you can disable the RPC server service by adding the <code>--disable-rpc-servers</code> parameter when running GPUStack.</p>
<h3 id="backend-parameters">Backend Parameters</h3>
<h4 id="how-can-i-know-the-purpose-of-the-backend-parameters">How can I know the purpose of the backend parameters?</h4>
<ul>
<li>
<p><a href="https://github.com/gpustack/llama-box?tab=readme-ov-file#usage">llama-box</a></p>
</li>
<li>
<p><a href="https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#named-arguments">vLLM</a></p>
</li>
<li>
<p><a href="https://github.com/gpustack/gpustack/blob/main/gpustack/worker/backends/ascend_mindie.py#L50-L211">MindIE</a></p>
</li>
</ul>
<h4 id="how-can-i-set-the-models-context-length">How can I set the modelâ€™s context length?</h4>
<p><strong>llama-box backend (GGUF models)</strong></p>
<p>GPUStack sets the default context length for models to 8K. You can customize the context length using the <code>--ctx-size</code> parameter, but it cannot exceed the modelâ€™s maximum context length:</p>
<p><a class="glightbox" href="../assets/faq/set-the-model-context-length-for-llama-box.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="set-the-model-context-length-for-llama-box" src="../assets/faq/set-the-model-context-length-for-llama-box.png" /></a></p>
<p>If editing, save the change and then recreate the model instance to take effect.</p>
<p><strong>vLLM backend (Safetensors models)</strong></p>
<p>GPUStack sets the default context length for models to 8K. You can customize the context length using the <code>--max-model-len</code> parameter, but it cannot exceed the modelâ€™s maximum context length:</p>
<p><a class="glightbox" href="../assets/faq/set-the-model-context-length-for-vllm.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="set-the-model-context-length-for-vllm" src="../assets/faq/set-the-model-context-length-for-vllm.png" /></a></p>
<p><strong>MindIE backend (Safetensors models)</strong></p>
<p>GPUStack sets the default context length for models to 8K. You can customize the context length using the <code>--max-seq-len</code> parameter, but it cannot exceed the modelâ€™s maximum context length:</p>
<p><a class="glightbox" href="../assets/faq/set-the-model-context-length-for-mindie.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="set-the-model-context-length-for-mindie" src="../assets/faq/set-the-model-context-length-for-mindie.png" /></a></p>
<p>If editing, save the change and then recreate the model instance to take effect.</p>
<h2 id="using-models">Using Models</h2>
<h3 id="using-vision-language-models">Using Vision Language Models</h3>
<h4 id="how-can-i-resolve-the-error-at-most-1-images-may-be-provided-in-one-request">How can I resolve the error At most 1 image(s) may be provided in one request?</h4>
<p>This is a limitation of vLLM. You can adjust the <code>--limit-mm-per-prompt</code> parameter in <code>Edit Model</code> â†’ <code>Advanced</code> â†’ <code>Backend Parameters</code> as needed. For example, <code>--limit-mm-per-prompt=image=4</code> means that it supports up to 4 images per inference request, see the details <a href="https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#vllm-serve">here</a>.</p>
<h2 id="managing-gpustack">Managing GPUStack</h2>
<h3 id="how-can-i-manage-the-gpustack-service">How can I manage the GPUStack service?</h3>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux</li>
</ul>
<p>Stop GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>stop<span class="w"> </span>gpustack
</code></pre></div>
<p>Start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>start<span class="w"> </span>gpustack
</code></pre></div>
<p>Restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<ul>
<li>macOS</li>
</ul>
<p>Stop GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<p>Start GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<p>Restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>launchctl<span class="w"> </span>bootout<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
sudo<span class="w"> </span>launchctl<span class="w"> </span>bootstrap<span class="w"> </span>system<span class="w"> </span>/Library/LaunchDaemons/ai.gpustack.plist
</code></pre></div>
<ul>
<li>Windows</li>
</ul>
<p>Run PowerShell as administrator (<strong>avoid</strong> using PowerShell ISE).</p>
<p>Stop GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Stop-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p>Start GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Start-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p>Restart GPUStack:</p>
<div class="highlight"><pre><span></span><code><span class="nb">Restart-Service</span> <span class="n">-Name</span> <span class="s2">&quot;GPUStack&quot;</span>
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Restart GPUStack container:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<h3 id="how-do-i-use-gpustack-behind-a-proxy">How do I use GPUStack behind a proxy?</h3>
<p><strong>Script Installation</strong></p>
<ul>
<li>Linux &amp; macOS</li>
</ul>
<p>Create or edit <code>/etc/default/gpustack</code> and add the proxy configuration:</p>
<div class="highlight"><pre><span></span><code>vim<span class="w"> </span>/etc/default/gpustack
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="nv">http_proxy</span><span class="o">=</span><span class="s2">&quot;http://username:password@proxy-server:port&quot;</span>
<span class="nv">https_proxy</span><span class="o">=</span><span class="s2">&quot;http://username:password@proxy-server:port&quot;</span>
<span class="nv">all_proxy</span><span class="o">=</span><span class="s2">&quot;socks5://username:password@proxy-server:port&quot;</span>
<span class="nv">no_proxy</span><span class="o">=</span><span class="s2">&quot;localhost,127.0.0.1,192.168.0.0/24,172.16.0.0/16,10.0.0.0/8&quot;</span>
</code></pre></div>
<p>Save and restart GPUStack:</p>
<div class="highlight"><pre><span></span><code>systemctl<span class="w"> </span>restart<span class="w"> </span>gpustack
</code></pre></div>
<p><strong>Docker Installation</strong></p>
<p>Pass environment variables when running GPUStack:</p>
<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-e<span class="w"> </span><span class="nv">http_proxy</span><span class="o">=</span><span class="s2">&quot;http://username:password@proxy-server:port&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>-e<span class="w"> </span><span class="nv">https_proxy</span><span class="o">=</span><span class="s2">&quot;http://username:password@proxy-server:port&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>-e<span class="w"> </span><span class="nv">all_proxy</span><span class="o">=</span><span class="s2">&quot;socks5://username:password@proxy-server:port&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>-e<span class="w"> </span><span class="nv">no_proxy</span><span class="o">=</span><span class="s2">&quot;localhost,127.0.0.1,192.168.0.0/24,172.16.0.0/16,10.0.0.0/8&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">           </span>â€¦â€¦
</code></pre></div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["search.suggest", "search.highlight", "content.tabs.link", "navigation.indexes", "content.tooltips", "navigation.path", "content.code.annotate", "content.code.copy", "content.code.select", "content.action.view", "content.action.edit"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="../javascripts/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>